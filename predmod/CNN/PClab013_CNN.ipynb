{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gdewael/teaching/blob/main/predmod/CNN/PClab013_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8hMi55H3fX"
      },
      "source": [
        "<img src=\"https://pbs.twimg.com/media/FQK3KkkWUAQsW5L?format=jpg&name=medium\" width = 400>\n",
        "\n",
        "*Image generated by DALL-E 2 upon prompted by \"robot meditating on mathematics, digital art, keywords: synthwave, transcendent, beautiful mind\"*\n",
        "\n",
        "*For more examples of what this model generates: https://twitter.com/hashtag/dalle*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfqwZm0OH3fY"
      },
      "source": [
        "# PC lab: Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH0OEOmGQ3bm"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY5lpFw0H3fZ"
      },
      "source": [
        "### 1.1 A brief history\n",
        "\n",
        "Convolutional neural networks caused a major step forward in the performance of computer vision. They first became popular because of an annual challenge in image classification: The [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://en.wikipedia.org/wiki/ImageNet#History_of_the_ImageNet_challenge), where models were challenged to classify an image as belonging to one of a thousand(!) different classes. Until 2011, hand-crafted features on images in combination with more traditional machine learning methods such as SVMs reigned supreme. The best models using such techniques classified image wrongly about 26\\% of the time. In 2012, a breakthrough was made in terms of performance due to the succesful training of a large-scale CNN. Although the concept of convolutions has been known for quite some decades in machine learning, the technology for large-scale training of CNNs did not take off until this time because it was difficult to train them efficiently (GPU usage was not popular, and no solid platforms for computation on GPUs had been developed). Since then, the field has taken off and multiple developments have come yearly. In 2015, human performance in image classification on this dataset had been beaten by a 152-layer CNN model.\n",
        "\n",
        "<img src=\"https://i0.wp.com/semiengineering.com/wp-content/uploads/2019/10/Synopsys_computer-vision-processors-EV7-Fig2-ImageNet.jpeg?ssl=1\" width = 400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycK0JrjlH3fZ"
      },
      "source": [
        "### 1.2 The Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0wOnHRDH3fa"
      },
      "source": [
        "A convolution is the iteration of a kernel with size $ M \\times N $ over a given input $ \\textbf{X} $, performing a 2D linear combination of the weights $ W $  of the kernel with the overlapping area of the input. For a normal convolution with single striding and no padding, the output $ y_{ij} $ is equal to:\n",
        "\n",
        "$$ y_{ij} = \\sum_{a=0}^{m-1} \\sum_{b=0}^{n-1} W_{ab} x_{(i+a)(j+b)} $$\n",
        "\n",
        "During a convolution, the kernels slides over the input image to obtain a new image of outputs. The stride of a kernel defines the horizontal and vertical stepsize during iteration. Input data can be padded with multiple layers of a zero-filled border, increasing the output dimensions.\n",
        "\n",
        "It is important to understand that a convolution applies the same operation at every local patch in the input. In this sense, convolutions are useful when you expect the input data to contain regularly appearing **local patterns**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0vz06CJH3fb"
      },
      "source": [
        "You can find visualizations of common convolutional set-ups [here](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUM7ZZU7H3fb"
      },
      "source": [
        "Several other examples &  an extended explanation on different types of convolutions can be found [here](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZd0ZIxTH3fc"
      },
      "source": [
        "### 1.3 Channels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UULBRmWFH3fd"
      },
      "source": [
        "A convolutional neural network usually processes an image with multiple input channels at every pixel. The number of channels can be seen as the dimensionality of every input **token**. In the case of image inputs, the tokens are pixels, and its input dimensionality is three (every pixel is defined by its red/green/blue values). The kernel, although often depicted as only evaluating one channel, actually takes **the sum of all channels** to obtain an output. If we only apply one kernel, we effectively compress the information in our image (since the RGB value of the pixel now has to be represented by a single number). For this reason, multiple parallel kernels are used that each output their own channel. Because of this, we can **learn higher-dimensional spaces of local regions of pixels**. The following image shows the difference between a convolution with one output channel and a convolution with 8 output channels. (The pictured convolution employs no padding, hence the width and height of the image are also affected)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/predmod/CNN/channelsdrawing.png\" width = 800>\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>THOUGHT EXERCISE:</b> \n",
        "<p> How much weights do the above two convolutional layers have? One single bias/intercept number/weight is usually added for every output channel.</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "The .gif below shows an illustrated example with numbers of how a single convolutional kernel (i.e. one output channel) works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFW16Y6xH3fd"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/2560/1*ciDgQEjViWLnCbmX-EeSrA.gif\" width = 800>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJf2amkAQ3b7"
      },
      "source": [
        "### 1.4 The CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyh0MsMgQ3b8"
      },
      "source": [
        "A Convolutional neural network consists of more than convolutions. Just like with MLPs, we usually add non-linearities such as the ReLU after a linear layer.\n",
        "Another popular operation in convolutional neural networks is a pooling operation such as **max pooling**. Maximum pooling reduces the (width $\\times$ height) dimensionality of the input, which effectively compresses the image, which can in turn reduce the amount of parameters present in a neural network, which (again) in turn reduces overfitting and computational burden. Maximum pooling is also initialized with specific arguments such as kernel size, stride and padding.\n",
        "\n",
        "Another advantage of max pooling is that it make the networks somewhat invariant as to where in an image a certain pattern appeared (*translation invariance*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGSMvqyRH3ff"
      },
      "source": [
        "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" width = 400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tM2WUMFQ3b9"
      },
      "source": [
        "After many convolutions, we are left with a 3-dimensional object for every image: $channels \\times width \\times height$. By now, the width and height may not represent pixels anymore but something which may be described as \"pseudo\"-pixels, since every remaining \"pseudo\"-pixel has aggregated information from a region of original input pixels. The channels together then represent the vector of information gathered in that \"pseudo\"-pixel.\n",
        "\n",
        "If we want to do classification (or regression) with this \"pseudo\"-image, we need to reshape or flatten this 3-dimensional object to a 1-dimensional vector representation representing the output classes. Hence, a CNN usually consists of a convolutional part, where we extract higher dimensional features of local regions, and an MLP part, where we put linear layers on top of the flattened representations of the image (i.e. letting the information between all regions recombine to get to a final class prediction).\n",
        "\n",
        "Note that this flattening/reshaping is only necessary for classification purposes, as we need to obtain a 1-dimensional vector at the end. If we were doing, for example, image segmentation (classifying every pixel as belonging to a category, not the whole image at once), we would keep our 3-dimensional representation of images. \n",
        "\n",
        "Below is an illustration of a toy example of a cat detector that shows how you can visualize what \"pseudo\"-pixels may signify in a learned neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT8a8M8hQ3b-"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/predmod/CNN/catdetectordrawing.png\" width = 800>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D1xSKVrQ3b_"
      },
      "source": [
        "Another example showing multi-class classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAP8Iw7H3fe"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg\" width = 600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvJMCnrmH3ff"
      },
      "source": [
        "One can interpret the convolutional layers as the section of the network in which local patterns are extracted (edges, contours, contrasts,...). These are used as inputs for the fully connected neural netwerk, which combines these features to train the classifier. \n",
        "\n",
        "The following picture shows a visualization of what a CNN extracts at each layer, starting from the first layers on the left going deeper towards the right. This visualization is obtained by optimizing an input image to maximally activate the convolution filters.\n",
        "\n",
        "<img src='https://1.bp.blogspot.com/-icbxyuiDoA0/WgEivsyFIgI/AAAAAAAACKo/jsfMgFlfiVA233zXg8xAH3ZAKOchgLb-wCLcBGAs/s1600/image4.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i81s0pxQ3cB"
      },
      "source": [
        "**Final note:** It is important to realize that we have been talking about convolutions on images up until now, these are what we call 2-D convolutions (because they convolve over both width and height). The same concept is also applicable on 1-D sequences, where every input channel can represent information of every input token in that sequence (e.g. a DNA or protein sequence with as tokens bases or AAs and channels as one-hot encodings for which base or AA is present at that position). A 1D Conv would then aggregate patterns from its neighboring inputs (DNA bases or AAs).\n",
        "\n",
        "Even 3-D convolutions are possible on 3-D inputs such as 3-D renders of biologial cells (from e.g. electron microscopy) or 3D MRI-scans. Even a sequence of images (what we know as a video) can be seen as a 3-D object. A 3-D convolution on a video would then extract local patterns per image but also aggregate information from the previous and next frames (in the case of video) OR, in the case of MRI or 3D microscopy: also aggregate information from different depth slices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIT0CU7qQ3cB"
      },
      "source": [
        "### 1.5 The Residual Connection\n",
        "\n",
        "Most recent neural networks use residual connections of some sort. A residual connection adds the input of some layer(s) to its output: $y = f(x) + x$. This skip-connection helps with vanishing gradient issues as it essentially lets a part of the input signal skip the layer(s) in $f(.)$. A logical requirement for the use of residual connections is that the output of $f(x)$ has the same dimensionality as the input $x$. For convolutions, this means performing a convolution with padding so that width and height of the image are preserved, and having as much output channels as there were input channels.\n",
        "\n",
        "It is very popular to define a \"Residual Block\" as $f(.) = $ `[Layer -> Activation -> Dropout -> Normalization]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RIWH-PeQ3cC"
      },
      "source": [
        "# 2. Coding\n",
        "\n",
        "Last PC-lab we familiarized ourself with PyTorch using MLPs and the MNIST dataset, on which we coded a classifier and an autoencoder. This week, we will repeat the same workflow, but work with convolutional neural networks instead of simple MLPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07N60wMXQ3cD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZriLRXuXQ3cE"
      },
      "source": [
        "### 2.1 The convolution\n",
        "\n",
        "Let's first test some things out with convolutional layers.\n",
        "In PyTorch, The Channel dimension is put as the second dimension, just after the batch dimension and before the width x height dimensions. The following code shows the dimensions for a batch of 8 toy data images of 28 x 28 pixels with 3 channels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgNLbaL0Q3cF"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(8, 3, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg_SFi76Q3cG"
      },
      "source": [
        "PyTorch implements 2D convolutions via the [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) class. Check out the documentation to see which options you can specify. Below we illustrate the usage of some prominent ones:\n",
        "\n",
        "Remember to check back [here](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) to see visually what all the options mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJpDwklbQ3cG",
        "outputId": "eb79beec-e543-49aa-8409-7c02ff80d0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 28, 28])\n",
            "torch.Size([8, 16, 24, 24])\n"
          ]
        }
      ],
      "source": [
        "conv = nn.Conv2d(3, 16, 5) # from 3 to 16 channels with kernel size 5\n",
        "print(x.shape)\n",
        "y = conv(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCi3I0D8Q3cH",
        "outputId": "d3036a7b-c116-43a6-d42a-7eebf0662cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 28, 28])\n",
            "torch.Size([8, 16, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# padding = \"same\" will perform padding so as to conserve the input width and height\n",
        "print(x.shape)\n",
        "conv = nn.Conv2d(3, 16, 5, padding = \"same\") \n",
        "y = conv(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wfT91i5Q3cI",
        "outputId": "caf350aa-3e65-4832-cdc8-cc0db48298ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 28, 28])\n",
            "torch.Size([8, 16, 14, 14])\n"
          ]
        }
      ],
      "source": [
        "print(x.shape)\n",
        "# performing a conv of 2 x 2 every block of 2 by 2 pixels:\n",
        "conv = nn.Conv2d(3, 16, 2, stride = 2) \n",
        "y = conv(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVN_QI_NQ3cI",
        "outputId": "26f84576-dee2-42af-e943-8a52e387fc75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 28, 28])\n",
            "torch.Size([8, 8, 24, 24])\n"
          ]
        }
      ],
      "source": [
        "print(x.shape)\n",
        "conv = nn.Conv2d(3, 8, 3, dilation = 2) \n",
        "y = conv(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CovDyb8RQ3cK"
      },
      "source": [
        "PyTorch has similar Conv objects for 1D and 3D tasks. [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) also has very similar arguments.\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p> Let's implement our very own custom PyTorch layer: a convolutional residual block.\n",
        "You will be able to use this layer in further exercises when implementing a complete CNN.\n",
        "Refer to the introduction part on residual connections above to see how a Residual connection should be constructed. Remember to use appropriate kernel sizes and padding.\n",
        "</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59YArYFvQ3cK"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim = 64, kernel_size = 5):\n",
        "        super().__init__()\n",
        "\n",
        "        # YOUR CODE HERE ....\n",
        "\n",
        "        # TIP: You may want to to use these objects: ..\n",
        "        #nn.Conv2d(...)\n",
        "        #nn.ReLU()\n",
        "        #nn.Dropout()\n",
        "        #nn.BatchNorm2d(...)\n",
        "\n",
        "        # YOUR CODE HERE ....\n",
        "\n",
        "    def forward(self, x):\n",
        "        return None # CHANGE THIS TO YOUR OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFIQCelaQ3cL",
        "outputId": "6547592b-c6e6-46f6-8489-40e61f477c27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 64, 16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "ResidualBlock()(torch.randn(2, 64, 16, 16)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-TCkhBjQ3cL"
      },
      "source": [
        "### 2.2 A CNN for MNIST classification\n",
        "\n",
        "With our residual block and PyTorch knowledge from last PC lab at the ready, we can implement our very own convolutional neural network. Let's first recap the most basic building blocks of training PyTorch models:\n",
        "\n",
        "The most basic blueprint of PyTorch model training consists of\n",
        "- Get your data\n",
        "- Wrap your data splits in a [data loader](https://pytorch.org/docs/stable/data.html)\n",
        "- Instantiate the model\n",
        "- Instantiate a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- Instantiate an [optimizer object](https://pytorch.org/docs/stable/optim.html), to which you pass the parameters you want to optimize\n",
        "- Iterate through your training data, for every batch:\n",
        "    - reset the gradients\n",
        "    - do forward pass\n",
        "    - compute loss\n",
        "    - backward pass\n",
        "    - update parameters\n",
        "\n",
        "(Optionally):\n",
        "- After every full iteration through all training data samples (called an epoch), loop through all batches of validation data:\n",
        "    - forward pass\n",
        "    - compute loss and validation scores\n",
        "\n",
        "Since we have implemented every necessary step of this process last PC lab already, we will focus our remaining time on modeling a proper CNN architecture. In order to test if our model will work, we will need to design a model architecture that works for our input image sizes. For this purpose, we can generate some random data in the same size as the MNIST data. Remember that MNIST consists of grayscale images, so they only have one input channel, as opposed to RGB images. They also have a 28 x 28 resolution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnNW-2LfQ3cM"
      },
      "outputs": [],
      "source": [
        "# some generated data in the same shape that we expect our MNIST data:\n",
        "x = torch.randn(8, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtvzWxL9Q3cM"
      },
      "source": [
        "With this generated toy data ready, we can implement a CNN and test it out. We recommend to model the CNN and classifier (MLP) part separately, so we can check the dimensionality of the image after it went through the CNN (this will influence how many input nodes the subsequent MLP needs to take in).\n",
        "\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p> Implement a CNN backbone with at least 3 convolutional operations. You can optionally make use of the residual blocks for the CNN operations. We recommend to also make use of max pooling layers to reduce dimensionality.\n",
        "Because the residual block strictly has the same output and input dimensionality, you should start your network with a normal convolutional layer that returns more than one channel. You can also put convolutions (and max pools) between residual blocks to change the number of hidden dimensions.\n",
        "</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV2-3V9RQ3cN"
      },
      "outputs": [],
      "source": [
        "class CNNBackBone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        return None # CHANGE THIS TO YOUR OUTPUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjFRHkw1Q3cN"
      },
      "source": [
        "Let's see the shape of our fake image batch after a forward pass through the CNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw78HG-uQ3cO",
        "outputId": "71deee10-a4ae-4f20-a31c-24ea96bdcec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1, 28, 28])\n",
            "torch.Size([4, 16, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "print(x.shape)\n",
        "y = CNNBackBone()(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USoAWODMQ3cP"
      },
      "source": [
        "Inspect the output shapes of your CNN backbone, did you compress the information in your image or expand the total dimensionality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl3wUcRlQ3cP"
      },
      "source": [
        "You will need to reshape it before application in an MLP. In PyTorch, you can use `nn.Flatten` for this purpose. Another option is to just reshape it manually using `x.view()` (equivalent to NumPy's `x.reshape()`). In order to implement it correctly using manual reshaping, you will need to specify the first dimension (corresping to the number of samples in the batch) reshaped shape as `-1`, so that it will reshape any sort of batch size correctly.\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p> Combine the CNN backbone with an MLP with 10 output nodes for MNIST classification. At its simplest, the MLP on top of the the CNN backbone, is a single linear layer that takes in the flattened input and returns 10 ouput nodes. Alternatively, you can make use of the code in the previous PC lab to quickly implement an MLP\n",
        "</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFoGwv_6Q3cP"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):PClab013_CNN_solved\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        return None # CHANGE THIS TO YOUR OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trsljd5WQ3cQ",
        "outputId": "92c8c6c1-ff10-4373-fc3a-14d17918552c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1, 28, 28])\n",
            "torch.Size([4, 10])\n",
            "torch.Size([4, 1, 28, 28])\n",
            "torch.Size([4, 10])\n"
          ]
        }
      ],
      "source": [
        "print(x.shape)\n",
        "y = CNNClassifier()(x)\n",
        "print(y.shape)\n",
        "\n",
        "# a smaller (4 instead of 8) batch of images:\n",
        "x = torch.randn(4, 1, 28, 28)\n",
        "print(x.shape)\n",
        "y = CNNClassifier()(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Fcc7tjQ3cQ"
      },
      "source": [
        "You can use this code to inspect how much parameters you model has in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdBM_7JpQ3cR",
        "outputId": "e632b43d-11ab-45db-8851-e814484a518b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22538"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "sum([p.numel() for p in CNNClassifier().parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XEWhRyvQ3cR"
      },
      "source": [
        "Keep in mind that a very large model will take a long time to train. For the purpose of this PC lab, and considering MNIST is quite an \"easy\" dataset, we recommend you to not have more than a hundred thousand parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7kvqekPQ3cR"
      },
      "source": [
        "With a model ready, let's load in the MNIST data:\n",
        "\n",
        "Note that in comparison with last PC lab, we do not reshape the data so that every image is a single represented by a single vector. Instead, we added an `unsqueeze` statement, which adds an extra dimension for the channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roXWY5C4Q3cS"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,                         \n",
        "    transform = ToTensor(), \n",
        "    download = True,            \n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data', \n",
        "    train = False, \n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "X_train = train_data.data\n",
        "y_train = train_data.targets\n",
        "\n",
        "X_test = test_data.data\n",
        "y_test = test_data.targets\n",
        "\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "X_train = X_train.unsqueeze(1)\n",
        "X_test = X_test.unsqueeze(1)\n",
        "\n",
        "np.random.seed(42)\n",
        "train_indices, val_indices = np.split(np.random.permutation(len(X_train)), [int(len(X_train)*0.8)])\n",
        "X_val = X_train[val_indices]\n",
        "y_val = y_train[val_indices]\n",
        "X_train = X_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, pin_memory=True, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fj_jgTlQ3cT",
        "outputId": "1fb1e15f-fb71-43ad-ccf9-27fdae7f3910"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([48000, 1, 28, 28]), torch.Size([48000]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PP1UfFOQ3cT"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p> Using the training code you made last PC lab, train your own CNN model. How is it performing in comparison to the model of last PC lab?\n",
        "\n",
        "You should be able to obtain a model with at least +-98% validation accuracy\n",
        "</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWL73Zo1Q3cT",
        "outputId": "e57ed532-01ed-4a8a-932e-6eef3cfc5c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \t 0.1670128967903341 0.06191570158659791 0.9804166666666667\n",
            "2 \t 0.06538364670249576 0.05746760350108768 0.9815\n",
            "3 \t 0.050441892926167686 0.03613456173830976 0.9893333333333333\n",
            "4 \t 0.04295966227657967 0.03708137488518454 0.9884166666666667\n",
            "5 \t 0.037609025543556523 0.037692815933774305 0.987\n",
            "6 \t 0.034657703719707264 0.0336055008443412 0.9881666666666666\n",
            "7 \t 0.031679649687934214 0.029307789449754637 0.9905833333333334\n",
            "8 \t 0.02939708389297842 0.04262454904074548 0.9865\n",
            "9 \t 0.027300760467062597 0.03301980593128671 0.9893333333333333\n",
            "10 \t 0.026473928793646035 0.035839119396050896 0.9885\n",
            "11 \t 0.023163386739496066 0.029958968718293667 0.99075\n",
            "12 \t 0.02105959483590967 0.029320808321702013 0.99175\n",
            "13 \t 0.022244087339064134 0.034128257822452726 0.9886666666666667\n",
            "14 \t 0.020836295155410446 0.043248101731776845 0.9874166666666667\n",
            "15 \t 0.021030654166358707 0.03177806183572557 0.9901666666666666\n",
            "16 \t 0.020479191227638922 0.029738774908351237 0.9915\n",
            "17 \t 0.018169925173629604 0.036206916600887325 0.98925\n",
            "18 \t 0.017829111952627555 0.03255380456251131 0.9903333333333333\n",
            "19 \t 0.017177871678496236 0.031101810198577974 0.9895\n",
            "20 \t 0.01811655074966597 0.0293525822483604 0.9905\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "model =  # your model from previous exercises here.\n",
        "\n",
        "# loss function & optimizer\n",
        "\n",
        "for i in range(1, N_EPOCHS + 1):\n",
        "    \n",
        "    # train loop\n",
        "\n",
        "    # eval loop\n",
        "\n",
        "    # record or print some variables that you want to keep track of during training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may also want to finally check your performance on the test set for comparison with the MLP we trained last PC lab."
      ],
      "metadata": {
        "id": "_5ljjUCHTMZm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVyTsee3Q3cU"
      },
      "source": [
        "### 2.3 Extra: The convolutional autoencoder\n",
        "\n",
        "As a last step, we can implement a convolutional autoencoder.\n",
        "\n",
        "Since we will go from an input image to an output image, we don't need to flatten our tensor at any point in our network. If we do this, our latent space will also be a channel $\\times$ width $\\times$ height tensor, conserving some positional information. The rationale is that by keeping this positional information in our bottleneck, the reconstruction will be made easier.\n",
        "\n",
        "Example:\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/26786663/27525317-b3026976-5a77-11e7-8767-8f4a06e5b696.jpg' width = 600>\n",
        "\n",
        "We can still choose to flatten our bottleneck and then unflatten again in the decoder. This will make us lose all positional information, however.\n",
        "\n",
        "Example:\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*gzJAJDLDavH_W7Zv2M2J7w.png' width = 600>\n",
        "\n",
        "Our encoder can be exactly the same structure as the CNNBackbone we made earlier in this PC lab. For the decoder, we need inverse convolutions (and maybe inverse max pooling) operations to upscale our image again from its compressed representation to its original dimensions.\n",
        "\n",
        "In PyTorch, we have these options:\n",
        "\n",
        "- [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) also visualized [here](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html), the inverse of convolutions, also called deconvolution.\n",
        "- [Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html#torch.nn.Upsample), performing simple non-parametric upscaling of inputs with any of a number of methods such as bilinear or bicubic upscaling.\n",
        "- [MaxUnpool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=maxunpool#torch.nn.MaxUnpool2d), performing the inverse of **a previous** max pooling operation. Essentially putting the maximal elements back in the location/index where they originally appeared before the previous max pooling. As such, this operation can only be used conjoined with a paired max pooling operation. (see examples in documentation).\n",
        "\n",
        "For this PC lab, we simply recommend `ConvTranspose2d` and `Upsample`.\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXTRA EXERCISE:</b> \n",
        "<p> Implement a Convolutional Autoencoder. Use your previously implemented CNNBackbone as an encoder and create a decoder from scratch using the layers discussed above. Try to make the model as symmetric as possible.</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6QR7CkbQ3cV",
        "outputId": "0166e5e8-4c26-4dfb-818f-d39a4e1ec22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your model should decode this shape:\n",
            "torch.Size([8, 16, 2, 2])\n",
            "to 8 x 1 x 28 x 28\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(8, 1, 28, 28)\n",
        "Encoder = CNNBackBone()\n",
        "\n",
        "encoded = Encoder(x)\n",
        "print(\"Your model should decode this shape:\")\n",
        "print(encoded.shape)\n",
        "print(\"to 8 x 1 x 28 x 28\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoY8HRAWQ3cV"
      },
      "outputs": [],
      "source": [
        "class CNNDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        return None # CHANGE THIS TO YOUR OUTPUT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSTrIBPtQ3cW"
      },
      "source": [
        "Testing your decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgIaWa1eQ3cW",
        "outputId": "8dd21da9-b471-4036-f7b5-c9088ae6712a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "CNNDecoder()(encoded).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2V_i3HZQ3cX"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXTRA EXERCISE:</b> \n",
        "<p> Train your Autoencoder using the same code as in last PC lab.</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKJy0xkHQ3cX",
        "outputId": "61b634d4-0527-4c3b-ae5a-cb868146aecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \t 0.11571551163805027 0.02453340083112319\n",
            "2 \t 0.017688279214935997 0.011958929071823755\n",
            "3 \t 0.011684133926096062 0.008336873807013034\n",
            "4 \t 0.008874724563832084 0.007623255327343941\n",
            "5 \t 0.007689311919733882 0.006959689841916164\n",
            "6 \t 0.0070480802860111 0.005824639985958735\n",
            "7 \t 0.00664850828750059 0.0056344154986242454\n",
            "8 \t 0.006354257110661517 0.0049948683017864825\n",
            "9 \t 0.0061384224582773945 0.004995861156222721\n",
            "10 \t 0.005957741623589148 0.0050827311159422\n",
            "11 \t 0.005810819514406224 0.004704256468142072\n",
            "12 \t 0.005689549503692736 0.004562137114194532\n",
            "13 \t 0.005591008405278747 0.004622515708518525\n",
            "14 \t 0.005508731423644349 0.004435152977394561\n",
            "15 \t 0.00543805132426011 0.004392285563983024\n",
            "16 \t 0.005365670891323437 0.004263338590661685\n",
            "17 \t 0.0053026299426953 0.0042711883429437875\n",
            "18 \t 0.005257730442409714 0.00432175178018709\n",
            "19 \t 0.005210241034859791 0.004167653135644893\n",
            "20 \t 0.005169183146208524 0.004296466346830129\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are free to explore all the same code we explored last PC lab for visualization of the data in 2D space and also to generate new samples.\n",
        "\n",
        "Just remember that our latent space does not consist of 1-D vectors anymore, but of 3-dimensional tensors.\n",
        "\n",
        "Keep in mind that the two autoencoders you made may not be comparable if they reconstruct their images to a different number of hidden dimensions. If your MLP autoencoder had a bottleneck of 16 hidden nodes, and your CNN bottleneck has a dimensionality of 8 x 2 x 2, than your MLP compresses the input image by twice as much. A fair evaluation would require a CNN bottleneck of 16 x 1 x 1 or 4 x 2 x 2."
      ],
      "metadata": {
        "id": "xky6WidJTYz3"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "744px",
        "left": "1548px",
        "right": "20px",
        "top": "120px",
        "width": "335px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}