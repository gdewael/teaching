{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PClab12_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xqzgGkp6Z0a"
      },
      "source": [
        "# PC lab: intro to Neural networks & PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjxvlCzY6h4t"
      },
      "source": [
        "<img src='http://www.joshclutter.com/img/bgimage.jpg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNEdF_h4-AmQ"
      },
      "source": [
        "Deep learning is the subfield of machine learning that concerns neural networks with representation learning capabilities. As of recent years, it is arguably the most quickly growing field within machine learning, enjoying major breakthroughs every year. Although the popularity of neural nets is a recent phenomenon, they were first described by Warren McCulloch and Walter Pitts in 1943. Early progress in training competitive neural networks was stalled by a multitude of reasons, such as the limited computer resources, sub-optimal network architectures and the use of smaller datasets. In this PC-lab we will introduce you to the basics of implementing a neural network using standard practices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jqUbY3CO_fY"
      },
      "source": [
        "# Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CX3qibbPBs7"
      },
      "source": [
        "The core unit of every (artificial) neural network is considered the neuron. The neuron can be observed as a switch. It receives **one or more inputs** $\\mathbf{x}$, processes a **weighted sum** $z$ (adding **bias** $b$) that is sent through the **sigmoid activation function $\\sigma()$**, outputting a **single response**  $a$:\n",
        "\n",
        "$$ \n",
        "z = \\sum\\limits_{i=1}^{n}(w_ix_i) + b = \\sum\\limits_{i=0}^{n}(w_ix_i)$$ with $$ x_0 = 1 \\\\\n",
        "$$\n",
        "\n",
        "$$ a = \\sigma(z) $$\n",
        "\n",
        "<img src='https://i.pinimg.com/originals/16/2d/d5/162dd5c265cf8b5ce4bb27e5e022289f.jpg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39M59Se9PlWO"
      },
      "source": [
        "The default recommended activation function is the **Rectified Linear Unit**, or **ReLU**. \n",
        "\n",
        "$$ ReLU(z) = max\\{0,z\\} $$\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Hossam-H-Sultan/publication/333411007/figure/fig7/AS:766785846525952@1559827400204/ReLU-activation-function.png\" style=\"width:15%\">\n",
        "\n",
        "The ReLU function has many properties that make optimization easy using gradient-based methods. It can be seen as a switch giving no response for $z < 0$ and giving a response $z$ for $z > 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-9uaqpP-g6"
      },
      "source": [
        "The most basic artificial neural network is the **feedforward neural network**. There are no feedback connections such as can be found in **recurrent neural networks**. A feedforward neural network is called a network as it is composed out of many inheriting functions making up the model, e.g. $f(\\textbf{x})= f^{(3)}(f^{(2)}(f^{(1)}(\\textbf{x})))$. Neural networks typically are constructed in different layers of neurons in which every neuron is connected with all the neurons of the previous layer, eventually resulting in a set of **output neurons** $\\mathbf{\\hat{\\textbf{y}}}$.\n",
        "\n",
        "To train the network, samples are processed in batches. This allows for faster training and improved convergence of the loss during gradient descent. Advantages of stochastic gradient descent or other optimization algorithms for loss calculation are not discussed in this PC-lab, but have been [extensively discussed](https://ruder.io/optimizing-gradient-descent/) before.\n",
        "Practically, the first fully-connected layer of the network using batch size $B$ is computed by matrix combination of the input $X \\in \\mathbb{R}^{B, D}$ with a set of weights $W^{(1)} \\in \\mathbb{R}^{D, M}$.\n",
        "\n",
        "\\begin{equation}\n",
        "XW^{(1)} =\n",
        "\\begin{bmatrix}\n",
        "1 & x_{0,1} & ...  & x_{0,D-1} & x_{0,D} \\\\\n",
        "1 & x_{1,1} & ... & x_{1,D-1} & x_{1,D} \\\\\n",
        "... & ... & ... & ... & ...\\\\\n",
        "1 & x_{B-1,1} & ... & x_{B-1,D-1} & x_{B-1,D} \\\\\n",
        "1 & x_{B,1} & ...  & x_{B,D-1} & x_{B,D} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "W_{0,0} & W_{0,1} & ...  & W_{0,M-1} & W_{0,M} \\\\\n",
        "W_{1,0} & W_{1,1} & ... & W_{1,M-1} & W_{1,M} \\\\\n",
        "... & ... & ... & ... & ...\\\\\n",
        "W_{D-1,0} & W_{D-1,1} & ... & W_{D-1,M-1} & W_{D-1,M} \\\\\n",
        "W_{D,0} & W_{D,1} & ...  & W_{D,M-1} & W_{D,M} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efc9XDrtQusO"
      },
      "source": [
        "Activation of the first layer:\n",
        "\n",
        "\\begin{equation}\n",
        "A^{(1)} =\n",
        "ReLU(XW^{(1)})\n",
        "\\end{equation}\n",
        "\n",
        "Adding a second layer means doing:\n",
        "\n",
        "\\begin{equation}\n",
        "A^{(2)} =\n",
        "ReLU(A^{(1)}W^{(2)})\n",
        "\\end{equation}\n",
        "\n",
        "And so on ...\n",
        "\n",
        "Note that at the output layer, no ReLUs would be used. For regression, no activation is necessary at the end. For binary classification, a sigmoid would be used (as this bounds the output between 0 and 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG3LPfPU-8ky"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "To implement neural networks with more ease, a few high-level python libraries are available. In this lab, we will use [PyTorch](https://pytorch.org). PyTorch is the most popular library for deep learning as of today. For this course it offers the advantage that it has the most 'pythonic' syntax, to the point where almost all NumPy functions have a PyTorch counterpart.\n",
        "\n",
        "We highly recommend you to use Google Colab for this PC lab, since we will be using GPU resources. Google Colab comes with everything pre-installed. If your computer has a GPU and you want to run code locally: you can find the installation instructions [here](https://pytorch.org/get-started/locally/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1cmzGhe6kJK"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKVm96u6BXev"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "Tensors are the fundamental data structures in PyTorch. They are analogous to NumPy arrays. The difference is that tensors can also run on GPU hardware. GPU hardware is optimized for many small computations. Matrix multiplications, the building blocks of all deep learning, run orders-of-magnitude faster on GPU than on CPU. Let's see how tensors are constructed and what we can do with them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJL-R3elBv9E"
      },
      "source": [
        "x = [[5,8],[9,8]]\n",
        "print(torch.tensor(x))\n",
        "print(np.array(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOjuB-_7B-er"
      },
      "source": [
        "x_numpy = np.array(x)\n",
        "print(torch.from_numpy(x_numpy))\n",
        "\n",
        "x_torch = torch.tensor(x)\n",
        "print(x_torch.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvQP_Y-HBEJt"
      },
      "source": [
        "print(np.random.randn(8).shape)\n",
        "print(np.random.randn(8,50).shape)\n",
        "\n",
        "\n",
        "print(torch.randn(8).shape) # an alternative for .shape in PyTorch is .size()\n",
        "print(torch.randn(8,50).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grsQLtyKBGcq"
      },
      "source": [
        "print(np.zeros((8,50)).shape)\n",
        "print(torch.zeros(8,50).shape) # works with 'ones' as well"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e24OlYbC7nr"
      },
      "source": [
        "print(np.zeros(8).dtype)\n",
        "print(torch.zeros(8).dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp-T8i15DXD7"
      },
      "source": [
        "In PyTorch, the standard data type is `float32`, which is called `float` within its framework. `float64` is called `double`.\n",
        "This is different from the NumPy defaults and naming conventions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFuID3EGDIea"
      },
      "source": [
        "x = torch.randn(8)\n",
        "print(x.dtype)\n",
        "x = x.to(torch.float64)\n",
        "print(x.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhOAtkL-D269"
      },
      "source": [
        "`torch.long` is synonymous to `torch.int64`. The only difference between int32 and int64 is the amount of bytes with which you will store every integer. If you go up to very high numbers, you will get numerical overflow faster with more compressed data types. We recommend you to always use the defaults: `torch.long` and `torch.float`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAeLblceDTBq"
      },
      "source": [
        "x = torch.randint(low=0, high=8, size=(8,), dtype=torch.int32)\n",
        "print(x)\n",
        "print(x.dtype)\n",
        "x = x.to(torch.long)\n",
        "print(x.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNm0VKLzEbxH"
      },
      "source": [
        "Indexing and other operations work as in NumPy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWdkTEzXDtpq"
      },
      "source": [
        "x = torch.randn(8,50,60)\n",
        "print(x.shape)\n",
        "print(x[:4,10:-10].shape)\n",
        "x[0,0,:10] = 0\n",
        "print(x[0,0,:16])\n",
        "\n",
        "print(torch.min(x), torch.max(x), torch.min(torch.abs(x)))\n",
        "# most of these functions are also tensor methods:\n",
        "print(x.min(), x.max(), x.abs().min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIZpsdFHEuTK"
      },
      "source": [
        "print(x.shape)\n",
        "x_cat0 = torch.cat([x, x], dim=0)\n",
        "print(x_cat0.shape)\n",
        "x_cat1 = torch.cat([x, x, x], dim=1)\n",
        "print(x_cat1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPxk_yKfFgUi"
      },
      "source": [
        "Matrix multiplication: let's say we have an input `x`, consisting of 8 samples with 26 features, that we linearly combine with weights `w` to get a single output for every sample:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6LGBkLiFNXK"
      },
      "source": [
        "x = torch.randn(8,26)\n",
        "w = torch.randn(26,1)\n",
        "\n",
        "y_hat = torch.matmul(x, w) # an alternative and equivalent syntax is x @ w\n",
        "print(y_hat)\n",
        "print(y_hat.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az2LZ1hnGDds"
      },
      "source": [
        "Note that matrix multiplication is different from element-wise multiplication. For element-wise, `*` is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEJoq1gsI56b"
      },
      "source": [
        "# Building a neural network in PyTorch\n",
        "\n",
        "Neural networks are initialized through the use of class objects. You have encountered class objects already during this course: sklearn models are all class objects. The difference here is that we will code our own class first, before using it.\n",
        "\n",
        "Many of the functionalities necessary to create [**all types of neural networks**](http://www.asimovinstitute.org/neural-network-zoo/) have [**already been implemented**](http://pytorch.org/docs/master/nn.html).\n",
        "\n",
        "The following code shows the most basic blueprint for a PyTorch neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCWUPuY_I-nq"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ExampleNet(nn.Module):\n",
        "    def __init__(self, hyperparameter_1=50):\n",
        "        super().__init__()\n",
        "\n",
        "        # create class attributes such as layers\n",
        "        # e.g. self.W = nn.Linear(80,1) makes a fully connected layer with 80 input neurons and 1 output neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "        # use your layers here\n",
        "        # e.g. x = self.W(x) uses the fully connected layer from previously (in essence, it just does a matrix multiplication with the weights stored in self.W)\n",
        "        return x\n",
        "\n",
        "ExampleNet() # the value for hyperparameter_1 will become the default = 50\n",
        "ExampleNet(hyperparameter_1=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNGXUjHmUGU_"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p>Implement a neural network with 2 hidden layers. The network should take samples with 80 features as input ($n\\times80$). The first layer should have 40 hidden neurons, the second 20, and the last layer (also called the prediction head or output head) should return 1 number for every input ($n\\times1$).</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huvkE_DDV0Xf"
      },
      "source": [
        "Make use of the previous blueprint for general layout, the [Linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) for linear combination, and the [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) for activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0qyCbkUVTvI"
      },
      "source": [
        "# for testing the model, we generate a random input of 16 samples\n",
        "X = torch.randn(16,80)\n",
        "\n",
        "\n",
        "############\n",
        "# your class Net(nn.Module): here\n",
        "...\n",
        "\n",
        "\n",
        "############\n",
        "\n",
        "\n",
        "model = Net()\n",
        "\n",
        "y_hat = model(X)\n",
        "print(y_hat.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnFpDxSoXAQx"
      },
      "source": [
        "We can now see how models work on GPU. In order to do this, we need to place both the input and the model itself on the 'CUDA' device:\n",
        "\n",
        "Note that to run this code block you need to request access to a GPU from Google Colab. To do this, go to Runtime > Change runtime type > Hardware accelerator = GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy-x6L-bW-6k"
      },
      "source": [
        "model = model.to('cuda')\n",
        "print(X.device)\n",
        "X = X.to('cuda')\n",
        "print(X.device)\n",
        "\n",
        "y_hat = model(X)\n",
        "\n",
        "y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StrSLzKYY0Ur"
      },
      "source": [
        "Notice how the output is still on the cuda device. Also, the output has a `grad_fn` attribute. This grad function will be used by PyTorch automatic differentation module to perform backward passes and compute gradients for every parameter with respect to the loss/cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E5Vv8AkWxH2"
      },
      "source": [
        "# Model training\n",
        "\n",
        "Now that we know how to define a network and implement its forward pass, it's time to make the network learn. To illustrate learning, we will create a toy example. By running the code below, you see a classification problem consisting of 4 half circles that are interleaved with each other. It should be clear from the visualization that simple linear models will not be able to solve this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p8jxC5sW5u4"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "X, y1 = make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=None)\n",
        "X2, y2 = make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=None)\n",
        "X2[:,0] = -X2[:,0]\n",
        "X2[:,0] += 1\n",
        "X2[:,1] += 2\n",
        "\n",
        "X = np.concatenate([X,X2])\n",
        "y = np.concatenate([y1,y2])\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, alpha=0.3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuxbSC5kyAxW"
      },
      "source": [
        "# split the numpy arrays and convert to tensors\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "X_train = torch.from_numpy(X_train).to(torch.float)\n",
        "X_test = torch.from_numpy(X_test).to(torch.float)\n",
        "y_train = torch.from_numpy(y_train).unsqueeze(-1).to(torch.float)\n",
        "y_test = torch.from_numpy(y_test).unsqueeze(-1).to(torch.float)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXwdZ68V5rK1"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p>Copy paste your model from the previous exercise so that it: (1) accepts two-dimensional inputs (instead of 80-dimensional) and (2) has an output bounded from 0 to 1, using a sigmoid function (available from PyTorch). The rest of the model can be kept the same (or experiment as you see fit).</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8xbC5uC5sAV"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(2,40)\n",
        "        self.ReLU1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(40,20)\n",
        "        self.ReLU2 = nn.ReLU()\n",
        "        self.out = nn.Linear(20,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.ReLU1(self.linear1(x))\n",
        "        x = self.ReLU2(self.linear2(x))\n",
        "        x = self.out(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "model = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGq6-eM26Nz7"
      },
      "source": [
        "The most basic training set-up in PyTorch contains the following:\n",
        "- Instantiate the model\n",
        "- Insantiate a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- Instantiate an [optimizer object](https://pytorch.org/docs/stable/optim.html), to which you pass the parameters you want to optimize\n",
        "- Wrap your data splits in a [data loader](https://pytorch.org/docs/stable/data.html)\n",
        "- Iterate through your training data, for every batch:\n",
        "    - reset the gradients\n",
        "    - do forward pass\n",
        "    - compute loss\n",
        "    - backward pass\n",
        "    - update parameters\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE:</b> \n",
        "<p>Complete the missing code in the training loop. Add a validation loop similar to the training loop. Remember that not all steps necessary for training are needed for validation (i.e. everything gradient-related).</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vooxYzKB8Sma"
      },
      "source": [
        "You can find what kind of inputs the BCELoss expects [here](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss). Every loss function will behave differently. For example, [BCELossWithLogits](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss) computes the same loss, but expects your model output to not have been through a sigmoid function already. Instead, this loss function combines the sigmoid operation and BCELoss in one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o97cqvS1UzVl"
      },
      "source": [
        "from time import time\n",
        "\n",
        "model = Net()\n",
        "\n",
        "#model = model.to('cuda') # uncomment to use GPU, only faster for bigger models\n",
        "\n",
        "CrossEntropyLoss = nn.BCELoss() \n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.99) # SGD = stochastic gradient descent\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, pin_memory=True, shuffle=True)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, pin_memory=True, shuffle=True)\n",
        "\n",
        "\n",
        "for n in range(100): # 100 epochs\n",
        "    t = time()\n",
        "\n",
        "    ########## Training loop ##########\n",
        "    model.train() # set model in training mode\n",
        "    train_loss_sum = 0\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        X_batch, y_batch = batch\n",
        "\n",
        "        #X_batch = X_batch.to('cuda')  # uncomment to use GPU, only faster for bigger models\n",
        "        #y_batch = y_batch.to('cuda')  # uncomment to use GPU, only faster for bigger models\n",
        "\n",
        "        y_hat = ... # forward pass\n",
        "        loss = ... # compute loss\n",
        "\n",
        "        loss.backward()   # Calculate gradients\n",
        "        optimizer.step()   # Update weights using defined optimizer\n",
        "        train_loss_sum += loss.item()\n",
        "\n",
        "\n",
        "    ########## Validation loop (with test data here since it's a toy problem) ##########\n",
        "\n",
        "    # we will record these things to do some computations at the end of the epoch\n",
        "    predicted_ys = []\n",
        "    true_ys = []\n",
        "\n",
        "    # all code within this 'with' construct will not keep gradients. This is not necessary in the validation loop:\n",
        "    with torch.no_grad():\n",
        "\n",
        "        model.eval()\n",
        "        test_loss_sum = 0\n",
        "        for i, batch in enumerate(test_dataloader):\n",
        "            X_batch, y_batch = batch\n",
        "\n",
        "            ...\n",
        "\n",
        "            test_loss_sum += ...\n",
        "\n",
        "            predicted_ys.append(y_hat) \n",
        "            true_ys.append(y_batch) \n",
        "\n",
        "        predicted_ys_all = torch.cat(predicted_ys)\n",
        "        true_ys_all = torch.cat(true_ys)\n",
        "        test_acc = torch.sum((predicted_ys_all > 0.5) == true_ys_all).item()/len(true_ys_all)\n",
        "    \n",
        "\n",
        "    print('epoch', n, 'train_loss', np.round(train_loss_sum/len(train_dataloader),4),\n",
        "          'test_loss', np.round(test_loss_sum/len(test_dataloader),4), \n",
        "          'test_acc', np.round(test_acc,4),\n",
        "          'time_elapsed (s)', np.round(time()-t,4), sep=\"\\t\")\n",
        "    t = time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rnFqyvr56-o"
      },
      "source": [
        "During the training process, we have used one part of the data to train on and one part of the data to evaluate on. It is good practice to follow the training process on a non-training split, so that we can see if/when the model starts overfitting. For example, after every train + val epoch, one could perform a check to see if the val accuracy was the best one encountered yet. Based on this criterium, the model can then be saved on disk. This way, after training, only the model with the best validation performance is kept.\n",
        "\n",
        "As a reward for your efforts, this last code block makes a visualization of the output landscape that the model has learnt.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKUeetnmGRum"
      },
      "source": [
        "xx = np.arange(X[:,0].min()-.5, X[:,0].max()+.5, 0.05)\n",
        "yy = np.arange(X[:,1].min()-.5, X[:,1].max()+.5, 0.05)\n",
        "\n",
        "XX, YY = np.meshgrid(xx, yy)\n",
        "inp_dat = np.hstack((XX.reshape(-1,1), YY.reshape(-1,1)))\n",
        "\n",
        "outp_dat = np.zeros(inp_dat.shape[0])\n",
        "with torch.no_grad():\n",
        "    for i in range(0,inp_dat.shape[0],32):\n",
        "        X_i = inp_dat[i:i+32,:]\n",
        "        X_i = torch.from_numpy(X_i).to(torch.float)\n",
        "        y_out = model(X_i)\n",
        "        outp_dat[i:i+32] = y_out.squeeze(-1).numpy()\n",
        "outp_dat = outp_dat.reshape(XX.shape)\n",
        "\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "default_cmap = cm.get_cmap('viridis', 2).colors\n",
        "lighter_cmap = LinearSegmentedColormap.from_list(name='lighter_cmap', colors=np.minimum(default_cmap+.35,1))\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.contourf(XX, YY, outp_dat, levels=25, cmap=lighter_cmap)\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, alpha=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeEacz8CQlJw"
      },
      "source": [
        "<b>EXTRA OPTIONAL EXERCISE:</b> \n",
        "<p>Play around with some hyperparameters that we defined:\n",
        "\n",
        "- learning rate\n",
        "- momentum of SGD\n",
        "- batch size\n",
        "- model architecture\n",
        "\n",
        "In particular, look how fast the model learns without momentum. What is momentum exactly doing?</p>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip1wwq47Qll0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}