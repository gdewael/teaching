{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "744px",
        "left": "1548px",
        "right": "20px",
        "top": "120px",
        "width": "335px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "colab": {
      "name": "PClab013_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSAezf-eH3fV"
      },
      "source": [
        "# PC lab: Convolutional Neural Networks \n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8hMi55H3fX"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/flacout/deep-dream-demo/master/image/deep-dream.jpg\" style=\"width:100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfqwZm0OH3fY"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY5lpFw0H3fZ"
      },
      "source": [
        "Convolutional neural networks caused a major step forward in the performance of image recognition. These networks are mostly identical to standard neural networks, in which features are first learned through multiple (layers of) convolutions. Obtained features are subsequently used as the input for a standard neural network, often performing a classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycK0JrjlH3fZ"
      },
      "source": [
        "### Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0wOnHRDH3fa"
      },
      "source": [
        "A convolution is the iteration of a kernel with size $ M \\times N $ over a given input $ \\textbf{X} $, performing a 2D linear combination of the weights $ W $  of the kernel with the overlapping area of the input. For a normal convolution with single striding and no padding, the output $ y_{ij} $ is equal to:\n",
        "\n",
        "$$ y_{ij} = \\sum_{a=0}^{m-1} \\sum_{b=0}^{n-1} W_{ab} x_{(i+a)(j+b)} $$\n",
        "\n",
        "During a convolution, the kernels slides over the input image to obtain a new image of outputs. The stride of a kernel defines the horizontal and vertical stepsize during iteration. Input data can be padded with multiple layers of a zero-filled border, increasing the output dimensions.\n",
        "\n",
        "It is important to understand that a convolution applies the same operation at every local patch in the input. In this sense, convolutions are useful when you expect the input data to contain regularly appearing **local patterns**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0vz06CJH3fb"
      },
      "source": [
        "**convolution step with M,N = 3; stride = 1 and padding of 1**\n",
        "<img src=\"https://miro.medium.com/max/790/1*1okwhewf5KCtIPaFib4XaA.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUM7ZZU7H3fb"
      },
      "source": [
        "**several other examples. An extended explanation on all types of convolutions can be found [here](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZd0ZIxTH3fc"
      },
      "source": [
        "## Convolutional neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UULBRmWFH3fd"
      },
      "source": [
        "A convolutional neural network usually processes the image with **multiple sequential convolutions**. For each layer, the kernel is evaluated for **all channels** of the input data. It is important to understand that the output depth is correlated to the amount of different kernels, or features, every node has been initialized with. The kernel, although often depicted as only evaluating one layer, actually takes **the sum of all layers (channels)** to obtain an output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFW16Y6xH3fd"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/2560/1*ciDgQEjViWLnCbmX-EeSrA.gif\" style=\"width:100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAP8Iw7H3fe"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg\" style=\"width:100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGSMvqyRH3ff"
      },
      "source": [
        "A classic example of a convolutional neural network applies an activation function (e.g. **ReLU**) on the output of every convolutional layer, after which the activation signals are **maximum pooled**. Maximum pooling reduces the dimensionality of the input, which can be used to reduce the amount of parameters present in a neural network, which in turn reduces overfitting and computational burden. Maximum pooling is also initialized with specific arguments such as kernel size, stride and padding.\n",
        "\n",
        "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvJMCnrmH3ff"
      },
      "source": [
        "The last layers of every convolutional neural network always consist of several fully connected layers. The mathematical description of these layers were discussed in the previous PC lab. One can interpret the convolutional layers as the section of the network in which local patterns are extracted (edges, contours, contrasts,...). These are used as inputs for the fully connected neural netwerk, which combines these features to train the classifier. \n",
        "\n",
        "The following picture shows a visualization of what a CNN extracts at each layer, starting from the first layers on the left going deeper towards the right. This visualization is obtained by optimizing an input image to maximally activate the convolution filters.\n",
        "\n",
        "<img src='https://1.bp.blogspot.com/-icbxyuiDoA0/WgEivsyFIgI/AAAAAAAACKo/jsfMgFlfiVA233zXg8xAH3ZAKOchgLb-wCLcBGAs/s1600/image4.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn6gzl-zH3ff"
      },
      "source": [
        "**The softmax function** takes the n-dimensional output of the model and rescales these values to probabilities that sum up to one. It is typically used as the output layer for multiclass classification.\n",
        "\n",
        "$$ \\sigma(\\hat{y})_j = \\frac{e^{\\hat{y}_j}}{\\sum_{k=1}^{K} e^{\\hat{y}_k}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRYctSDjdqOP"
      },
      "source": [
        "# Structure of the exercises.\n",
        "\n",
        "During this PC-lab you will be introduced with an examplory workflow when training a predictive model for image recognition. More specifically, we will create a <b>convolutional neural network</b> to <b>recognize vehicles from animals</b> using the <b>CIFAR-10</b> dataset in <b>PyTorch</b>.\n",
        "\n",
        "Let's first recap from last week what is needed to train a model in PyTorch:\n",
        "\n",
        "1. Define a model (class) with \\__init__(self, ...) and forward(self, ...) functions\n",
        "2. Instantiate the model\n",
        "3. Insantiate a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "4. Instantiate an [optimizer object](https://pytorch.org/docs/stable/optim.html), to which you pass the parameters you want to optimize\n",
        "5. Wrap your data splits in a [data loader](https://pytorch.org/docs/stable/data.html)\n",
        "6. Perform an epoch, consisting of:\n",
        "    - Iterating through all training data and\n",
        "        - resetting gradients\n",
        "        - forward pass\n",
        "        - compute loss\n",
        "        - backward pass\n",
        "        - update parameters\n",
        "    - Iterating through all validation data and\n",
        "        - forward pass\n",
        "        - compute loss\n",
        "\n",
        "We iterate through validation data at every epoch so we can keep track of the performance throughout training.\n",
        "\n",
        "Last week, we used simple, rudimentary code in order to perform the training steps. This week, we will provide you with code that abstracts away the repetitive steps of this process, allowing you to focus on the fun stuff: implementing models and watching them train.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jLOL7fmH3fj"
      },
      "source": [
        "<img src=\"https://www.researchgate.net/profile/Jean-Elsner/publication/329969195/figure/fig1/AS:708799606317059@1546002403551/Images-from-the-CIFAR-10-13-dataset-and-their-corresponding-classes-CIFAR-10.ppm\" width='25%' align=\"right\">\n",
        "\n",
        "## CIFAR-10 Data loading and preprocessing\n",
        "\n",
        "- 60000 $32 \\times 32$ colour images\n",
        "- 50000 training images + 10000 validation images\n",
        "- 10 classes $\\rightarrow$ 6000 images per class\n",
        "- classes are: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks\n",
        "\n",
        "\n",
        "Collected for **MSc thesis** \n",
        "\n",
        "<ul>\n",
        "<a href=\"learning-features-2009-TR.pdf\">Learning Multiple Layers of Features from Tiny Images</a>, Alex Krizhevsky, 2009.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6uKSVHzH3fk"
      },
      "source": [
        "A list featuring some papers using CIFAR-10 can be found [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#CIFAR-10). To load the CIFAR-10 dataset into our environment, we can use the pytorch module. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sfOeVU9H3fk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "validationset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_IIa078H3fk"
      },
      "source": [
        "## Building the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s2TO0S0_roJ"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    <h2>Exercise:</h2>\n",
        "        <p>Look up the documentation for the (<a href=https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html>2-dimensional convolutional layer</a>). What are the parameters that need to be defined? What kind of dimensions will the input have for different types of parameters? Use the code-block below to experiment, try to get an understanding of how you should define the Conv layer and what it does to your input dimensionality. Do the same experiment for the <a href=https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html>Max Pooling layer</a>. Finally, carefully look at the note at the bottom of the code block.</p>\n",
        "        </div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tIKVJHC_ODQ"
      },
      "source": [
        "example_batch = next(iter(train_loader))\n",
        "X_example, y_example = example_batch\n",
        "print(X_example.size(), y_example.size())\n",
        "\n",
        "\n",
        "test_conv = nn.Conv2d(...)\n",
        "test_pool = nn.MaxPool2d(...)\n",
        "\n",
        "X_after_conv = test_conv(X_example)\n",
        "print(X_after_conv.size())\n",
        "\n",
        "X_after_pool = test_pool(X_example)\n",
        "print(X_after_pool.size())\n",
        "\n",
        "X_after_both = test_pool(test_conv(X_example))\n",
        "print(X_after_both.size())\n",
        "\n",
        "# NOTE: a quick way to chain multiple 'nn.'-type layers is through the use of nn.Sequential():\n",
        "# e.g. ConvPoolBlock = nn.Sequential(nn.Conv2d(...), nn.ReLU(), nn.MaxPool2d(...))\n",
        "# x = ConvPoolBlock(x) can now be used to sequentially execute the conv, relu and maxpool in one operation."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_pWUresH3fl"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    <h2>Exercise:</h2>\n",
        "        <p>Implement a convolutional neural network. The network should have 2 or 3 convolutional layers (<a href=https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html>Conv2d</a>). Use 3x3 or 5x5 kernel sizes. Use Max Pooling  layers with 2x2 kernels to reduce dimensionality after convolutional layers. You are free in choosing the rest of the hyperparameters. At the end of the network, the output should be flattened (<code>x.view(-1, ...)</code> in forward pass) and sent through some <a href=https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear>fully connected layers</a>. The final output of the model should be of size <code>batch x 10</code>, representing the output score for the ten possible classes. The difficulty here is figuring out how much dimensions you have as input to the first fully connected layer (right after flattening). You can use the previous code block in order to help with this.</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "Keep in mind that the larger the model (both in number of layers and size of every layer), the longer training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WBTLeeo_H3fl"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Use this Sequential as blueprint for other layers, remember to fill in the parameters.\n",
        "        self.ConvBlock1 = nn.Sequential(nn.Conv2d(3, ...), nn.ReLU(), nn.MaxPool2d(...))\n",
        "        self.ConvBlock2 = ...\n",
        "        \n",
        "        ... # more if you want\n",
        "\n",
        "        self.FC_layers = nn.Sequential(nn.Linear(..., ...), nn.ReLU(),\n",
        "                                       ... , # more if you want\n",
        "                                       nn.Linear(..., 10))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.ConvBlock1(x)\n",
        "        x = self.ConvBlock2(x)\n",
        "        ... # more if you want\n",
        "\n",
        "        # Flatten array, '.view(...)' works similarly to numpy's '.reshape(...)'\n",
        "        # Fill in the number of dimensions that your first linear layer expects as input.\n",
        "        x = x.view(-1, ...)\n",
        "        x = self.FC_layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ7o92iXH3fl"
      },
      "source": [
        "Notice the similarities between the creation of this neural network and the one created in the previous lab. To be able to call a variant of this network, we can choose to add parameter values to `def __init__(self, .....)`, such as kernel sizes, or the total amount of nodes in each fully connected layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvDwrvSH3fp"
      },
      "source": [
        "## 3. Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfHoercCH3fq"
      },
      "source": [
        "### Helper functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpkMD3SrH3fq"
      },
      "source": [
        "The next step is to train the model. The train_loader and validation_loader object is used to obtain data batches of fixed size. The `fit()` function trains the model for a specified amount of epochs while storing data obtained during training with the `logger()` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otFlllkEH3fq"
      },
      "source": [
        "\n",
        "def fit(model, train_loader, validation_loader, criterion, optimizer, log, epochs=20, device='cpu'):\n",
        "    epoch = 0   # set starting epoch\n",
        "    model = model.to(device)\n",
        "\n",
        "    while epoch<epochs:\n",
        "        print(\"\\nepoch {}\".format(epoch))\n",
        "        epoch +=1\n",
        "        model.train()\n",
        "        for i, data in enumerate(train_loader):   # iterate randomized batches\n",
        "            optimizer.zero_grad()\n",
        "            X_batch, y_batch = data\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_hat = model.forward(X_batch)\n",
        "            loss = criterion(y_hat, y_batch)\n",
        "            loss.backward()   # Calculate gradient\n",
        "            optimizer.step()   # Update weights using defined optimizer\n",
        "            if device != 'cpu':\n",
        "                log.log_metrics(y_batch.data.cpu().numpy(), y_hat.data.cpu().numpy(), loss.item())\n",
        "            else:\n",
        "                log.log_metrics(y_batch.data.numpy(), y_hat.data.numpy(), loss.item())\n",
        "            if (i%100 == 1):\n",
        "                log.output_metrics()\n",
        "        \n",
        "        # Repeat this process for the validation dataset\n",
        "        model.eval()\n",
        "        for i, data in enumerate(validation_loader, 0):\n",
        "            X_batch, y_batch = data\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            y_hat = model.forward(X_batch)\n",
        "            loss = criterion(y_hat, y_batch)\n",
        "            if device != 'cpu':\n",
        "                log.log_metrics(y_batch.data.cpu().numpy(), y_hat.data.cpu().numpy(), loss.item(), validation=True)\n",
        "            else:\n",
        "                log.log_metrics(y_batch.data.numpy(), y_hat.data.numpy(), loss.item(),validation=True)\n",
        "        log.output_metrics(validation=True)\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI-AeEjmH3fr"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <h2><code>class logger()</code> </h2>\n",
        "    <p><code>logger()</code> has been implemented as a convenient way to store model metrics throughout the training process.  An object of this class can be created before training and is used for calculating, storing, printing and plotting model metrics. The object does only store the metrics as defined at initialization. Feel free to add yor own metrics to the class.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR_ESTQNH3fr"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "class logger(object):\n",
        "    def __init__(self, metrics, max_i):\n",
        "        self.i = [0,0]\n",
        "        self.max_i = max_i\n",
        "        self.log_loss, self.log_auc, self.log_acc = False, False, False\n",
        "        self.metrics = {\"train\":{}, \"validation\":{}}\n",
        "        if \"loss\" in metrics:\n",
        "            self.log_loss = True \n",
        "            self.metrics[\"train\"].update({\"loss\":[0]})\n",
        "            self.metrics[\"validation\"].update({\"loss\":[0]})\n",
        "        if \"acc\" in metrics:\n",
        "            self.log_acc = True\n",
        "            self.metrics[\"train\"].update({\"acc\":[0]})\n",
        "            self.metrics[\"validation\"].update({\"acc\":[0]})\n",
        "        \n",
        "    def log_metrics(self, y_true, y_hat, loss, validation=False):\n",
        "        if validation:\n",
        "            sw = 1\n",
        "            sw_str = \"validation\"\n",
        "        else:\n",
        "            sw = 0\n",
        "            sw_str = \"train\"\n",
        "        self.i[sw] += 1\n",
        "            \n",
        "        if self.log_loss:\n",
        "            update = (self.metrics[sw_str][\"loss\"][-1]*(self.i[sw]-1)+loss)/self.i[sw]\n",
        "            self.metrics[sw_str][\"loss\"].append(update)\n",
        "        if self.log_acc:\n",
        "            acc = sum(y_hat.argmax(axis=1) == y_true)/len(y_true)\n",
        "            update = (self.metrics[sw_str][\"acc\"][-1]*(self.i[sw]-1)+acc)/self.i[sw]\n",
        "            self.metrics[sw_str][\"acc\"].append(update)\n",
        "        \n",
        "    def output_metrics(self, validation=False):\n",
        "        data = \"validation\" if validation else \"train\"\n",
        "        if validation:\n",
        "            print_str = \"\\n{:<10s}:\\t100.0%\".format(data)\n",
        "        else:\n",
        "            print_str = \"\\r{:<10s}:\".format(data)\n",
        "            print_str += \"\\t{:4.2f}%\".format((self.i[0]%self.max_i)/self.max_i*100)\n",
        "        for k, v in self.metrics[data].items():\n",
        "            print_str += \"\\t{}: {:5.3f}\".format(k, v[-1])\n",
        "        print(print_str, end = \"\")\n",
        "                \n",
        "    def plot_metrics(self):\n",
        "        fig, axes = plt.subplots(len(self.metrics[\"train\"]),\n",
        "                               2, figsize=(12,6*len(self.metrics[\"train\"])))\n",
        "        for i, dict_0 in enumerate(self.metrics.items()):\n",
        "            for j, dict_1 in enumerate(dict_0[1].items()):\n",
        "                axes[j,i].plot(range(len(dict_1[1][1:])),dict_1[1][1:])\n",
        "                axes[j,i].set_title(\"{} {}\".format(dict_0[0], dict_1[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpViMwavH3fs"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sMsKR1rH3fs"
      },
      "source": [
        "Now that we have loaded our data, defined our convolutional neural network, and created all the functions necessary for training, the actual training process can start. For this instance, we will use the cross entropy loss to optimize our model. Notice how `nn.CrossEntropyLoss()` incorporaties the softmax function on the inputs. [Adam](http://sebastianruder.com/optimizing-gradient-descent/index.html#adam) is used to determine the step size using the gradient of the loss with respect to the weights. Adam is currently often considered the best option for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Sf73UyL3H3fs"
      },
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "log = logger(metrics=[\"loss\",\"acc\"], max_i =len(train_loader))\n",
        "fit(model, train_loader, validation_loader, criterion, optimizer, log, epochs=20, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w59k-K7HKbqU"
      },
      "source": [
        "Is this performance in line with your expectations? Think about how the input dimensionality (number of pixels in every image) impacts performance and runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snueHtqBH3fs"
      },
      "source": [
        "## 4. Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duMz35wRH3fs"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U0fbDHcH3fs"
      },
      "source": [
        "After training we can plot saved metrics using the `logger.plot_metrics()` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI90fjkeH3ft"
      },
      "source": [
        "log.plot_metrics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyEEcbPrH3fu"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h2>Exercise:</h2>\n",
        "        <p><b>Evaluate</b> the metrics after 15-20 epochs. Should the model train longer?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgLUVjGrH3fu"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h2>Exercise:</h2>\n",
        "        <p><b>Try</b> different optimizer functions and compare the different loss curves. Additionally, <b>try</b> dropout after the linear layers, compare training dynamics.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEXivFm5H3fu"
      },
      "source": [
        "### Confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49u4g1_IH3fu"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h2>Exercise:</h2>\n",
        "        <p><b>Write out </b> some code to obtain the confusion matrix of the predictions on the validation data. (Second code block under this)</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAtcEM3cREq8"
      },
      "source": [
        "model.eval()\n",
        "device = model.ConvBlock1[0].weight.device # were you working on cpu or cuda?\n",
        "\n",
        "y_hat_all = []\n",
        "y_true_all = []\n",
        "\n",
        "for i, data in enumerate(validation_loader):\n",
        "    X_batch, y_batch = data\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_hat = model.forward(X_batch)\n",
        "    if device != 'cpu':\n",
        "        y_hat_all.append(y_hat.cpu())\n",
        "    else:\n",
        "        y_hat_all.append(y_hat)\n",
        "    y_true_all.append(y_batch)\n",
        "y_hat_all = torch.cat(y_hat_all)\n",
        "y_true_all = torch.cat(y_true_all)\n",
        "print(y_hat_all[:5])\n",
        "print(y_true_all[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em55DW5sH3fu"
      },
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def confusion_matrix_from_variables(y_hat, y_true):\n",
        "    ... # convert y_hat to a format that works for confusion_matrix\n",
        "    matrix = confusion_matrix(y_true, y_hat)\n",
        "    return matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMP1VJpsH3fu"
      },
      "source": [
        "confusion_matrix_from_variables(y_hat_all, y_true_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGlRq5_nH3fv"
      },
      "source": [
        "### Sample predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4HCQtiLH3fv"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    fig, ax = plt.subplots(1,1,figsize=(20,20))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)),)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upuHKk8qH3fv"
      },
      "source": [
        "# creating a shuffled version of the validation loader. Now, every time you run this cell you will get different images\n",
        "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=32,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "dataiter = iter(validation_loader)\n",
        "images, labels = dataiter.next()\n",
        "imshow(torchvision.utils.make_grid(images[:8]),)\n",
        "outputs = model.forward(images.to(device))\n",
        "predicted = torch.max(outputs.data, 1)[1]\n",
        "print('\\t'.join(np.array(classes)[predicted.cpu().numpy()[:8]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khJmHFwuH3fx"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "    <h2>EXTRA (optional) Exercise:</h2>\n",
        "        <p><b>Optimize</b> the model in any way you can. You can change to the architecture of the model, vary the hyperparameters and add any of the many optimization techniques found in literature. </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4GGsKZyTiLO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}