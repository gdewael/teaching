{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Lab 2: Nearest neighbour and data preprocessing\n",
    "Predictive modelling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In our previous lab session, we explored the iris dataset. In this dataset, there are no missing values in the variable \"Species\". But what if there are new observations without a value in the \"Species\" variable? So, imagine the case that for a 'new' iris flower we know the values for the other columns (sepal/petal length and height) but \n",
    "it is unknown to which of the three species it belongs to (dataset irisNA.csv). A natural task would be\n",
    "to try to guess to which species each of the new flowers belongs to. This task\n",
    "(or problem) is called a classification problem in machine learning. In this practical exercise session, a first\n",
    "simple algorithm that provides an answer to this problem is described.\n",
    "### Notations and vocabulary\n",
    "In the iris dataset (iris120.csv), each instance (each flower is an instance) is described by five properties:\n",
    "the species it belongs to, the width of its petals, the length of its petals, the width of its sepals and the\n",
    "length of its sepals. In this PC-lab, for simplicity, only species, sepal length and sepal width will be used.\n",
    "These properties can be seen as variables, and for a given flower, each of these variables takes a specific\n",
    "value. In a classification setting, the aim is to predict the value of one of the variables (here the species),\n",
    "based on the value of the other variables (here petal width and length). The variable of which the values\n",
    "have to be predicted is called the output variable and the variables used to make this prediction are called\n",
    "the input variables or features. A dataset consists of a set of observations of input-output couples $(\\boldsymbol{x}, y)$. In this dataset, the observed values\n",
    "for the features of the $i$-th instance are denoted $\\boldsymbol{x_i}$ $= (x_{i1}, ... , x_{ip})^T$ , where $p$ the number of features, and the observed value of its output\n",
    "is denoted $y_i$. Using this notation, a training dataset $T$ containing $n$ instances can be written as $$T = \\{(\\boldsymbol{x_1}, y_1), ... , (\\boldsymbol{x_n}, y_n)\\}.$$\n",
    "Using this dataset, we will try to build a model (generally denoted $f$) that is able to predict the value of the\n",
    "output variable, based on the value of the input variables. When this output variable is nominal, this process\n",
    "is called a classification problem.    \n",
    "In the iris problem, both input variables take real values $(\\boldsymbol{x_i} \\in \\mathbb{R}^2)$. The output variable, however, is\n",
    "nominal, it takes values from a finite set $\\{setosa, versicolor, virginica\\}$. Because of this, the model $\\textit{f}$ we are\n",
    "looking for is one which performs a mapping $$\\textit{f} : \\mathbb{R}^2 \\rightarrow  \\{setosa, versicolor, virginica\\}.$$   \n",
    "### Nearest neighbour for classification\n",
    "Several techniques exist that are capable of deriving classification models from data. A very simple one is\n",
    "the nearest neighbour model. This model departs from the assumption that instances whose features are\n",
    "highly similar, are likely to have the same labels. The one nearest neighbour (1-NN) model applies this\n",
    "idea in its most extreme form: the label for an instance (with unknown label) is predicted as the label of the\n",
    "closest instance in the training dataset.   \n",
    "To be able to select the ‘closest’ instance in the training dataset, a distance measure has to be defined. In\n",
    "this text, we will use $d(\\boldsymbol{x_i}, \\boldsymbol{x_j})$ to denote the distance between two feature vectors $\\boldsymbol{x_i}$ and $\\boldsymbol{x_j}$. As a simple\n",
    "distance measure, the Euclidean distance can be used\n",
    "$$d_E(\\boldsymbol{x_i}, \\boldsymbol{x_j}) = \\sqrt{\\sum_{k=1}^{p} (x_{i,k} - x_{j,k})^2}$$\n",
    "Using this distance function, the nearest neighbour algorithm performs the following steps:\n",
    "1. For an instance with unknown label and known feature vector $\\boldsymbol{x}$, calculate the distance to each instance in the dataset: $d_E(\\boldsymbol{x}, \\boldsymbol{x_i})$ where $i = 1, ... ,n.$\n",
    "2. Select the closest instance and take its label as the prediction for the unknown label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **Load the dataset iris120.csv in to the memory and select the columns 'Sepal.Length', 'Sepal.Width', and 'Species'. Additionally, load the set of unclassified\n",
    "instances (irisNA.csv) and select the same columns. Both datasets should be loaded as data frames. **\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Species\n",
       "0           5.4          3.9      NaN\n",
       "1           5.0          3.4      NaN\n",
       "2           5.8          4.0      NaN\n",
       "3           5.4          3.9      NaN\n",
       "4           4.4          3.0      NaN"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "iris120 = pd.read_csv('iris120.csv')\n",
    "irisNA = pd.read_csv('irisNA.csv')\n",
    "\n",
    "iris120_2col = iris120[['Sepal.Length', 'Sepal.Width', 'Species']]\n",
    "irisNA_2col = irisNA[['Sepal.Length', 'Sepal.Width', 'Species']]\n",
    "irisNA_2col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **Implement the nearest neighbour algorithm for the iris problem in a function called nnIrisPredict.\n",
    "Use this function to predict the species of unknown flowers irisNA.csv in the dataset. Make sure\n",
    "your function has the following structure:**\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginica'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nnIrisPredict(featuresNewInstance, trainDataset):\n",
    "#...\n",
    "    dist = np.sqrt((featuresNewInstance['Sepal.Length'] - trainDataset.iloc[:]['Sepal.Length'])**2+(featuresNewInstance['Sepal.Width']\n",
    "                                                                                                - trainDataset.iloc[:]['Sepal.Width'])**2)\n",
    "    nn = np.argmin(dist)\n",
    "    label = trainDataset.iloc[nn]['Species']\n",
    "    return (label)\n",
    "\n",
    "new_observation = irisNA_2col.iloc[10][0:2]\n",
    "train = iris120_2col\n",
    "nnIrisPredict(new_observation, train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where trainDataset is a data frame (containing flowers with known species label) with columns\n",
    "\"Sepal.Length\", \"Sepal.Width\" and \"Species\". featuresNewInstance is a vector with the sepal\n",
    "length in the first position and the sepal width in the second position. Label should be one of the\n",
    "strings \"setosa\", \"versicolor\" and \"virginica\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nearest neighbour algorithm for regression\n",
    "In the previous section, the output was a nominal variable (no numerical values, specific classes). When the output is real-valued, the prediction problem is called a regression problem. As with nominal outputs, the nearest neighbour algorithm can\n",
    "be used to predict the output variable of unlabeled instances. The algorithm is identical to the one for\n",
    "classification, however, the prediction will be the real-valued label of the closest instance in the training\n",
    "dataset instead of its class label.\n",
    "\n",
    "## Data preprocessing\n",
    "In this section, some elementary data preprocessing steps are described.\n",
    "### Dummy encoding of nominal variables\n",
    "The basic nearest neighbour algorithm implemented in the previous assignment can only be used with\n",
    "numerical features. However, often types of variables such as nominal variables or ordinal variables are\n",
    "present. A simple solution to this problem exists in using a dummy encoding for each nominal variable.\n",
    "When a variable $\\boldsymbol{x^i}$ is nominal with $k$ values, it is replaced by $k$ new binary variables. As an example,\n",
    "consider the weather dataset, here the variable \"Outlook\" ($\\boldsymbol{x^1}$) has three values: Sunny, Overcast and Rainy.\n",
    "Each of these values is represented by a dummy variable: $\\boldsymbol{x^{1a}}$, $\\boldsymbol{x^{1b}}$ and $\\boldsymbol{x^{1c}}$ with the following values:   \n",
    "  *  $x^{1a} = 1$ if $x^1 = \"Sunny\"$ and  $x^{1a} = 0$ otherwise\n",
    "  * $x^{1b} = 1$ if $x^1 = \"Overcast\"$ and $x^{1b} = 0$ otherwise\n",
    "  * $x^{1c} = 1$ if $x^1 = \"Rainy\"$ and $x^{1c} = 0$ otherwise\n",
    "  \n",
    "See the following example in python. In this example, we use  the Abalone dataset (abaloneTrain700.csv) which contains measurements of physical properties of several abalone (an edible sea snail) specimen. Using these physical properties, the aim is to build a predictive model for the age of these animals (more information concerning this dataset can be found in [abalone.info](https://archive.ics.uci.edu/ml/datasets/Abalone)). In the following example, we replace the nominal variable 'sex' with 3 dummy variables (as many as the values it takes). In python, there are functions such as the [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function which are used for this purpose. So firstly, we create the dummy variables, then we concatenate them with the original dataset and finally we remove the original variable form the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>wholeWeight</th>\n",
       "      <th>shuckedWeight</th>\n",
       "      <th>visceraWeight</th>\n",
       "      <th>shellWeight</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.2975</td>\n",
       "      <td>0.6035</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.4485</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.1305</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height  wholeWeight  shuckedWeight  visceraWeight  \\\n",
       "0   I   0.665     0.500   0.170       1.2975         0.6035         0.2910   \n",
       "1   F   0.460     0.365   0.115       0.4485         0.1650         0.0830   \n",
       "2   F   0.560     0.445   0.180       0.9030         0.3575         0.2045   \n",
       "3   I   0.395     0.300   0.090       0.2790         0.1340         0.0490   \n",
       "4   I   0.530     0.400   0.145       0.5550         0.1935         0.1305   \n",
       "\n",
       "   shellWeight  age  \n",
       "0       0.3595    9  \n",
       "1       0.1700   14  \n",
       "2       0.2950    9  \n",
       "3       0.0750    8  \n",
       "4       0.1950    9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('abaloneTrain700.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   F  I  M\n",
      "0  0  1  0\n",
      "1  1  0  0\n",
      "2  1  0  0\n",
      "3  0  1  0\n",
      "4  0  1  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>wholeWeight</th>\n",
       "      <th>shuckedWeight</th>\n",
       "      <th>visceraWeight</th>\n",
       "      <th>shellWeight</th>\n",
       "      <th>age</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.665</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.2975</td>\n",
       "      <td>0.6035</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.460</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.4485</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.560</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.395</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.1305</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  diameter  height  wholeWeight  shuckedWeight  visceraWeight  \\\n",
       "0   0.665     0.500   0.170       1.2975         0.6035         0.2910   \n",
       "1   0.460     0.365   0.115       0.4485         0.1650         0.0830   \n",
       "2   0.560     0.445   0.180       0.9030         0.3575         0.2045   \n",
       "3   0.395     0.300   0.090       0.2790         0.1340         0.0490   \n",
       "4   0.530     0.400   0.145       0.5550         0.1935         0.1305   \n",
       "\n",
       "   shellWeight  age  F  I  M  \n",
       "0       0.3595    9  0  1  0  \n",
       "1       0.1700   14  1  0  0  \n",
       "2       0.2950    9  1  0  0  \n",
       "3       0.0750    8  0  1  0  \n",
       "4       0.1950    9  0  1  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('abaloneTrain700.csv')\n",
    "dummies = pd.get_dummies(data.sex) # create dummies\n",
    "print(dummies.head())\n",
    "data_with_dummies = pd.concat([data, dummies], axis=1) #  concatenate them with the dataset\n",
    "data_with_dummies = data_with_dummies.drop(['sex'], axis=1) # remove the original sex column\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "Missing values are commonly encountered in data mining studies. Often, missing values are imputed\n",
    "(replaced by a value). Several techniques exist to choose this value. A simple, but often used method\n",
    "is mean imputation. Here, each missing value is replaced by the mean of the observed values for\n",
    "that variable. More advanced methods exist of building separate models to predict the missing\n",
    "values.\n",
    "When implementing the mean imputation, the [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) of scikit-learn library might be handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_dummies.isnull().values.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the data\n",
    "In realistic datasets, most features have different means and standard deviations. For the nearest\n",
    "neighbour algorithm, it can easily be seen that features with a high standard deviation will be more\n",
    "influential than features with a lower standard deviation. In most cases, this is unwanted since it\n",
    "is not known in advance which features are most important. To overcome this problem, features\n",
    "are often standardized. The standardized version of $x^i$ can be obtained as   \n",
    "$$\\frac{x^i - \\mu_i}{\\sigma_i}$$    \n",
    "where $\\mu_i$ and $\\sigma_i$ represent the sample mean and standard deviation of $\\boldsymbol{x^i}$.\n",
    "The [Scaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) of scikit-learn can be used to perform this standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.27945836,  1.01101707,  0.88165536, ..., -0.63466969,\n",
       "         1.39770759, -0.77341678],\n",
       "       [-0.53327082, -0.42969941, -0.61663338, ...,  1.57562275,\n",
       "        -0.71545723, -0.77341678],\n",
       "       [ 0.35098732,  0.4240585 ,  1.15407149, ...,  1.57562275,\n",
       "        -0.71545723, -0.77341678],\n",
       "       ...,\n",
       "       [ 0.57205185,  0.53077824,  0.20061502, ..., -0.63466969,\n",
       "        -0.71545723,  1.29296393],\n",
       "       [-0.09114175, -0.16290006, -0.07180111, ..., -0.63466969,\n",
       "        -0.71545723,  1.29296393],\n",
       "       [-2.16914836, -2.19057509, -1.97871405, ..., -0.63466969,\n",
       "         1.39770759, -0.77341678]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = data_with_dummies['age'] # keep the target variable\n",
    "data_with_dummies = data_with_dummies.drop(['age'], axis=1) # remove it from the feature set\n",
    "data_with_dummies.head()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_with_dummies)\n",
    "data_scaled = scaler.transform(data_with_dummies)\n",
    "\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **To prepare the datasets abaloneTrain700.csv and abaloneTest700.csv for analysis, perform\n",
    "the preprocessing steps described in the previous section (more information concerning this\n",
    "dataset can be found in [abalone.info](https://archive.ics.uci.edu/ml/datasets/Abalone)).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   F  I  M\n",
      "0  1  0  0\n",
      "1  0  1  0\n",
      "2  0  0  1\n",
      "3  0  1  0\n",
      "4  0  1  0\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('abaloneTest700.csv')\n",
    "test_dummies = pd.get_dummies(test.sex) # create dummies\n",
    "print(test_dummies.head())\n",
    "test_with_dummies = pd.concat([test, test_dummies], axis=1) #  concatenate them with the dataset\n",
    "test_with_dummies = test_with_dummies.drop(['sex'], axis=1) # remove the original sex column\n",
    "test_with_dummies.head()\n",
    "test_y = test_with_dummies['age'] # keep the target variable\n",
    "test_with_dummies = test_with_dummies.drop(['age'], axis=1) # remove it from the feature set\n",
    "test_with_dummies.head()\n",
    "\n",
    "test_scaled = scaler.transform(test_with_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest-neighbours\n",
    "A simple extension of the nearest neighbour algorithm consists of taking more than only the nearest\n",
    "neighbour into account. Let $N_k(\\boldsymbol{x}) \\subset T$ be the k nearest neighbours of an instance with feature\n",
    "vector $\\boldsymbol{x}$. The k-nearest neighbour prediction $Y(\\boldsymbol{x})$ can be determined as follows\n",
    "1. For classification problems:  $Y(\\boldsymbol{x})$ is set as the label that occurs most often in $N_k(\\boldsymbol{x})$ (the\n",
    "mode).\n",
    "2. For regression problems, the average of the $k$ nearest outputs can be taken $Y(\\boldsymbol{x}) = \\frac{1}{k}\\sum_{\\boldsymbol{x_i} \\in N_k(\\boldsymbol{x})} y_i$    \n",
    "\n",
    "### k-nearest-neighbours in python\n",
    "As with the 1-nearest neighbour algorithm, it is possible to implement a version of the k-nearest neighbours\n",
    "algorithm in python. However, an alternative is to use a pre-implemented version of this\n",
    "algorithm. For most popular machine learning algorithms, these functions are packed in specific python\n",
    "“packages\". An implementation of\n",
    "the k-nearest-neighbours algorithm is available in the [scikit-learn](http://scikit-learn.org/stable/) library (as well). \n",
    "You can load and see more info about the usage of this function by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsRegressor in module sklearn.neighbors._regression:\n",
      "\n",
      "class KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      " |  KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |  \n",
      " |  Regression based on k-nearest neighbors.\n",
      " |  \n",
      " |  The target is predicted by local interpolation of the targets\n",
      " |  associated of the nearest neighbors in the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <regression>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.9\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, default=5\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |      Uniform weights are used by default.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, default=30\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : int, default=2\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : str or callable, default='minkowski'\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      " |      list of available metrics.\n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      " |      must be square during fit. X may be a :term:`sparse graph`,\n",
      " |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      " |  \n",
      " |  metric_params : dict, default=None\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  effective_metric_ : str or callable\n",
      " |      The distance metric to use. It will be same as the `metric` parameter\n",
      " |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      " |      'minkowski' and `p` parameter set to 2.\n",
      " |  \n",
      " |  effective_metric_params_ : dict\n",
      " |      Additional keyword arguments for the metric function. For most metrics\n",
      " |      will be same with `metric_params` parameter, but may also contain the\n",
      " |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      " |      'minkowski'.\n",
      " |  \n",
      " |  n_samples_fit_ : int\n",
      " |      Number of samples in the fitted data.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      " |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      " |  >>> neigh.fit(X, y)\n",
      " |  KNeighborsRegressor(...)\n",
      " |  >>> print(neigh.predict([[1.5]]))\n",
      " |  [0.5]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  NearestNeighbors\n",
      " |  RadiusNeighborsRegressor\n",
      " |  KNeighborsClassifier\n",
      " |  RadiusNeighborsClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      " |     different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsRegressor\n",
      " |      sklearn.neighbors._base.KNeighborsMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      sklearn.neighbors._base.NeighborsBase\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the k-nearest neighbors regressor from the training dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : KNeighborsRegressor\n",
      " |          The fitted k-nearest neighbors regressor.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the target for the provided data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n",
      " |          Target values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors required for each sample. The default is the\n",
      " |          value passed to the constructor.\n",
      " |      \n",
      " |      return_distance : bool, default=True\n",
      " |          Whether or not to return the distances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NearestNeighbors\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples)\n",
      " |      NearestNeighbors(n_neighbors=1)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      " |      (array([[0.5]]), array([[2]]))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False)\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |          For ``metric='precomputed'`` the shape should be\n",
      " |          (n_queries, n_indexed). Otherwise the shape should be\n",
      " |          (n_queries, n_features).\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors for each sample. The default is the value\n",
      " |          passed to the constructor.\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      " |          `n_samples_fit` is the number of samples in the fitted data\n",
      " |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      " |          The matrix is of CSR format.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X)\n",
      " |      NearestNeighbors(n_neighbors=2)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[1., 0., 1.],\n",
      " |             [0., 1., 1.],\n",
      " |             [1., 0., 1.]])\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination :math:`R^2` of the\n",
      " |      prediction.\n",
      " |      \n",
      " |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      " |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      " |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      " |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      " |      can be negative (because the model can be arbitrarily worse). A\n",
      " |      constant model that always predicts the expected value of `y`,\n",
      " |      disregarding the input features, would get a :math:`R^2` score of\n",
      " |      0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "help(neighbors.KNeighborsRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the help window, in scikit-learn, an estimator for classification/regression is a Python object that implements the methods fit(X, y) and predict(T). The constructor of an estimator takes as arguments the parameters of the model (in our case the basic parameters are the number of neighbours and the distance metric). So the first step is to create a KNN instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsRegressor(n_neighbors=5, metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our estimator instance `knn`. It now must be fitted to the model, that is, it must learn from the model. This is done by passing our training set to the fit method. As a training set, let us use all the examples of the abalone dataset we loaded before as training set apart from the last one. We select this training set with the [:-1] Python syntax, which produces a new array that contains all but the last entry of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "          metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(data_scaled[:-1], y[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can predict new values, in particular, we can ask to the estimator which is the class of our last example of the dataset, which we have not used to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = knn.predict(data_scaled[-1].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prediction quality\n",
    "To test the performance of the nearest neighbour algorithm, an option is to use test data with\n",
    "known labels and to compare the predictions with these labels. In this case, we have two different\n",
    "datasets, a training set T and a test set T$^*$. In case of a regression problem, the mean of squared\n",
    "residuals is commonly used to evaluate the quality of a model. This measure is calculated as follows:   \n",
    "1. Mean of squared residuals on test set:\n",
    "$$Err_{T^*} = \\frac{1}{|T^*|}\\sum_{\\boldsymbol{x_i} \\in T^*} (Y(\\boldsymbol{x_i}) - y_i)^2$$\n",
    "2. Additionally, the error on the training data itself can be computed:   \n",
    "$$Err_{T} = \\frac{1}{|T|}\\sum_{\\boldsymbol{x_i} \\in T} (Y(\\boldsymbol{x_i}) - y_i)^2$$\n",
    "\n",
    "Naturally, it is desirable to keep the mean of squared residuals as small as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the mean squared error of the prediction we get before (for the last example of the abalone dataset) by calculating it ourselves or by using the [mean_squared_error()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6399999999999997"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_pred, [y.iloc[-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **The abalone dataset was split in two subsets: abaloneTrain700.csv and abaloneTest700.csv.\n",
    "Build a 3-nearest-neighbour classifier and use it to obtain\n",
    "predicted values for the age of the abalone specimen. Calculate the mean of squared residuals\n",
    "for these predictions, try to obtain the mean of squared residuals on the training data as well.\n",
    "Which one is the smallest ?**   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of the test set is  6.481746031746033\n",
      "The mean squared error of the training set is  3.5119047619047614\n"
     ]
    }
   ],
   "source": [
    "knn = neighbors.KNeighborsRegressor(n_neighbors=3, metric = 'euclidean')\n",
    "knn.fit(data_scaled, y)\n",
    "y_pred_test = knn.predict(test_scaled)\n",
    "y_pred_train = knn.predict(data_scaled)\n",
    "mse_test = mean_squared_error(y_pred_test, test_y)\n",
    "mse_train = mean_squared_error(y_pred_train, y)\n",
    "print('The mean squared error of the test set is ', mse_test)\n",
    "print('The mean squared error of the training set is ', mse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **Plot the mean of squared residuals as a function of the number of neighbours k which is\n",
    "taken into account, where k ranges between 1 and 50 (step size equal to 3). **\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1481d8d0b70>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGONJREFUeJzt3XtwXOd53/Hvs/fFZRcEARAARQkkzSgUZUmRGUa+pa5l17JpW3bidmzXtXKZatrxRFbStFXtZHyZiRsnaZp27Haqkd1omlq2k/iiOGliRVZtubUu0K0kRdMSb7IIigBBgrjfn/5xzoIgCJAUsMDinPP7zGDOZQ+w7yEXP7589n33NXdHRESiL1XrBoiISHUo0EVEYkKBLiISEwp0EZGYUKCLiMSEAl1EJCYU6CIiMaFAFxGJCQW6iEhMZNbyyVpaWryrq2stn1JEJPKeeuqp0+7eernr1jTQu7q66O7uXsunFBGJPDM7fiXXqeQiIhITCnQRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISExEItC/9cwJ/uyxKxqGKSKSWJcNdDP7spn1mtn+eeeazewhM3sh3G5YzUb+9b6T/I8fKdBFRC7lSnrofwrctuDcPcDD7r4DeDg8XjWd5QI958ZW8ylERCLvsoHu7j8Aziw4fTtwf7h/P/C+KrfrAu3lIkPj0wxPTK/m04iIRNpya+ib3P0kQLhtW+pCM7vTzLrNrLuvr29ZT9ZRLgDwyrnxZX2/iEgSrPqbou5+r7vvdvfdra2X/bCwRbUr0EVELmu5gX7KzDoAwm1v9Zp0sc5yEUB1dBGRS1huoD8I3BHu3wF8uzrNWVxbKQ+ohy4icilXMmzxAeBHwLVm9rKZ/Trw+8DbzewF4O3h8aopZNNsrM9xUoEuIrKkyy5w4e4fWuKhW6vclkvqaCpwUiUXEZElRWKmKEB7qaiSi4jIJUQm0DvKBZVcREQuITKB3l4ucG5sitFJTS4SEVlMZAK9sykYi65euojI4iIT6O2lYCy66ugiIouLTKBXpv+rhy4isrjIBPr56f8auigispjIBHohm6a5PkePeugiIouKTKADtJcKqqGLiCwhUoGusegiIkuLVqBr+r+IyJKiFejlIgOjU4xNztS6KSIi606kAr29FI50GVTZRURkoUgF+vmx6Cq7iIgsFK1Abwpmi54cUA9dRGShSAW6Si4iIkuLVKAXc2ma6rIquYiILCJSgQ5BL10lFxGRi0Uu0DubippcJCKyiMgFenu5oBq6iMgiIhfoHaUCZ0YmGZ/S5CIRkfmiF+hNWuhCRGQx0Qt0LXQhIrKoyAX63EIXgxq6KCIyX+QCXT10EZHFRS7Q63IZysWsxqKLiCwQuUAHLXQhIrKYSAZ6MBZdNXQRkfkiGegd5aJKLiIiC0Q00Av0a3KRiMgFIhnolaGLvYMTNW6JiMj6EclA18pFIiIXi2ighysXaaSLiMicSAZ6uyYXiYhcJJKB3pDP0FjI8IpKLiIicyIZ6KDJRSIiC60o0M3s42a238wOmNnd1WrUlegoa+UiEZH5lh3oZnY98M+BPcCNwLvNbEe1GnY56qGLiFxoJT30ncBj7j7q7tPA94H3V6dZl9deLnB6eILJ6dm1ekoRkXVtJYG+H/hFM9toZnXAu4At1WnW5XWGQxdPaX1RERFgBYHu7geBzwMPAX8LPAdML7zOzO40s24z6+7r61t2QxfS0EURkQut6E1Rd/+Su9/s7r8InAFeWOSae919t7vvbm1tXcnTXUCzRUVELpRZyTebWZu795rZ1cAvAa+vTrMub24pOvXQRUSAFQY68JdmthGYAj7m7mer0KYr0ljI0pjPqOQiIhJaUaC7+5ur1ZDlaC8XVHIREQlFdqYohCsXqYcuIgJEPNA7ygV6FOgiIkDkA72oyUUiIqGIB3oBd+gdUi9dRCTSga6hiyIi50U60Dubgun/qqOLiEQ80M/30DV0UUQk0oHemM9Qn0trcpGICBEPdDPTWHQRkVCkAx2COrpq6CIiMQj09lJBNXQREWIQ6B3lAr1DE0zNaHKRiCRb9AO9qRhOLpqodVNERGoq8oGuoYsiIoHIB3qHlqITEQHiEOilYLaohi6KSNJFPtBLxQx1uTQ9Awp0EUm2yAf63OSiQdXQRSTZIh/oENTRVUMXkaSLRaC3l4qqoYtI4sUi0DubCpwaHGdak4tEJMFiEejt5QKzDn3DmlwkIskVi0DXWHQRkdgEejAW/aSGLopIgsUk0Cs9dA1dFJHkikWgl4tZCtmURrqISKLFItDNjI5ykZODCnQRSa5YBDqEk4sGVHIRkeSKTaBrbVERSbrYBHpHucCpoQlmZr3WTRERqYnYBHp7ucjMrNOnlYtEJKFiE+idGrooIgkXm0A/vxSd6ugikkyxCfS52aIKdBFJqNgE+oa6LPlMSiUXEUms2AR6MLlIC12ISHLFJtBBY9FFJNlWFOhm9ptmdsDM9pvZA2ZWqFbDlqOjXFQPXUQSa9mBbmabgbuA3e5+PZAGPlithi1HRzlYuUiTi0QkiVZacskARTPLAHVAz8qbtHwd5QLTs06/Vi4SkQRadqC7+wngj4CXgJPAOXf/brUathztGrooIgm2kpLLBuB2YCvQCdSb2UcWue5OM+s2s+6+vr7lt/QKaKELEUmylZRc3gYcdfc+d58CvgG8YeFF7n6vu+92992tra0reLrL09qiIpJkKwn0l4BbzKzOzAy4FThYnWYtT3N9jlxaKxeJSDKtpIb+OPAXwNPAvvBn3Vuldi2LmdGuyUUiklCZlXyzu38K+FSV2lIVwWxR1dBFJHliNVMU0PR/EUms2AV6e7nIqcFxZjW5SEQSJnaB3lEuMDXj9I9M1ropIiJrKpaBDhqLLiLJE8NA12xREUmm2AW6lqITkaSKXaBvDCcX9ajkIiIJE7tAT6WMTeW8eugikjixC3SAjpIWuhCR5IlloGspOhFJolgGekdTEOiaXCQiSRLPQC8VmJyZ5cyoJheJSHLEMtArKxep7CIiSRLLQK/MFu0Z0NBFEUmOeAZ6Uzi5aFA9dBFJjlgGekt9nkzKNHRRRBIlloGeShmbShq6KCLJEstAB+hsKqiGLiKJEttAby8XVUMXkUSJbaBXlqJz1+QiEUmG2AZ6e6nA5PQsZ0enat0UEZE1EdtA72zSWHQRSZbYBrpmi4pI0sQ20OfWFtUboyKSELEN9JaGcHKRSi4ikhCxDfS0JheJSMLENtAhWOhC0/9FJCliH+iaXCQiSRHrQO8sB9P/NblIRJIg1oHeXi4yMT3LgCYXiUgCxDrQ54Yuqo4uIgkQ60BvL1cWutDQRRGJv1gHemc4W7RnQD10EYm/WAd6a2OedMo0Fl1EEiHWgZ5OGW2NedXQRSQRYh3oUPlcdNXQRST+EhDoRZVcRCQRlh3oZnatmT0772vQzO6uZuOqoV0rF4lIQmSW+43ufgi4CcDM0sAJ4JtValfVdJQLjE3NMDg2TbkuW+vmiIismmqVXG4FDrv78Sr9vKrpqAxdVB1dRGKuWoH+QeCBxR4wszvNrNvMuvv6+qr0dFdubnKR6ugiEnMrDnQzywHvBf58scfd/V533+3uu1tbW1f6dK+apv+LSFJUo4f+TuBpdz9VhZ9VdW2NeVKGhi6KSOxVI9A/xBLllvUgk07R1qiFLkQk/lYU6GZWB7wd+EZ1mrM62staik5E4m9Fge7uo+6+0d3PVatBq0GzRUUkCWI/UxSCoYuaXCQicZeQQC8wOjnD4Ph0rZsiIrJqEhHoGosuIkmQiEA/PxZddXQRia9kBHpTMP1fQxdFJM4SEehtjXnMFOgiEm+JCPRsOkVrQ55XVHIRkRhLRKBDZSy6eugiEl8JCvSiAl1EYi0xga7p/yISd4kJ9I5ygeGJaYbGp2rdFBGRVZGcQNfQRRGJueQEuha6EJGYS0ygt5cq0/81dFFE4ikxgb6pVNDkIhGJtcQEei6ToqUhz8kBBbqIxFNiAh3CyUWDCnQRiadEBXp7qaAauojEVqICvbOpqJKLiMRWogK9vVxgSJOLRCSmEhXolbHop1RHF5EYSlSgV8aia+iiiMRRogK9szL9X3V0EYmhRAV6WykPqIcuIvGUqEDPZ9K0NOR4ZVBDF0UkfhIV6BCMdOlRyUVEYihxgb69tYEfHennq0+8hLvXujkiIlWTuED/nb3XsaermXu+sY+7v/YswxPTtW6SiEhVZGrdgLXW2pjn/l/bwxcfeZE/+fufsO/lc3zhwzdzXWep1k0TkQhxdyZnZhmbnGFsaoaxyRlGJ2cYnwq2Y1Pz9idn+OXXXUW5mF3VNiUu0AHSKeOuW3ewZ2szdz3wDO/7L/+HT79nFx/aswUzq3XzRGQVuTvDE9OcG5sKvkaD7UB4PBAeD45PBWE9OcPo1AzjYUifD+1pZl9F1fbNO1pWPdBtLevIu3fv9u7u7jV7vitxeniC3/zaszz6wmnec2Mnn3v/9TQWVvcPXURWbmpmdi6AB0Yng+1YsD84dnFID847nrlEEmfTRrmYpVTIUsylqculKWSDbTGbppjLhNsUdbnMgscu3lYeayxkSaeW12E0s6fcffflrktkD32+loY89//qHv7r9w/zH757iH0vD/CFD9/M9ZvLtW6aSCJMzcyGveIwlEenODs6eT6sxyY5Oxr0pAfmXXOp97/MoFTIUi5maaoLtldtKFIuXniuXMxddK4ul47s/9QT30Of7/Ej/dz11Wc4OzrF7777Oj7yC1dH9i9WZLVNzcwyMjHN0HjwNTwxzfDE1Nz+0Pg0w/P35z02PD7NULgdm5pZ8jlSBk11OZrqsjQVs8F+ZVt3PoSb6nJsqOwXczQWMqSW2Rtej660h65AX6B/eILf+vpzfP8nfey9oYN//0uvpaQSjCSQu9M7NMGRvhGO9Y9w9PTI3P6Js2OXDOKKlEFDPkNjIUtjIUNDPkNDuJ1/bkNdlvJcWGfZUJejXJelIRevYF4ulVyWaWNDnv/+Kz/Pf/vBEf7ou4fYf+IcX1QJRmLs7MgkR06PcOx0ENpH+0c4Ggb36OT50M5lUnRtrGN7az1v+ZlWSsVsGMyZMJizc2FdKgTBXcxGt3wRReqhX0L3sTP8xgPP0D88ySf37uSjr79GL06JDHdnfGo2HJkxzcDoFEfnBfeR00FoD4yeXx8gnTK2bCiytaWerpZ6toXbrS31dJaL6i3XyJqUXMysCbgPuB5w4Nfc/UdLXR+1QAc4MzLJb//5c3zvx7288/p2Pv+BG1SCkVXl7vQNB6WOnoGxuXHMo5MzjE5NMz63P3PBsLqxyemLxkEv9evdWS7MBfX8ry3NdWTTiZtvuO6tVaDfDzzq7veZWQ6oc/eBpa6PYqADzM469/3wCJ//20N0NhX44odv5oarmmrdLIm48akZjvePcqRvmCOnRzjcO8zh0yMc6RtmaHzxERy5TOqCIXJ1uTR12QyFXJq6yvC58PFgPzN3famYoaulnmua6ynm0mt8t7ISqx7oZlYCngO2+RX+kKgGesVTx8/yG195mr7hCT7xrp38yhu6VIKRS5rf2z7cN8yRviCwD/eN8PLZ0QsmpnSUC2xrrWdbSwPbW+vZ1trAluY66vNhgGfTZNR7TqS1CPSbgHuB54EbgaeAj7v7yFLfE/VABxgYDUowf3+wl3fs2sQffODGVZ/9JdEwPDHN08fPsu/EufO97d5hhuaNly5kU2ydF9jbW+vZ3trA1pZ66vMaoyCLW4tA3w08BrzR3R83s/8EDLr77y647k7gToCrr776dcePH1/W860n7s6XfniU3/9fP6alIc8n9u7kPTd0qLeeMGdGJnny2BmeOHqGJ4+d4UDP4NwMxEpve3trA9tawvBua6CjVNAbi/KqrUWgtwOPuXtXePxm4B5337vU98Shhz7fcz8d4JPf2sf+E4P8wtZmPv3eXezs0Id8xVXPwBhPHD3DE8fO8OTRM7zQOwwEde2f29LEnq3N7NnazE1bmvTxEVJVqz4O3d1fMbOfmtm17n4IuJWg/JIYN25p4tsfexNfe/Kn/OHf/Zi9//lR/tkt1/Bbb7+Wcp1+oaPM3TncN8KTYXg/fvQMJwaCla4a8xle17WB99+8mT1dzbz2qjL5jN5klNpb6SiXmwiGLeaAI8CvuvvZpa6PWw99voHRSf74oZ/wZ48dp6kux79+x7X8k91blv1hPLK2ZmadgycHgx54WELpH5kEoKUhx56tzfx8V9AD/9n2kv5eZU1p6n+NPN8zyKcfPMATx87w2s1lPnP7Lm6+ekOtmxVZs7POmdFJegcn6B0ap3dogt7BYHtubIrpWWd21pmZdWY92M44c+dmPHx83nZmlovO9Q9Pzn3Y05bmYhDeYYBvbanX+yNSUwr0GnJ3Hnyuh8/9zUFODU7wyzdfxb9957W0NRZq3bR1Y2bW6R+ZOB/UgxOcWiS0+4YmmF7ko05LhQwb6nOkU0bajHTKSFW2KSNtXHCusp+Ze/zCa0vFLK+7ZgN7tjbTUS7W4E9EZGn6LJcaMjNuv2kzb9u5iS888iL3PXqEvzvwCne/bQd3vKErUjPxhsan6BkYZ3xqhonpWSamZ5iYmj2/Pz3LxNxjSz1+fn9ofJpTg+P0j0wu+pnUzfU52hrztDbmeU1bI5tKedoa87SVCuF+gdbGPIWsatYiC6mHvgaOnh7hs391gEcO9fGatgY+/Z5dvGlHS62bdZH+4Qn29wxyoOccB3oGOXDiHMf6R1/Vz8ikjHwmRT6bDraZFPlMmnw2RSGTpi6fpq0xz6ZSIQzuMKhLBVob8uQy0fnHTmStqOSyDj188BSf/c7zHO8f5bZd7Xxy7062NNeteTvcnZ5z4xw4cY79PYM833OO/ScGeWVwfO6aLc1FdnWUuX5zia6WeorZ9Fwwz4V0JhUepylkU+TSKc1kFFkFKrmsQ7fu3MQbX9PCl354lC9870UeOdTLv3zLdv7FP9i+aiWE2VnnWP/I+Z73iWB7NvyEvZTBttYGbtnWzK7OMrs2l9jVUdawS5EIUg+9RnoGxvjc3xzkO//vJFdtKPI7e6/jHbs2XTCawt2ZnnXGp2YYn5qdq2MH2+BcZVu5pnJ8anCcAz3neL5nkJHwM61z6RQ/097A9Z1ldnWWuK6zzM6ORupy+nddZD1TySUi/u/h03zmwec5dGqIzU3F4DOspysBPfOqVhWfry6X5rqOErs6S+zaHAT4jrZG1ahFIkgll4h4w/YW/vquN/GVJ16i+9hZ8pkUhfANxUI2qE1XjvPZYPXxQmW/cm34hmPl+kp9W58ZIpIsCvR1IJNO8dHXd/HR13fVuikiEmH6/7eISEwo0EVEYkKBLiISEwp0EZGYUKCLiMSEAl1EJCYU6CIiMaFAFxGJiTWd+m9mfcDxy1zWApxeg+asR0m+d0j2/evek+tK7v8ad2+93A9a00C/EmbWfSWfWRBHSb53SPb9696Tee9Q3ftXyUVEJCYU6CIiMbEeA/3eWjeghpJ875Ds+9e9J1fV7n/d1dBFRGR51mMPXURElmHdBLqZ3WZmh8zsRTO7p9btWW1m9mUz6zWz/fPONZvZQ2b2QrjdUMs2rhYz22Jmj5jZQTM7YGYfD8/H/v7NrGBmT5jZc+G9fyY8v9XMHg/v/Wtmlqt1W1eTmaXN7Bkz+054nIj7N7NjZrbPzJ41s+7wXNVe9+si0M0sDXwReCdwHfAhM7uutq1adX8K3Lbg3D3Aw+6+A3g4PI6jaeBfuftO4BbgY+HfdxLufwJ4q7vfCNwE3GZmtwCfB/5jeO9ngV+vYRvXwseBg/OOk3T//9Ddb5o3VLFqr/t1EejAHuBFdz/i7pPAV4Hba9ymVeXuPwDOLDh9O3B/uH8/8L41bdQacfeT7v50uD9E8Iu9mQTcvweGw8Ns+OXAW4G/CM/H8t4rzOwqYC9wX3hsJOj+F1G11/16CfTNwE/nHb8cnkuaTe5+EoLQA9pq3J5VZ2ZdwM8Bj5OQ+w/LDc8CvcBDwGFgwN2nw0vi/vr/E+DfALPh8UaSc/8OfNfMnjKzO8NzVXvdr5c1RRdbzVjDb2LOzBqAvwTudvfBoKMWf+4+A9xkZk3AN4Gdi122tq1aG2b2bqDX3Z8ys7dUTi9yaSzvH3iju/eYWRvwkJn9uJo/fL300F8Gtsw7vgroqVFbaumUmXUAhNveGrdn1ZhZliDM/6e7fyM8nZj7B3D3AeB/E7yP0GRmlQ5WnF//bwTea2bHCEqrbyXosSfi/t29J9z2Evxjvocqvu7XS6A/CewI3+nOAR8EHqxxm2rhQeCOcP8O4Ns1bMuqCWumXwIOuvsfz3so9vdvZq1hzxwzKwJvI3gP4RHgA+Flsbx3AHf/d+5+lbt3Efyef8/d/ykJuH8zqzezxso+8I+A/VTxdb9uJhaZ2bsI/qVOA19299+rcZNWlZk9ALyF4JPWTgGfAr4FfB24GngJ+MfuvvCN08gzszcBjwL7OF9H/QRBHT3W929mNxC88ZUm6FB93d0/a2bbCHqszcAzwEfcfaJ2LV19Ycnlt9393Um4//AevxkeZoCvuPvvmdlGqvS6XzeBLiIiK7NeSi4iIrJCCnQRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISEwo0EVEYuL/Awf1BZb9ydizAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mse = []\n",
    "for k in np.arange(1,50,3):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=k, metric = 'euclidean')\n",
    "    knn.fit(data_scaled, y)\n",
    "    y_pred_test = knn.predict(test_scaled)\n",
    "    mse_test = mean_squared_error(y_pred_test, test_y)\n",
    "    mse.append(mse_test)\n",
    "plt.plot(np.arange(1,50,3), mse)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
