{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Lab 2: Nearest neighbour and data preprocessing\n",
    "Predictive modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In our previous lab session, we explored the iris dataset. In this dataset, there are no missing values in the variable \"Species\". But what if there are new observations without a value in the \"Species\" variable? So, imagine the case that for a 'new' iris flower we know the values for the other columns (sepal/petal length and height) but \n",
    "it is unknown to which of the three species it belongs to (dataset irisNA.csv). A natural task would be\n",
    "to try to guess to which species each of the new flowers belongs to. This task\n",
    "(or problem) is called a classification problem in machine learning. In this practical exercise session, a first\n",
    "simple algorithm that provides an answer to this problem is described.\n",
    "### Notations and vocabulary\n",
    "In the iris dataset (iris120.csv), each instance (each flower is an instance) is described by five properties:\n",
    "the species it belongs to, the width of its petals, the length of its petals, the width of its sepals and the\n",
    "length of its sepals. In this PC-lab, for simplicity, only species, sepal length and sepal width will be used.\n",
    "These properties can be seen as variables, and for a given flower, each of these variables takes a specific\n",
    "value. In a classification setting, the aim is to predict the value of one of the variables (here the species),\n",
    "based on the value of the other variables (here petal width and length). The variable of which the values\n",
    "have to be predicted is called the output variable and the variables used to make this prediction are called\n",
    "the input variables or features. A dataset consists of a set of observations of input-output couples $(\\boldsymbol{x}, y)$. In this dataset, the observed values\n",
    "for the features of the $i$-th instance are denoted $\\boldsymbol{x_i}$ $= (x_{i1}, ... , x_{ip})^T$ , where $p$ the number of features, and the observed value of its output\n",
    "is denoted $y_i$. Using this notation, a training dataset $T$ containing $n$ instances can be written as $$T = \\{(\\boldsymbol{x_1}, y_1), ... , (\\boldsymbol{x_n}, y_n)\\}.$$\n",
    "Using this dataset, we will try to build a model (generally denoted $f$) that is able to predict the value of the\n",
    "output variable, based on the value of the input variables. When this output variable is nominal, this process\n",
    "is called a classification problem.    \n",
    "In the iris problem, both input variables take real values $(\\boldsymbol{x_i} \\in \\mathbb{R}^2)$. The output variable, however, is\n",
    "nominal, it takes values from a finite set $\\{setosa, versicolor, virginica\\}$. Because of this, the model $\\textit{f}$ we are\n",
    "looking for is one which performs a mapping $$\\textit{f} : \\mathbb{R}^2 \\rightarrow  \\{setosa, versicolor, virginica\\}.$$   \n",
    "### Nearest neighbour for classification\n",
    "Several techniques exist that are capable of deriving classification models from data. A very simple one is\n",
    "the nearest neighbour model. This model departs from the assumption that instances whose features are\n",
    "highly similar, are likely to have the same labels. The one nearest neighbour (1-NN) model applies this\n",
    "idea in its most extreme form: the label for an instance (with unknown label) is predicted as the label of the\n",
    "closest instance in the training dataset.   \n",
    "To be able to select the ‘closest’ instance in the training dataset, a distance measure has to be defined. In\n",
    "this text, we will use $d(\\boldsymbol{x_i}, \\boldsymbol{x_j})$ to denote the distance between two feature vectors $\\boldsymbol{x_i}$ and $\\boldsymbol{x_j}$. As a simple\n",
    "distance measure, the Euclidean distance can be used\n",
    "$$d_E(\\boldsymbol{x_i}, \\boldsymbol{x_j}) = \\sqrt{\\sum_{k=1}^{p} (x_{i,k} - x_{j,k})^2}$$\n",
    "Using this distance function, the nearest neighbour algorithm performs the following steps:\n",
    "1. For an instance with unknown label and known feature vector $\\boldsymbol{x}$, calculate the distance to each instance in the dataset: $d_E(\\boldsymbol{x}, \\boldsymbol{x_i})$ where $i = 1, ... ,n.$\n",
    "2. Select the closest instance and take its label as the prediction for the unknown label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **Load the dataset iris120.csv in to the memory and select the columns 'Sepal.Length', 'Sepal.Width', and 'Species'. Additionally, load the set of unclassified\n",
    "instances (irisNA.csv) and select the same columns. Both datasets should be loaded as data frames. **\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "iris120 = pd.read_csv('iris120.csv')\n",
    "irisNA = pd.read_csv('irisNA.csv')\n",
    "\n",
    "iris120_cols = iris120[['Sepal.Length', 'Sepal.Width', 'Species']]\n",
    "irisNA_cols = irisNA[['Sepal.Length', 'Sepal.Width', 'Species']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **Implement the nearest neighbour algorithm for the iris problem in a function called nnIrisPredict.\n",
    "Use this function to predict the species of unknown flowers irisNA.csv in the dataset. Make sure\n",
    "your function has the following structure:**\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nnIrisPredict(featuresNewInstance, trainDataset):\n",
    "    \n",
    "\n",
    "    dist = np.sqrt((featuresNewInstance['Sepal.Length'] - trainDataset.iloc[:]['Sepal.Length'])**2 + \\\n",
    "                   (featuresNewInstance['Sepal.Width'] - trainDataset.iloc[:]['Sepal.Width'])**2)\n",
    "    nn = np.argmin(dist)\n",
    "    label = trainDataset.iloc[nn]['Species']\n",
    "    return (label)\n",
    "\n",
    "new_observation = irisNA_cols.loc[15][[0,1]]\n",
    "train = iris120_cols\n",
    "#call the function\n",
    "nnIrisPredict(new_observation, train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where trainDataset is a data frame (containing flowers with known species label) with columns\n",
    "\"Sepal.Length\", \"Sepal.Width\" and \"Species\". featuresNewInstance is a vector with the sepal\n",
    "length in the first position and the sepal width in the second position. Label should be one of the\n",
    "strings \"setosa\", \"versicolor\" and \"virginica\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nearest neighbour algorithm for regression\n",
    "In the previous section, the output was a nominal variable (no numerical values, specific classes). When the output is real-valued, the prediction problem is called a regression problem. As with nominal outputs, the nearest neighbour algorithm can\n",
    "be used to predict the output variable of unlabeled instances. The algorithm is identical to the one for\n",
    "classification, however, the prediction will be the real-valued label of the closest instance in the training\n",
    "dataset instead of its class label.\n",
    "\n",
    "## Data preprocessing\n",
    "In this section, some elementary data preprocessing steps are described.\n",
    "### Dummy encoding of nominal variables\n",
    "The basic nearest neighbour algorithm implemented in the previous assignment can only be used with\n",
    "numerical features. However, often types of variables such as nominal variables or ordinal variables are\n",
    "present. A simple solution to this problem exists in using a dummy encoding for each nominal variable.\n",
    "When a variable $\\boldsymbol{x^i}$ is nominal with $k$ values, it is replaced by $k$ new binary variables. As an example,\n",
    "consider the weather dataset, here the variable \"Outlook\" ($\\boldsymbol{x^1}$) has three values: Sunny, Overcast and Rainy.\n",
    "Each of these values is represented by a dummy variable: $\\boldsymbol{x^{1a}}$, $\\boldsymbol{x^{1b}}$ and $\\boldsymbol{x^{1c}}$ with the following values:   \n",
    "  *  $x^{1a} = 1$ if $x^1 = \"Sunny\"$ and  $x^{1a} = 0$ otherwise\n",
    "  * $x^{1b} = 1$ if $x^1 = \"Overcast\"$ and $x^{1b} = 0$ otherwise\n",
    "  * $x^{1c} = 1$ if $x^1 = \"Rainy\"$ and $x^{1c} = 0$ otherwise\n",
    "  \n",
    "See the following example in python. In this example, we use  the Abalone dataset (abaloneTrain700.csv) which contains measurements of physical properties of several abalone (an edible sea snail) specimen. Using these physical properties, the aim is to build a predictive model for the age of these animals (more information concerning this dataset can be found in [abalone.info](https://archive.ics.uci.edu/ml/datasets/Abalone)). In the following example, we replace the nominal variable 'sex' with 3 dummy variables (as many as the values it takes). In python, there are functions such as the [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function which are used for this purpose. So firstly, we create the dummy variables, then we concatenate them with the original dataset and finally we remove the original variable form the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>wholeWeight</th>\n",
       "      <th>shuckedWeight</th>\n",
       "      <th>visceraWeight</th>\n",
       "      <th>shellWeight</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.2975</td>\n",
       "      <td>0.6035</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.4485</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.1305</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height  wholeWeight  shuckedWeight  visceraWeight  \\\n",
       "0   I   0.665     0.500   0.170       1.2975         0.6035         0.2910   \n",
       "1   F   0.460     0.365   0.115       0.4485         0.1650         0.0830   \n",
       "2   F   0.560     0.445   0.180       0.9030         0.3575         0.2045   \n",
       "3   I   0.395     0.300   0.090       0.2790         0.1340         0.0490   \n",
       "4   I   0.530     0.400   0.145       0.5550         0.1935         0.1305   \n",
       "\n",
       "   shellWeight  age  \n",
       "0       0.3595    9  \n",
       "1       0.1700   14  \n",
       "2       0.2950    9  \n",
       "3       0.0750    8  \n",
       "4       0.1950    9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('abaloneTrain700.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   F  I  M\n",
      "0  0  1  0\n",
      "1  1  0  0\n",
      "2  1  0  0\n",
      "3  0  1  0\n",
      "4  0  1  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>wholeWeight</th>\n",
       "      <th>shuckedWeight</th>\n",
       "      <th>visceraWeight</th>\n",
       "      <th>shellWeight</th>\n",
       "      <th>age</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.665</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.2975</td>\n",
       "      <td>0.6035</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.460</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.4485</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.560</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.395</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.1305</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  diameter  height  wholeWeight  shuckedWeight  visceraWeight  \\\n",
       "0   0.665     0.500   0.170       1.2975         0.6035         0.2910   \n",
       "1   0.460     0.365   0.115       0.4485         0.1650         0.0830   \n",
       "2   0.560     0.445   0.180       0.9030         0.3575         0.2045   \n",
       "3   0.395     0.300   0.090       0.2790         0.1340         0.0490   \n",
       "4   0.530     0.400   0.145       0.5550         0.1935         0.1305   \n",
       "\n",
       "   shellWeight  age  F  I  M  \n",
       "0       0.3595    9  0  1  0  \n",
       "1       0.1700   14  1  0  0  \n",
       "2       0.2950    9  1  0  0  \n",
       "3       0.0750    8  0  1  0  \n",
       "4       0.1950    9  0  1  0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('abaloneTrain700.csv')\n",
    "dummies = pd.get_dummies(data.sex) # create dummies\n",
    "print(dummies.head())\n",
    "data_with_dummies = pd.concat([data, dummies], axis=1) #  concatenate them with the dataset\n",
    "data_with_dummies = data_with_dummies.drop(['sex'], axis=1) # remove the original sex column\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "Missing values are commonly encountered in data mining studies. Often, missing values are imputed\n",
    "(replaced by a value). Several techniques exist to choose this value. A simple, but often used method\n",
    "is mean imputation. Here, each missing value is replaced by the mean of the observed values for\n",
    "that variable. More advanced methods exist of building separate models to predict the missing\n",
    "values.\n",
    "When implementing the mean imputation, the [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) of scikit-learn library might be handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the data\n",
    "In realistic datasets, most features have different means and standard deviations. For the nearest\n",
    "neighbour algorithm, it can easily be seen that features with a high standard deviation will be more\n",
    "influential than features with a lower standard deviation. In most cases, this is unwanted since it\n",
    "is not known in advance which features are most important. To overcome this problem, features\n",
    "are often standardized. The standardized version of $x^i$ can be obtained as   \n",
    "$$\\frac{x^i - \\mu_i}{\\sigma_i}$$    \n",
    "where $\\mu_i$ and $\\sigma_i$ represent the sample mean and standard deviation of $\\boldsymbol{x^i}$.\n",
    "The [Scaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) of scikit-learn can be used to perform this standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = data_with_dummies['age'] # keep the target variable\n",
    "data_with_dummies = data_with_dummies.drop(['age'], axis=1) # remove it from the feature set\n",
    "data_with_dummies.head()\n",
    "data_scaled = scale(data_with_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **To prepare the datasets abaloneTrain700.csv and abaloneTest700.csv for analysis, perform\n",
    "the preprocessing steps described in the previous section (more information concerning this\n",
    "dataset can be found in [abalone.info](https://archive.ics.uci.edu/ml/datasets/Abalone)).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest-neighbours\n",
    "A simple extension of the nearest neighbour algorithm consists of taking more than only the nearest\n",
    "neighbour into account. Let $N_k(\\boldsymbol{x}) \\subset T$ be the k nearest neighbours of an instance with feature\n",
    "vector $\\boldsymbol{x}$. The k-nearest neighbour prediction $Y(\\boldsymbol{x})$ can be determined as follows\n",
    "1. For classification problems:  $Y(\\boldsymbol{x})$ is set as the label that occurs most often in $N_k(\\boldsymbol{x})$ (the\n",
    "mode).\n",
    "2. For regression problems, the average of the $k$ nearest outputs can be taken $Y(\\boldsymbol{x}) = \\frac{1}{k}\\sum_{\\boldsymbol{x_i} \\in N_k(\\boldsymbol{x})} y_i$    \n",
    "\n",
    "### k-nearest-neighbours in python\n",
    "As with the 1-nearest neighbour algorithm, it is possible to implement a version of the k-nearest neighbours\n",
    "algorithm in python. However, an alternative is to use a pre-implemented version of this\n",
    "algorithm. For most popular machine learning algorithms, these functions are packed in specific python\n",
    "“packages\". An implementation of\n",
    "the k-nearest-neighbours algorithm is available in the [scikit-learn](http://scikit-learn.org/stable/) library (as well). \n",
    "You can load and see more info about the usage of this function by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "help(neighbors.KNeighborsRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the help window, in scikit-learn, an estimator for classification/regression is a Python object that implements the methods fit(X, y) and predict(T). The constructor of an estimator takes as arguments the parameters of the model (in our case the basic parameters are the number of neighbours and the distance metric). So the first step is to create a KNN instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsRegressor(n_neighbors=5, metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our estimator instance `knn`. It now must be fitted to the model, that is, it must learn from the model. This is done by passing our training set to the fit method. As a training set, let us use all the examples of the abalone dataset we loaded before as training set apart from the last one. We select this training set with the [:-1] Python syntax, which produces a new array that contains all but the last entry of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "knn.fit(data_scaled[:-1], y[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can predict new values, in particular, we can ask to the estimator which is the class of our last example of the dataset, which we have not used to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = knn.predict(data_scaled[-1].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prediction quality\n",
    "To test the performance of the nearest neighbour algorithm, an option is to use test data with\n",
    "known labels and to compare the predictions with these labels. In this case, we have two different\n",
    "datasets, a training set T and a test set T$^*$. In case of a regression problem, the mean of squared\n",
    "residuals is commonly used to evaluate the quality of a model. This measure is calculated as follows:   \n",
    "1. Mean of squared residuals on test set:\n",
    "$$Err_{T^*} = \\frac{1}{|T^*|}\\sum_{\\boldsymbol{x_i} \\in T^*} (Y(\\boldsymbol{x_i}) - y_i)^2$$\n",
    "2. Additionally, the error on the training data itself can be computed:   \n",
    "$$Err_{T} = \\frac{1}{|T|}\\sum_{\\boldsymbol{x_i} \\in T} (Y(\\boldsymbol{x_i}) - y_i)^2$$\n",
    "\n",
    "Naturally, it is desirable to keep the mean of squared residuals as small as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the mean squared error of the prediction we get before (for the last example of the abalone dataset) by calculating it ourselves or by using the [mean_squared_error()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_pred, [y.iloc[-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **The abalone dataset was split in two subsets: abaloneTrain700.csv and abaloneTest700.csv.\n",
    "Build a 3-nearest-neighbour classifier and use it to obtain\n",
    "predicted values for the age of the abalone specimen. Calculate the mean of squared residuals\n",
    "for these predictions, try to obtain the mean of squared residuals on the training data as well.\n",
    "Which one is the smallest ?**   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>: **Plot the mean of squared residuals as a function of the number of neighbours k which is\n",
    "taken into account, where k ranges between 1 and 50 (step size equal to 3). **\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
