{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixjoojZvfEhh"
      },
      "source": [
        "# PC Lab 2: Nearest neighbour and data preprocessing\n",
        "Predictive modelling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH1K-8KXfEhn"
      },
      "source": [
        "## Introduction\n",
        "In our previous lab session, we explored the iris dataset. In this dataset, there are no missing values in the variable \"Species\". But what if there are new observations without a value in the \"Species\" variable? So, imagine the case that for a 'new' iris flower we know the values for the other columns (sepal/petal length and height) but \n",
        "it is unknown to which of the three species it belongs to (dataset irisNA.csv). A natural task would be\n",
        "to try to guess to which species each of the new flowers belongs to. This task\n",
        "(or problem) is called a classification problem in machine learning. In this practical exercise session, a first\n",
        "simple algorithm that provides an answer to this problem is described.\n",
        "### Notations and vocabulary\n",
        "In the iris dataset (iris120.csv), each instance (each flower is an instance) is described by five properties:\n",
        "the species it belongs to, the width of its petals, the length of its petals, the width of its sepals and the\n",
        "length of its sepals. In this PC-lab, for simplicity, only species, sepal length and sepal width will be used.\n",
        "These properties can be seen as variables, and for a given flower, each of these variables takes a specific\n",
        "value. In a classification setting, the aim is to predict the value of one of the variables (here the species),\n",
        "based on the value of the other variables (here petal width and length). The variable of which the values\n",
        "have to be predicted is called the output variable and the variables used to make this prediction are called\n",
        "the input variables or features. A dataset consists of a set of observations of input-output couples $(\\boldsymbol{x}, y)$. In this dataset, the observed values\n",
        "for the features of the $i$-th instance are denoted $\\boldsymbol{x_i}$ $= (x_{i1}, ... , x_{ip})^T$ , where $p$ the number of features, and the observed value of its output\n",
        "is denoted $y_i$. Using this notation, a training dataset $T$ containing $n$ instances can be written as $$T = \\{(\\boldsymbol{x_1}, y_1), ... , (\\boldsymbol{x_n}, y_n)\\}.$$\n",
        "Using this dataset, we will try to build a model (generally denoted $f$) that is able to predict the value of the\n",
        "output variable, based on the value of the input variables. When this output variable is nominal, this process\n",
        "is called a classification problem.    \n",
        "In the iris problem, both input variables take real values $(\\boldsymbol{x_i} \\in \\mathbb{R}^2)$. The output variable, however, is\n",
        "nominal, it takes values from a finite set $\\{setosa, versicolor, virginica\\}$. Because of this, the model $\\textit{f}$ we are\n",
        "looking for is one which performs a mapping $$\\textit{f} : \\mathbb{R}^2 \\rightarrow  \\{setosa, versicolor, virginica\\}.$$   \n",
        "### Nearest neighbour for classification\n",
        "Several techniques exist that are capable of deriving classification models from data. A very simple one is\n",
        "the nearest neighbour model. This model departs from the assumption that instances whose features are\n",
        "highly similar, are likely to have the same labels. The one nearest neighbour (1-NN) model applies this\n",
        "idea in its most extreme form: the label for an instance (with unknown label) is predicted as the label of the\n",
        "closest instance in the training dataset.   \n",
        "To be able to select the ‘closest’ instance in the training dataset, a distance measure has to be defined. In\n",
        "this text, we will use $d(\\boldsymbol{x_i}, \\boldsymbol{x_j})$ to denote the distance between two feature vectors $\\boldsymbol{x_i}$ and $\\boldsymbol{x_j}$. As a simple\n",
        "distance measure, the Euclidean distance can be used\n",
        "$$d_E(\\boldsymbol{x_i}, \\boldsymbol{x_j}) = \\sqrt{\\sum_{k=1}^{p} (x_{i,k} - x_{j,k})^2}$$\n",
        "Using this distance function, the nearest neighbour algorithm performs the following steps:\n",
        "1. For an instance with unknown label and known feature vector $\\boldsymbol{x}$, calculate the distance to each instance in the dataset: $d_E(\\boldsymbol{x}, \\boldsymbol{x_i})$ where $i = 1, ... ,n.$\n",
        "2. Select the closest instance and take its label as the prediction for the unknown label."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first codeblock downloads all the necessary data for this pc-lab."
      ],
      "metadata": {
        "id": "RfMArc34t4_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/gdewael/teaching/main/predmod/lab2/iris120.csv\", \"iris120.csv\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/gdewael/teaching/main/predmod/lab2/irisNA.csv\", \"irisNA.csv\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/gdewael/teaching/main/predmod/lab2/abaloneTrain700.csv\", \"abalone.csv\")"
      ],
      "metadata": {
        "id": "IE_U9a1Ot4TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXJNJBhcfEhr"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **Load the dataset iris120.csv in to the memory and select the columns 'Sepal.Length', 'Sepal.Width', and 'Species'. Additionally, load the set of unclassified\n",
        "instances (irisNA.csv) and select the same columns. Both datasets should be loaded as data frames. **\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7LJ85LnpfEhu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris120 = # ... load in dataset iris120.csv\n",
        "iris120 = iris120[] # ... select the right columns\n",
        "\n",
        "irisNA =  # ... repeat for irisNA.csv\n",
        "irisNA = irisNA[] # ... repeat for irisNA.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris120.head()"
      ],
      "metadata": {
        "id": "3G3zeHSb1Dxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "irisNA.head()"
      ],
      "metadata": {
        "id": "pKsIKSYh1EIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zczv03YfEhv"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **Implement the nearest neighbour algorithm for the iris problem in a function called nnIrisPredict.\n",
        "Use this function to predict the species of unknown flowers irisNA.csv in the dataset. Make sure\n",
        "your function has the following structure:**\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_observation_features = irisNA.iloc[0, 0:2] # taking the first two columns\n",
        "new_observation_features"
      ],
      "metadata": {
        "id": "IUv7C6fp1QuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataset = iris120\n",
        "trainDataset"
      ],
      "metadata": {
        "id": "21wZFkvZ1UyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zP8apShlfEhw"
      },
      "outputs": [],
      "source": [
        "def nnIrisPredict(featuresNewInstance, trainDataset):\n",
        "    # create a variable 'dist' containing the euclidean distance of the\n",
        "    # new instance to all instances (rows) in the training dataset:\n",
        "    dist = # ...\n",
        "    # what is the index of the nearest neighbor 'nn' in our training dataset:\n",
        "    nn = np.argmin(dist)\n",
        "    # Extract the species label of that nearest neighbor\n",
        "    # and return that as prediction:\n",
        "    label = trainDataset.iloc[nn]['Species']\n",
        "    return label\n",
        "\n",
        "\n",
        "# call the function\n",
        "nnIrisPredict(new_observation_features, trainDataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-BvbEnfEhx"
      },
      "source": [
        "where trainDataset is a data frame (containing flowers with known species label) with columns\n",
        "\"Sepal.Length\", \"Sepal.Width\" and \"Species\". featuresNewInstance is a vector with the sepal\n",
        "length in the first position and the sepal width in the second position. Label should be one of the\n",
        "strings \"setosa\", \"versicolor\" and \"virginica\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXpRMd6CfEhy"
      },
      "source": [
        "## The nearest neighbour algorithm for regression\n",
        "In the previous section, the output was a nominal variable (no numerical values, specific classes). When the output is real-valued, the prediction problem is called a regression problem. As with nominal outputs, the nearest neighbour algorithm can\n",
        "be used to predict the output variable of unlabeled instances. The algorithm is identical to the one for\n",
        "classification, however, the prediction will be the real-valued label of the closest instance in the training\n",
        "dataset instead of its class label.\n",
        "\n",
        "## Data preprocessing\n",
        "In this section, some elementary data preprocessing steps are described.\n",
        "### Dummy encoding of nominal variables\n",
        "The basic nearest neighbour algorithm implemented in the previous assignment can only be used with\n",
        "numerical features. However, often types of variables such as nominal variables or ordinal variables are\n",
        "present. A simple solution to this problem exists in using a dummy encoding for each nominal variable.\n",
        "When a variable $\\boldsymbol{x^i}$ is nominal with $k$ values, it is replaced by $k$ new binary variables. As an imaginary example,\n",
        "consider a dataset with a feature containing weather status. This feature ($\\boldsymbol{x^1}$) could for example contain three values: Sunny, Overcast and Rainy.\n",
        "Each of these values could be represented by a dummy variable: $\\boldsymbol{x^{1a}}$, $\\boldsymbol{x^{1b}}$ and $\\boldsymbol{x^{1c}}$ with the following values:   \n",
        "  *  $x^{1a} = 1$ if $x^1 = \"Sunny\"$ and  $x^{1a} = 0$ otherwise\n",
        "  * $x^{1b} = 1$ if $x^1 = \"Overcast\"$ and $x^{1b} = 0$ otherwise\n",
        "  * $x^{1c} = 1$ if $x^1 = \"Rainy\"$ and $x^{1c} = 0$ otherwise\n",
        "  \n",
        "See the following example in python. In this example, we use  the Abalone dataset (abaloneTrain700.csv) which contains measurements of physical properties of several abalone (an edible sea snail) specimen. Using these physical properties, the aim is to build a predictive model for the age of these animals (more information concerning this dataset can be found in [abalone.info](https://archive.ics.uci.edu/ml/datasets/Abalone)). In the following example, we replace the nominal variable 'sex' with 3 dummy variables (as many as the values it takes). In python, there are functions such as the [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function which are used for this purpose. (Another option is scikit-learn's [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)) So firstly, we create the dummy variables, then we concatenate them with the original dataset and finally we remove the original variable form the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8KcIkTvyfEhz"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "abalone = pd.read_csv('abalone.csv')\n",
        "print(abalone.head())\n",
        "# create dummies\n",
        "dummies = pd.get_dummies(abalone.sex) \n",
        "print(dummies.head())\n",
        "# concatenate them with the dataset\n",
        "abalone_with_dummies = pd.concat([abalone, dummies], axis=1)\n",
        "# remove the original sex column\n",
        "abalone_with_dummies = abalone_with_dummies.drop(['sex'], axis=1)\n",
        "print(abalone_with_dummies.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A8puB8nfEh0"
      },
      "source": [
        "### Missing values\n",
        "Missing values are commonly encountered in data mining studies. Often, missing values are imputed\n",
        "(replaced by a value). Several techniques exist to choose this value. A simple, but often used method\n",
        "is mean imputation. Here, each missing value is replaced by the mean of the observed values for\n",
        "that variable. More advanced methods exist of building separate models to predict the missing\n",
        "values.\n",
        "When implementing the mean imputation, the [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) of scikit-learn library might be handy. The PC labs in this practical have no missing values however:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(abalone_with_dummies).any()"
      ],
      "metadata": {
        "id": "NRrYEy-u6wFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yR7l-8fEh1"
      },
      "source": [
        "### Standardizing the data\n",
        "In realistic datasets, most features have different means and standard deviations. For the nearest\n",
        "neighbour algorithm, it can easily be seen that features with a high standard deviation will be more\n",
        "influential than features with a lower standard deviation. In most cases, this is unwanted since it\n",
        "is not known in advance which features are most important. To overcome this problem, features\n",
        "are often standardized. The standardized version of $x^i$ can be obtained as   \n",
        "$$\\frac{x^i - \\mu_i}{\\sigma_i}$$    \n",
        "where $\\mu_i$ and $\\sigma_i$ represent the sample mean and standard deviation of $\\boldsymbol{x^i}$.\n",
        "The [Scaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) of scikit-learn can be used to perform this standardizing.\n",
        "\n",
        "Notice that we're doing here (scaling the training and test set together in one operation) is usually considered bad practice as it will leak data from train to test and hence bias model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M1gEg0bnfEh2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import scale\n",
        "\n",
        "y = abalone_with_dummies['age'].values # keep the target variable\n",
        "X = abalone_with_dummies.drop(['age'], axis=1) # remove it from the feature set\n",
        "X = scale(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "E4_M6bMzFqHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "qZgA0ehRF2D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55n8InUsfEh7"
      },
      "source": [
        "\n",
        "\n",
        "### Data splitting and Prediction quality\n",
        "To test the performance of the nearest neighbour algorithm, an option is to use test data with\n",
        "known labels and to compare the predictions with these labels. In this case, we have two different\n",
        "datasets, a training set T and a test set T$^*$. In case of a regression problem, the mean of squared\n",
        "residuals is commonly used to evaluate the quality of a model. This measure is calculated as follows:   \n",
        "1. Mean of squared residuals on test set:\n",
        "$$Err_{T^*} = \\frac{1}{|T^*|}\\sum_{\\boldsymbol{x_i} \\in T^*} (Y(\\boldsymbol{x_i}) - y_i)^2$$\n",
        "2. Additionally, the error on the training data itself can be computed:   \n",
        "$$Err_{T} = \\frac{1}{|T|}\\sum_{\\boldsymbol{x_i} \\in T} (Y(\\boldsymbol{x_i}) - y_i)^2$$\n",
        "\n",
        "Naturally, it is desirable to keep the mean of squared residuals as small as possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi0sPwRufEh2"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **To prepare the dataset for the following exercise, split the dataset in a portion (80%) we will use to train on, and a portion we will use to predict on (20%) (test set). See the documentation of scikit learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for more info on how to do this.**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cr8fjikmfEh2"
      },
      "outputs": [],
      "source": [
        "# import train_test_split function\n",
        "\n",
        "# you should get four variables named: X_train, y_train, X_test & y_test\n",
        "# ... = train_test_split( ... )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GHn7LXLfEh3"
      },
      "source": [
        "## k-nearest-neighbours\n",
        "A simple extension of the nearest neighbour algorithm consists of taking more than only the nearest\n",
        "neighbour into account. Let $N_k(\\boldsymbol{x}) \\subset T$ be the k nearest neighbours of an instance with feature\n",
        "vector $\\boldsymbol{x}$. The k-nearest neighbour prediction $Y(\\boldsymbol{x})$ can be determined as follows\n",
        "1. For classification problems:  $Y(\\boldsymbol{x})$ is set as the label that occurs most often in $N_k(\\boldsymbol{x})$ (the\n",
        "mode).\n",
        "2. For regression problems, the average of the $k$ nearest outputs can be taken $Y(\\boldsymbol{x}) = \\frac{1}{k}\\sum_{\\boldsymbol{x_i} \\in N_k(\\boldsymbol{x})} y_i$    \n",
        "\n",
        "### k-nearest-neighbours in python\n",
        "As with the 1-nearest neighbour algorithm, it is possible to implement a version of the k-nearest neighbours\n",
        "algorithm in python. However, an alternative is to use a pre-implemented version of this\n",
        "algorithm. For most popular machine learning algorithms, these functions are packed in specific python\n",
        "“packages\". An implementation of\n",
        "the k-nearest-neighbours algorithm is available in the [scikit-learn](http://scikit-learn.org/stable/) library (as well). \n",
        "You can load and see more info about the usage of this function by typing: (or by going to the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iwPa3SamfEh4"
      },
      "outputs": [],
      "source": [
        "from sklearn import neighbors\n",
        "help(neighbors.KNeighborsRegressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KnzVsnefEh4"
      },
      "source": [
        "As you can see in the help window, in scikit-learn, an estimator for classification/regression is a Python object that implements the methods fit(X, y) and predict(T). The constructor of an estimator takes as arguments the parameters of the model (in our case the basic parameters are the number of neighbours and the distance metric). So the first step is to create a KNN instance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RnFLYebsfEh5"
      },
      "outputs": [],
      "source": [
        "knn = neighbors.KNeighborsRegressor(n_neighbors = 5, metric = 'euclidean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98x8uXcOfEh6"
      },
      "source": [
        "We call our estimator instance `knn`. It now must be fitted to the data, that is, it must learn from the data. This is done by passing our training set to the fit method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gbsJfKqQfEh6"
      },
      "outputs": [],
      "source": [
        "knn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-S_kM6ifEh6"
      },
      "source": [
        "Now you can predict new values, in particular, we can ask to the estimator which is the age of the first example in our test dataset (remember: by doing this, we are now comparing the features of this example to the features of all training samples, then determining what the closest neighbors are, then averaging the age of those neighbors as predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_TebEnkJfEh7"
      },
      "outputs": [],
      "source": [
        "y_pred = knn.predict(X_test[0].reshape(1, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS5zGDBEfEh7"
      },
      "source": [
        "We can calculate the mean squared error of the prediction we get before (for the first example of the test dataset) by calculating it ourselves or by using the [mean_squared_error()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function of scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dMZHjt52fEh8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(y_test[0], y_pred)\n",
        "print(mean_squared_error(y_pred, y_test[0].reshape(1, -1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UhPfQZGfEh8"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **Build a 3-nearest-neighbor regressor in a similar manner as before. Predict the age for all training samples and test samples and compute the mean squared error of both prediction sets. Which predictions are better?**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M17HY9osfEh8"
      },
      "outputs": [],
      "source": [
        "# Instantiate the knn object\n",
        "\n",
        "# Fit\n",
        "\n",
        "# Predict train\n",
        "\n",
        "# Predict test\n",
        "\n",
        "# MSE train\n",
        "\n",
        "# MSE test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7cAMV8ofEh9"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE</b>: **Now do the same but iterate over possible values for 'k' as the number of nearest neighbors taken into account, where k ranges between 1 and 50 (step size equal to 3). Plot the resulting errors of the models as a function of 'k'. 'k' is what we call a hyperparameter: a type of parameter that is (most often) tuned by hand according to the prediction task. For some datasets/tasks we will for example need more or less neighbors (as predictions tasks have varying difficulty).**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Vq3BS2nLfEh9"
      },
      "outputs": [],
      "source": [
        "#results_train = []\n",
        "#results_test = []\n",
        "# for i in range(...):\n",
        "\n",
        "for i in range(...):\n",
        "    # call, fit and predict ...\n",
        "\n",
        "    # append MSE train to list\n",
        "    # results_train.append(...)\n",
        "\n",
        "    # append MSE test to list\n",
        "    # results_test.append(...)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(...) # training MSEs\n",
        "plt.plot(...) # test MSEs\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('$k$ nearest neighbors')\n",
        "plt.legend(['Train MSE', 'Test MSE'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bFmhqH_tKM63"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Lab2_KNN_2022.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}