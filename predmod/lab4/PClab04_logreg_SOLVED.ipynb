{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC lab 4: Logistic regression for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a binary classification setting, we are interested in assigning an observation $\\mathbf{x}$ to one of two possible classes, denoted by $y$. For example, maybe we would like to tell if a patient has a particular disease (y = 1) or not (y = 0), given certain symptoms $\\mathbf{x}$. Generally speaking, we want to predict the probability that the class label $y = 1$, conditional on the data that we have observed, $\\mathbf{x}$. This probability is also called the *class posterior* or the *class-membership probability*, which we can denote as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(Y=1|X) = P(X) = p(y= 1|\\mathbf{x})\n",
    "\\end{equation}\n",
    "\n",
    "The book uses the statistical notation on the left, but the notation with the feature vector $\\mathbf{x}$ is more common in machine learning literature. In any case, both notations mean exactly the same. In this PC lab, we will cover one of the most popular classifiers: logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like linear regression, logistic regression (LR) is a linear model. However, LR does not model the mean of a continuous outcome, but the logarithm of the [odds](https://en.wikipedia.org/wiki/Odds) of the probability $P(X)$:\n",
    "\n",
    "\\begin{equation}\n",
    "log \\frac{P(X)}{1-P(X)} = w_{0}x_{0} + w_{1}x_{1} + ... + w_{p}x_{p} = \\mathbf{w^Tx}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are really interested in the probability $p$ and not in the odds of p. Therefore, it is common to apply the inverse log-odds transformation on both sides of the equation. This transformation is the **logistic function $\\phi(z)$**, hence the name of logistic regression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^{z}}{1+e^{z}}\n",
    "\\end{equation}\n",
    "\n",
    "Verify for yourself that applying $\\phi(z)$ on the log-odds yields $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we can make predictions for $p$ with logistic regression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{x}) = \\phi(w_{0}x_{0} + w_{1}x_{1} + ... + w_{p}x_{p})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to classify a data point $\\mathbf{x}$, we can calculate $p$ with LR and simply assign it to class 1 if $p$ exceeds a certain probability threshold. A typical threshold is 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE: What would happen to our predictions when we would choose a lower threshold, let's say 0.2? How would this affect the accuracy of our predictions? Can you think of a situation where we would want to do this? </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stop for a moment to have a look at what the logistic transformation does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "x = np.arange(-8,8,0.01) # Generate a range of x values\n",
    "y = 1/(1+np.exp(-x)) # Calculate the logistic transformation of these x'es\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y, marker='.');\n",
    "ax.set_xlabel('x');\n",
    "ax.set_ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, $\\phi$ monotonically maps any number from the real domain to a number in [0,1]. Indeed, this is a desirable property if we want to predict a probability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a LR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: the cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the logistic regression model to predict the probability of belonging to a certain class, all that remains is the question of how to find the weights of the model on a given set of training data. As always, this is the problem of minimizing a loss function to find an optimal set of weights. Where we used the mean squared error (MSE) for linear regression, we will use the **cross-entropy** loss function for LR. Minimizing the binary cross-entropy loss is equivalent to minimizing the negative log-likelihood of the data under a binomial distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "l_{log} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}-y_{i}log(p(\\mathbf{x}_i))-(1-y_i)log(1-p(\\mathbf{x}_i))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $y_i$ is the class of data point $i$ and $p(\\mathbf{x}_i)$ is the class-membership probability predicted by logistic regression for the observation $\\mathbf{x}_i$. If we look at the cross-entropy loss **for a single data point** $l_{log}^{i}$, we can break it down in two parts:\n",
    "\n",
    "\\begin{equation}\n",
    "l_{log}^{i} = \n",
    "\\begin{cases}\n",
    "   -log(p(\\mathbf{x}_i)) & \\text{if} \\ y_i = 1\\\\    \n",
    "   -log(1-p(\\mathbf{x}_i)) & \\text{if} \\ y_i = 0\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that the cross-entropy loss will be larger for smaller values of $p(\\mathbf{x}_i)$ if $y_i = 1$, and vice versa. Let's visualize the cross-entropy loss for these two cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "p = np.arange(0.01,0.99,0.01) # Generate a range of predicted probabilities between zero and 1\n",
    "l_0 = -np.log(p)  # cross-entropy loss if y = 1\n",
    "l_1 = -np.log(1-p)\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(p,l_0, marker='.');\n",
    "ax.scatter(p, l_1, marker='.');\n",
    "ax.set_xlabel('Predicted class-membership probability $p$');\n",
    "ax.set_ylabel('Cross-entropy loss');\n",
    "ax.legend(['Cross-entropy loss when y is 1', 'Cross-entropy loss when y is 0']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE: Make sure you understand the cross-entropy loss. Verify that it correctly penalizes wrong predictions in both cases. Suppose that we have no information about the data at all, what would be the best guess for p to minimize the cross-entropy loss?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the weights with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression, the solutions to the normal equations provide a convenient analytical solution to obtain the optimal set of model weights $\\mathbf{w}$ on a set of training data. There is no such solution to find the optimal weights for a logistic regression model, so instead an optimization algorithm such as **gradient descent** is used to train a LR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative optimization algorithm that searches for the optimum of an objective function by making small changes to a set of optimization variables. Gradient descent (and more complex optimization algorithms, but we offer a separate course for that) are widely used in machine learning to find the optimal set of model weights that minimize a certain loss function. Especially when there is no analytical solution for the weights available like for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, gradient descent uses the **gradient** of the loss function with respect to the model weights to perform updates to those weights in each iteration. At iteration $k+1$, the algoritm computes the gradient of a loss function $J(\\mathbf{w})$ evaluated in the training data. Then, it performs an update to the current parameter values that is relative to the gradient multiplied with the learning rate $\\gamma$, which is a constant:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\gamma\\nabla{J(\\mathbf{w}_{k})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the weights are often initialized with random draws from some distribution. The algorithm continues to do updates, until it converges or until some stopping criterion is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform gradient descent to find the weights of a logistic regression model, we need to compute the gradient of the loss function with respect to the model parameters. Recall that, for a single data point, the cross-entropy loss function was as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "l_{log}^{i}(\\mathbf{w}) = -y_{i}log(p(\\mathbf{x}_i))-(1-y_i)log(1-p(\\mathbf{x}_i))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $p(\\mathbf{x}_i)$ is nothing else than the weighted sum of the inputs squashed through the sigmoid function:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{x}_i) = \\phi(w_{0}x_{0i} + w_{1}x_{1i} + ... + w_{p}x_{pi})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, let's first calculate the partial derivative of the sigmoid function:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial z} \\phi(z) = \\frac{\\partial}{\\partial z} \\frac{1}{1+e^{-z}} = \\frac{e^{-z}}{(1+e^{-z})^2}\n",
    "\\end{equation}\n",
    "\n",
    "We can rewrite this as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1 +e^{-z} -1}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}} \\Big( 1 - \\frac{1}{1+e^{-z}}\\Big) = \\phi(z)(1 - \\phi(z))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this result and by applying the chain rule, we can compute the partial derivative of the loss function with respect to the weight $w_j$. We will use the symbol $z$ to denote the weighted sum of the features (i.e., the input for the logistic function) and drop the superscript $i$ for clarity:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l_{log}(\\mathbf{w})}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\Big(-ylog(\\phi(z))-(1-y)log(1-\\phi(z)) \\Big) \\\\ = \\Big(  \\frac{-y}{\\phi(z)} + \\frac{1-y}{1-\\phi(z)}  \\Big)\\frac{\\partial}{\\partial w_j}\\phi(z) \\\\ = \\Big(  \\frac{-y}{\\phi(z)} + \\frac{1-y}{1-\\phi(z)}  \\Big) \\phi(z)(1-\\phi(z))\\frac{\\partial}{\\partial w_j}z\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $z = w_{0}x_{0} + w_{1}x_{1} + ... + w_{p}x_{p}$, $\\frac{\\partial}{\\partial w_j}z$ is nothing more than $x_j$, so we can rewrite the above as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l_{log}(\\mathbf{w})}{\\partial w_j} = \\Big( -y(1-\\phi(z) + (1-y)\\phi(z))\\Big)x_j \\\\ = \\big(  -y + \\phi(z) \\big)x_j = \\big( \\phi(z) - y \\big)x_j\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this partial derivative of the loss w.r.t $w_j$, we can write the update rule of the gradient descent algorithm for the $j^{th}$ weight:\n",
    "    \n",
    "\\begin{equation}\n",
    "w_{j,k+1} = w_{j,k} - \\gamma(\\phi(z_k)-y)x_{j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the algorithm will each time perform an update to the weight $w_{j}$ that is in proportion to the difference between the predicted probability of class membership in the previous iteration and the actual class. Makes sense! The entire gradient is simply the vector that contains the partial derivatives with respect to the entire weight vector $\\mathbf{w}$, and in reality gradient descent acts on $\\mathbf{w}$ and not on an individual weight $w_j$. Also, the gradient is typically not calculated for one data point, but evaluated over the entire training data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, software packages such as [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegressionscikit-learn) do this optimization under the hood, so there is no need to implement it manually each time we want to use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: predicting the status of a breast cancer tumor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first application of logistic regression, we will use the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). The dataset contains information on the disease status of 569 breast cancer patients: they were either diagnosed with a malign (status M) or with a benign (status B) tumor. \n",
    "\n",
    "For each patient, the dataset also contains 30 features that represent statistics of the cell nuclei present in images taken after [fine needle aspirate tissue samples](https://en.wikipedia.org/wiki/Fine-needle_aspiration). These 30 features are the mean, standard deviation and the maximum of 10 measurements on the cell nuclei:\n",
    "- radius\n",
    "- texture\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness\n",
    "- compactness\n",
    "- concavity\n",
    "- concave points\n",
    "- symmetry\n",
    "- fractal dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on these feature of the cell nuclei, we would like to predict whether a patient has a malign or a benign breast cancer tumor.** Let's read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('./wdbc.data', header=None, index_col=0, names=['Patient ID', 'status'] + list(np.arange(1,31,1)))\n",
    "status = data['status']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the distribution of the disease status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data['status']).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 350 benign cases and roughly 200 malign cases. This is a fairly balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE: Suppose that the dataset was unbalanced, with 525 B cases and only 25 M cases. Can you think of any problems this could give if we would evaluate the accuracy of our logistic regression predicitions? We will come back to this problem in one of the next labs.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform LR, we will encode the disease status as a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder().fit(status)\n",
    "encoder.classes_  # 'B' will become class 0, 'M' will become class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.transform(status)\n",
    "x = data.drop('status', axis=1).values # Drop the disease status from the dataframe, convert to numpy array\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE: Using scikit-learn, split the data in a 80% training and a 20% test set. Fit a logistic regression model and evaluate trainig and testing accuracy. You should be able to achieve a fairly high accuracy! </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [this method](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for train-test splitting and [this implementation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to perform logistic regression. You can use the [score method](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score) to evaluate the accuracy of your model. This method computes the accuracy as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "score = \\frac{\\text{Number of correctly classified instances}}{\\text{Total number of instances}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** solution **\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(X_train, y_train)\n",
    "\n",
    "LRmodel.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRmodel.score(X_test, y_test)\n",
    "# ** solution **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of which features are considered important by the LR model, we can visualize the weights it has learned in a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "pd.Series(LRmodel.coef_.flatten()).plot(ax=ax, kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>Use your LR model to predict the class probabilities and the classes for the training data. Use the [```predict_proba()```](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) method to generate the predicted probabilities. Use the code below to plot the two against each other. Which data points are most likely to be misclassified?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** solution **\n",
    "predicted_class_probabilities = LRmodel.predict_proba(X_train)[:,1]\n",
    "predicted_classes = LRmodel.predict(X_train)\n",
    "\n",
    "#** solution **\n",
    "\n",
    "misclassified = predicted_classes !=  y_train\n",
    "\n",
    "colors = ['#b2182b' if wrong else '#2166ac' for wrong in misclassified ]\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(predicted_class_probabilities, predicted_classes, marker='.', s=100, color=colors)\n",
    "ax.set_xlabel('Predicted class probabilies').set_fontsize(20)\n",
    "ax.set_ylabel('Predicted classes').set_fontsize(20)\n",
    "ax.legend(['Correctly classified'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the misclassified points are those points where the predicted probability of class membership is rather close to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-versus-one classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-versus-one classification is another approach to a multiclass classification problem. For a K-class problem, the strategy consists of training $\\frac{K(K-1)}{2}$ classifiers. Each of these classifiers much learn to distinguish to classes. One the classifiers are trained, a voting scheme is applied to make a prediction for an unseen data point: each classifier has to decide between two possible classes. The final predicted class is that class that gets the largest number of votes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-versus-all classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one-versus-all (OvA) classification, a single classifier is trained per class, with the samples of that class as positive samples and all other samples as negatives. The strategy proceeds as follows for a K-class classification problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs:**\n",
    "* a classification algorithm L (learner)\n",
    "* feature matrix $\\mathbf{X}$\n",
    "* label vector y where $y_i \\in {1,...,K}$\n",
    "\n",
    "\n",
    "**Procedure:**\n",
    "for each k in {1,...,K}:\n",
    "* construct a new label vector z where $z_i$ is 1 if $y_i$ = k and 0 otherwise\n",
    "* train L on $\\mathbf{X}$ to obtain a classifier $f_k$. The classifier should return class probabilities and not hard labels.\n",
    "\n",
    "**Returns**\n",
    "A list of trained classifiers $f_k$ for each  k in {1,...,K}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions for a new sample $\\mathbf{x}$, the $k$ classifiers are applied to $\\mathbf{x}$ and the final predicted label is the label that is predicted with the highest confidence (probability):\n",
    "\n",
    "$\\hat{y} = \\underset{k \\in {1,...,K}}{\\mathrm{argmax}} \\, f_k(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate a toy dataset with three classes and two features, and split it in training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers= [[-2.5, 0], [0, 1], [3.5, -1]], random_state=42)\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the plot\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "colors=['#66c2a5', '#fc8d62', '#8da0cb']\n",
    "for i, color in enumerate(colors):\n",
    "    idx_train = np.where(y_train==i)\n",
    "    idx_test = np.where(y_test==i)\n",
    "    plt.scatter(X_train[idx_train,0], X_train[idx_train,1], c=color, edgecolor='black', s=30)\n",
    "    plt.scatter(X_test[idx_test,0], X_test[idx_test, 1],c='white', edgecolor=color, s=70)\n",
    "    \n",
    "ax.legend(['Class 1 - train',\n",
    "           'Class 1 - test',\n",
    "           'Class 2 - train',\n",
    "           'Class 2 - test',\n",
    "           'Class 3 - train',\n",
    "           'Class 3 - test']);\n",
    "\n",
    "ax.set_xlabel('Feature 1');\n",
    "ax.set_ylabel('Feature 2');\n",
    "ax.set_title('Toy dataset for multiclass classification').set_fontsize(20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = np.zeros(len(y_train))\n",
    "z_train[np.where(y_train==i)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Implement a one-versus-all loop to tackle this classification problem. Train a list of classifiers on the training data. Make predictions on the test data. You can use the code below to get started. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***solution***\n",
    "\n",
    "L1 = LogisticRegression()\n",
    "L2 = LogisticRegression()\n",
    "L3 = LogisticRegression()\n",
    "\n",
    "L = [L1, L2, L3]\n",
    "\n",
    "# Train the list of classifiers in one-v-all fashion\n",
    "for i,l in enumerate(L):\n",
    "    z_train = (y_train==i)\n",
    "    l.fit(X_train, z_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for l in L:\n",
    "    predictions.append(l.predict_proba(X_test)[:,1])\n",
    "\n",
    "\n",
    "predicted_classes = np.array([np.argmax([pred[i] for pred in predictions]) for i in range(len(X_test))])\n",
    "\n",
    "# ***solution***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Run the code below to visualize your predictions. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_accuracy=np.round(np.mean(y_test == predicted_classes)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "colors=['#66c2a5', '#fc8d62', '#8da0cb']\n",
    "\n",
    "for i, color in enumerate(colors):\n",
    "    idx_train = np.where(y_train==i)\n",
    "    idx_test = np.where(y_test==i)\n",
    "    \n",
    "    plt.scatter(X_train[idx_train,0], X_train[idx_train,1], c=color, edgecolor='black', s=30)\n",
    "    plt.scatter(X_test[idx_test,0], X_test[idx_test, 1],c='white', edgecolor=color, s=70)\n",
    "        \n",
    "ax.legend(['Class 1 - train',\n",
    "           'Class 1 - test',\n",
    "           'Class 2 - train',\n",
    "           'Class 2 - test',\n",
    "           'Class 3 - train',\n",
    "           'Class 3 - test']);\n",
    "\n",
    "# add predictions\n",
    "for i, color in enumerate(colors):\n",
    "    idx_predicted = np.where(predicted_classes==i)\n",
    "    plt.scatter(X_test[idx_predicted,0], X_test[idx_predicted,1], c=color, marker='s', s=2)\n",
    "\n",
    "ax.set_xlabel('Feature 1');\n",
    "ax.set_ylabel('Feature 2');\n",
    "ax.set_title('Toy dataset for multiclass classification - classification accuracy: {}%'.format(classification_accuracy)).set_fontsize(20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
