{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC lab 6: Linear model selection - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen linear regression to tackle regression problems. With linear regression, we model a continous outcome as a linear function of the features:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = w_{0}x_{0} + w_{1}x_{1} + ... + w_{p}x_{p} = \\sum\\limits_{i=0}^{p}w_{i}x_{i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well when there are a lot of training observations and when the number of features (the dimensionality of the problem) is not too large. However, there are a couple of situations where ordinary linear regression might give problems:\n",
    "\n",
    "* When the number of features $p$ becomes large with respect to the number of observations $n$, the variance of the model weights estimated by linear regression increases, which might result in poor predictive performance. Futhermore, there is no longer an analytical solution provided by least squares when $p > n$. \n",
    "* It is possible that there are a lot of uninteresting or redundant features. If we want a sparse and interpretable model, we might want to do feature selection to reduce $p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will cover two solutions to the problems above: subset selection and regularization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In subset selection, we only use a subset of the features that are available. The goal is to come up with a model that is sparse and that generalizes better to unseen data. There are two main strategies for subset selection: in *best subset selection*, we fit all the $p \\choose k$ models for each $k = 1, 2.. p$ and retain the best model for each $k$. Finally, we select the model that performs best on some measure that controls for overfitting: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bestsubset](img/best_subset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This becomes quickly unfeasible for large values of $p$. Therefore, an alternative approach is to perform *stepwise selection*, which explores a much smaller set of feature combinations. Stepwise selection can be performed either backward or forward. For large $p$ they are the only computationally feasible subset selection methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![forward](img/forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is important to account for the fact that the MSE (or, equivalently, the RSS) will always go down on the training data as we add more and more features. To select the best model out of several candidates, it is important to have an estimate of the test error of each model. This can be done indirectly by using a metric that penalizes for model complexity such as the AIC or the adjusted $R^2$. Another option is to use cross-validation to get an estimate of the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's apply subset selection on two datasets.** The first dataset contains features of mixtures used to produce concrete. The goal is to predict the compressive strength of the concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>blastFurnaceSlag</th>\n",
       "      <th>flyAsh</th>\n",
       "      <th>water</th>\n",
       "      <th>superelastizer</th>\n",
       "      <th>coarseAggregate</th>\n",
       "      <th>fineAggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>compressiveStrength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement  blastFurnaceSlag  flyAsh  water  superelastizer  coarseAggregate  \\\n",
       "0   540.0               0.0     0.0  162.0             2.5           1040.0   \n",
       "1   540.0               0.0     0.0  162.0             2.5           1055.0   \n",
       "2   332.5             142.5     0.0  228.0             0.0            932.0   \n",
       "3   332.5             142.5     0.0  228.0             0.0            932.0   \n",
       "4   198.6             132.4     0.0  192.0             0.0            978.4   \n",
       "\n",
       "   fineAggregate  age  compressiveStrength  \n",
       "0          676.0   28                79.99  \n",
       "1          676.0   28                61.89  \n",
       "2          594.0  270                40.27  \n",
       "3          594.0  365                41.05  \n",
       "4          825.5  360                44.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "import pandas as pd \n",
    "\n",
    "concretedata = pd.read_table('concreteComprStrength.txt', delim_whitespace=True, header=0, index_col=None)\n",
    "concretedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to perform best subset selection with linear regression. The code implements algorithm 6.1 from the book as shown above. For each number of features $k$, the code will print the number of feature combinations that is explored. We fit a model for each combination, so there is quite some computation involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations  with 1 features: 8.0\n",
      "Number of combinations  with 2 features: 28.0\n",
      "Number of combinations  with 3 features: 56.0\n",
      "Number of combinations  with 4 features: 70.0\n",
      "Number of combinations  with 5 features: 56.0\n",
      "Number of combinations  with 6 features: 28.0\n",
      "Number of combinations  with 7 features: 8.0\n",
      "Number of combinations  with 8 features: 1.0\n",
      "Best CV score using 1 features: 0.243730682879381\n",
      "Best CV score using 2 features: 0.3397874857551484\n",
      "Best CV score using 3 features: 0.4714272242491287\n",
      "Best CV score using 4 features: 0.5451881435953686\n",
      "Best CV score using 5 features: 0.6081456334245801\n",
      "Best CV score using 6 features: 0.6054089767480288\n",
      "Best CV score using 7 features: 0.6043137137427337\n",
      "Best CV score using 8 features: 0.6025601286431239\n",
      "Best feature combination: ('cement', 'blastFurnaceSlag', 'flyAsh', 'water', 'age')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy as scipy\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from itertools import combinations \n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Specify features and labels\n",
    "X = concretedata.drop(['compressiveStrength'], axis=1)\n",
    "y = concretedata['compressiveStrength']\n",
    "\n",
    "# Dicts to store the feature combinations and their scores\n",
    "scores_perNfeat = {'mean':[], 'max':[], 'var':[]}\n",
    "best_feature_perNfeat = []\n",
    "# Define splitter object outside loop to ensure same splits over loop\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=None)\n",
    "\n",
    "# For k in 1,...,p\n",
    "for k in np.arange(1,X.shape[1]+1):\n",
    "    print('Number of combinations  with {} features: {}'.format(k, scipy.special.comb(X.shape[1],k)))\n",
    "    best_score_k = 0\n",
    "    scores_k_comb = []\n",
    "    # Create and loop over all the possible feature combinations given k\n",
    "    for c in combinations(X.columns, k):\n",
    "        # Select features from data\n",
    "        X_c = X[list(c)]\n",
    "        # Define model\n",
    "        model = LinearRegression()\n",
    "        # Fit model and compute score on training data\n",
    "        scores = cross_validate(model, X_c.values, y.values, cv=splitter)\n",
    "        mean_cv_score = np.mean(scores['test_score'])\n",
    "        # Save scores\n",
    "        scores_k_comb.append(mean_cv_score)\n",
    "        if mean_cv_score > best_score_k:\n",
    "            best_features = c\n",
    "            best_score_k = mean_cv_score\n",
    "    # Save scores based on # features\n",
    "\n",
    "    scores_perNfeat['max'].append(np.max(scores_k_comb))\n",
    "    scores_perNfeat['var'].append(np.var(scores_k_comb))\n",
    "    scores_perNfeat['mean'].append(np.mean(scores_k_comb))\n",
    "    # Save best feature combination\n",
    "    best_feature_perNfeat.append(best_features)\n",
    "\n",
    "#Get variance of scores within grouped scores \n",
    "for i, score in enumerate(scores_perNfeat['max']):\n",
    "    print(\"Best CV score using {} features: {}\".format(i+1, score))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,8), sharey=True)\n",
    "\n",
    "ax.errorbar(np.arange(len(scores_perNfeat['mean']))+1, scores_perNfeat['mean'], scores_perNfeat['var'], label='mean score')\n",
    "ax.scatter(np.arange(len(scores_perNfeat['max']))+1, scores_perNfeat['max'], alpha=0.5, label='max score')\n",
    "ax.set_xlabel('#features')\n",
    "ax.set_ylabel('accuracy')\n",
    "fig.legend()\n",
    "print('Best feature combination: {}'.format(best_feature_perNfeat[np.argmax(scores_perNfeat['max'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same thing for a second dataset. This time, the features are measurements from a flow cytometry experiment. The 'SC' features measure scatter, and say something about the morphologhy of the cells (FSC: forwad scatter, SSC: sideway scatter). The 'FL' features are fluorescence features from different parts of the spectrum. There are two possible bacterial species present in the dataset. The goal is to classify the correct species based on the measurements from the flow cytometer. Species number one corresponds to *Pseudomonas putida*, while species number 6 is *Brachybacterium faecium*. We will use logistic regression to do the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "bacterialdata = pd.read_csv('fc_data_new.csv', index_col=0)\n",
    "bacterialdata = bacterialdata.drop(['Width', 'Time'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacterialdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Specify features and labels\n",
    "X = bacterialdata.drop(['species'], axis=1)\n",
    "y = bacterialdata['species']\n",
    "\n",
    "# Dicts to store the feature combinations and their scores\n",
    "scores_perNfeat = {'mean':[], 'max':[], 'var':[]}\n",
    "best_feature_perNfeat = []\n",
    "# Define splitter object outside loop to ensure same splits over loop\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=None)\n",
    "\n",
    "# For k in 1,...,p\n",
    "for k in np.arange(1,X.shape[1]+1):\n",
    "    print('Number of combinations  with {} features: {}'.format(k, scipy.special.comb(X.shape[1],k)))\n",
    "    best_score_k = 0\n",
    "    scores_k_comb = []\n",
    "    # Create and loop over all the possible feature combinations given k\n",
    "    for c in combinations(X.columns, k):\n",
    "        # Select features from data\n",
    "        X_c = X[list(c)]\n",
    "        # Define model\n",
    "        model = LogisticRegression() # or model = LDA()\n",
    "        # Fit model and compute score on training data\n",
    "        scores = cross_validate(model, X_c.values, y.values, cv=splitter, n_jobs=4)\n",
    "        mean_cv_score = np.mean(scores['test_score'])\n",
    "        # Save scores\n",
    "        scores_k_comb.append(mean_cv_score)\n",
    "        if mean_cv_score > best_score_k:\n",
    "            best_features = c\n",
    "            best_score_k = mean_cv_score\n",
    "    # Save scores based on # features\n",
    "\n",
    "    scores_perNfeat['max'].append(np.max(scores_k_comb))\n",
    "    scores_perNfeat['var'].append(np.var(scores_k_comb))\n",
    "    scores_perNfeat['mean'].append(np.mean(scores_k_comb))\n",
    "    # Save best feature combination\n",
    "    best_feature_perNfeat.append(best_features)\n",
    "\n",
    "#Get variance of scores within grouped scores \n",
    "for i, score in enumerate(scores_perNfeat['max']):\n",
    "    print(\"Best CV score using {} features: {}\".format(i+1, score))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,8), sharey=True)\n",
    "\n",
    "ax.errorbar(np.arange(len(scores_perNfeat['mean']))+1, scores_perNfeat['mean'], scores_perNfeat['var'], label='mean score')\n",
    "ax.scatter(np.arange(len(scores_perNfeat['max']))+1, scores_perNfeat['max'], alpha=0.5, label='max score')\n",
    "ax.set_xlabel('#features')\n",
    "ax.set_ylabel('accuracy')\n",
    "fig.legend()\n",
    "print('Best feature combination: {}'.format(best_feature_perNfeat[np.argmax(scores_perNfeat['max'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise</h2>\n",
    "    <p>Implement stepwise selection for features as explained at the start of the notebook</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Specify features and labels\n",
    "X = bacterialdata.drop(['species'], axis=1)\n",
    "y = bacterialdata['species']\n",
    "\n",
    "# Lists to store the feature combinations and their scores\n",
    "scores_perNfeat = {'mean':[], 'max':[], 'var':[]}\n",
    "best_feature_perNfeat = []\n",
    "\n",
    "features = [] # start with empty set of features\n",
    "\n",
    "# Define splitter object outside loop to ensure same splits over loop\n",
    "splitter = KFold(n_splits=3, shuffle=True, random_state=None)\n",
    "\n",
    "# For k in 1,...,p\n",
    "for k in np.arange(1,X.shape[1]+1):\n",
    "    best_score_k = 0\n",
    "    scores_k_comb = []\n",
    "    # Create and loop over all the possible features to add\n",
    "    for feature_idx in np.arange(0,X.shape[1]):\n",
    "        if feature_idx in features:\n",
    "            continue\n",
    "        temp_features = features + [feature_idx]\n",
    "        # Select features from data\n",
    "        X_c = X.iloc[:, temp_features]\n",
    "        # Define model\n",
    "        model = LogisticRegression()\n",
    "        # Fit model and compute score on training data\n",
    "        scores = cross_validate(model, X_c.values, y.values, cv=splitter, n_jobs=4)\n",
    "        mean_cv_score = np.mean(scores['test_score'])\n",
    "        # Save scores\n",
    "        scores_k_comb.append(mean_cv_score)\n",
    "        if mean_cv_score > best_score_k:\n",
    "            best_feature = feature_idx\n",
    "            best_score_k = mean_cv_score\n",
    "    features.append(best_feature)\n",
    "    # Save scores based on # features\n",
    "    scores_perNfeat['max'].append(np.max(scores_k_comb))\n",
    "    scores_perNfeat['var'].append(np.var(scores_k_comb))\n",
    "    scores_perNfeat['mean'].append(np.mean(scores_k_comb))\n",
    "    # Save best feature combination\n",
    "    best_feature_perNfeat.append(features[:])\n",
    "\n",
    "#Get variance of scores within grouped scores \n",
    "for i, score in enumerate(scores_perNfeat['max']):\n",
    "    print(\"Best training score using {} features: {}\".format(i+1, score))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,8), sharey=True)\n",
    "ax.errorbar(np.arange(len(scores_perNfeat['mean']))+1, scores_perNfeat['mean'], scores_perNfeat['var'], label='tune_data')\n",
    "ax.scatter(np.arange(len(scores_perNfeat['max']))+1, scores_perNfeat['max'], alpha=0.5)\n",
    "ax.set_xlabel('#features')\n",
    "ax.set_ylabel('accuracy')\n",
    "fig.legend()\n",
    "print('final set of features: {}'.format(set(best_feature_perNfeat[np.argmax(scores_perNfeat['max'])])))\n",
    "best_feature_perNfeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization methods: ridge regression and the lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is one of the most important concepts in machine learning to avoid overfitting. It comes in many forms. In linear regression, regularization techniques typically constrain the coefficient estimates. In return for a little extra bias, this reduces the variance of the coefficient esimates. The two main shrinkage techniques are **ridge regression** and the **lasso**.\n",
    "\n",
    "**Ridge regression penalizes the sum of the squares of the model weights by adding a term to the MSE loss function**:\n",
    "![ridge](img/ridge.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso does a similar thing, but penalizes the absolute value of the model coefficients. The effect of both approaches is that the model coefficients are shrunk towards zero, resulting in less overfitting and less variance in the predictions (at the cost of a little more bias). We will apply both models on two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models for high dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply ridge regression on the Communities and Crime Data Set. The dataset contains 123 population statistics on 1994 communities. We would like to predict the number of violent crimes per 100000 inhabitants. This is the final column of the dataframe. Of the 123 features, a lot contain missing values, so we will drop these columns and use only 99 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./communities.csv')\n",
    "\n",
    "# Drop columns with missing values\n",
    "data = data.dropna(axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise</h2>\n",
    "    <p>Use linear regression and ridge regression to predict the number of violent crimes per 100,000 inhabitants. Use 5-fold cross-validation to evaluate both models. The scikit-learn implementation of RidgeCV automatically performs cross-validation to tune the hyperparameter that determines the amount of regularization, so you don't need to implement a second cross-validation loop. Which model performs best?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we apply ridge regression, it's important to make sure that all the features are on the same scale. If one of the features is on a completely different scale (let's say, income can be measured in dollars or in thousands of dollars), this might lead the ridge regression coefficient to change substantially because of the penalty term in the optimization problem. We can make sure that all the features are on the same scale by use of the standard scaling: (see book p. 232)**\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{x}_{ij} = \\frac{x_{ij}-\\bar{x}_{j}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{ij} - \\bar{x}_{j})^2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this with the ```StandardScaler``` from scikit-learn. We will do the scaling each time in the cross-validation loop using only training data statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** solution \n",
    "\n",
    "# Select X and y\n",
    "y = data['ViolentCrimesPerPop'].values\n",
    "X = data.drop(['ViolentCrimesPerPop'], axis=1).values\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "linreg_scores = []\n",
    "ridge_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Scale X\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    linregmodel = LinearRegression()\n",
    "    ridgemodel = RidgeCV(alphas=np.logspace(-3,3,100))\n",
    "    \n",
    "    linregmodel.fit(X_train, y_train)\n",
    "    linreg_scores.append(linregmodel.score(X_test,y_test))\n",
    "    \n",
    "    ridgemodel.fit(X_train, y_train)\n",
    "    print('Regularization parameter: {}'.format(ridgemodel.alpha_))\n",
    "    ridge_scores.append(ridgemodel.score(X_test,y_test))\n",
    "    \n",
    "\n",
    "print('Average validation fold R² of linear regression: {}'.format(np.mean(linreg_scores)))\n",
    "print('Average validation fold R² of ridge regression: {}'.format(np.mean(ridge_scores)))\n",
    "\n",
    "# ** solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we don't have 99 features, but four times as many features. And suppose that a lot of features are correlated. This situation is very common in lots of datasets. We will mimic this situation by adding correlated features to our original feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X + np.random.normal(loc=0, scale=0.05, size=(X.shape))\n",
    "X_2 = X + np.random.normal(loc=0, scale=0.1, size=(X.shape))\n",
    "X_3 = X + np.random.normal(loc=0, scale=0.01, size=(X.shape))\n",
    "X_expanded = np.concatenate((X, X_1, X_2, X_3), axis=1)\n",
    "\n",
    "X_expanded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise</h2>\n",
    "    <p>Repeat the comparison between ridge regression and linear regression from above, but with the new feature matrix that contains correlated features. What happens with the performance of linear regression? What happens with the regularization parameter? Now, add even more correlated features and repeat the analysis.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** solution\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "linreg_scores = []\n",
    "ridge_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_expanded):\n",
    "    X_train, X_test = X_expanded[train_index], X_expanded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Scale X\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    linregmodel = LinearRegression()\n",
    "    ridgemodel = RidgeCV(alphas=np.logspace(-4,4,100))\n",
    "    \n",
    "    linregmodel.fit(X_train, y_train)\n",
    "    linreg_scores.append(linregmodel.score(X_test,y_test))\n",
    "    \n",
    "    ridgemodel.fit(X_train, y_train)\n",
    "    print('Regularization parameter: {}'.format(ridgemodel.alpha_))\n",
    "    ridge_scores.append(ridgemodel.score(X_test,y_test))\n",
    "    \n",
    "\n",
    "print('Average validation fold R² of linear regression: {}'.format(np.mean(linreg_scores)))\n",
    "print('Average validation fold R² of ridge regression: {}'.format(np.mean(ridge_scores)))\n",
    "# ** solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, it's interesting to look at how the amount of regularization in ridge regression and lasso regression affects the magnitudes of the fitted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "y = data['ViolentCrimesPerPop'].values\n",
    "X = data.drop(['ViolentCrimesPerPop'], axis=1).values\n",
    " \n",
    "# Scale X\n",
    "#scaler = StandardScaler()\n",
    "#x = scaler.fit_transform(X)\n",
    "x = X\n",
    "alphas = np.logspace(-5,2,100)\n",
    "\n",
    "ridge_coefficients = np.ndarray(shape=(50, len(alphas)))\n",
    "lasso_coefficients = np.ndarray(shape=(50, len(alphas)))\n",
    "\n",
    "for i,a in enumerate(alphas):\n",
    "    ridgemodel = Ridge(alpha=a)\n",
    "    lassomodel = Lasso(alpha=a, max_iter=10000)\n",
    "    \n",
    "    ridgemodel.fit(X,y)\n",
    "    lassomodel.fit(X,y)\n",
    "    \n",
    "    ridge_coefficients[:, i] = ridgemodel.coef_[:50]\n",
    "    lasso_coefficients[:, i] = lassomodel.coef_[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2)) = plt.subplots(figsize=(15,10), nrows=2, sharex=True)\n",
    "\n",
    "for c in range(ridge_coefficients.shape[0]):\n",
    "    pd.Series(ridge_coefficients[c,:]).plot(ax=ax1)\n",
    "    pd.Series(lasso_coefficients[c,:]).plot(ax=ax2)\n",
    "\n",
    "ax2.set_xlabel('Regularization parameter').set_fontsize(20)\n",
    "ax1.set_ylabel('Model weights')\n",
    "ax2.set_ylabel('Model weights')\n",
    "ax1.set_title('Model weights for increasing regularization - Ridge regression').set_fontsize(20)\n",
    "ax2.set_title('Model weights for increasing regularization - Lasso').set_fontsize(20)\n",
    "ax2.get_xaxis().set_ticks(alphas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise</h2>\n",
    "    <p>Make sure you understand the plots above. What is the main difference between ridge regression and the lasso?\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with the lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are interested in selecting only a couple of features out of a high dimensional dataset. Let's fit ridge regression and lasso regression on the data and look at the model coefficients for both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ridgemodel = RidgeCV(cv=5)\n",
    "lassomodel = LassoCV(cv=5, max_iter=10000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale X\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "ridgemodel.fit(X_train, y_train)\n",
    "ridgescore = np.round(ridgemodel.score(X_test, y_test),2)\n",
    "lassomodel.fit(X_train, y_train)\n",
    "lassoscore = np.round(lassomodel.score(X_test, y_test),2)\n",
    "\n",
    "# Plot of the coefficients for ridge regression\n",
    "fig, ((ax1, ax2)) = plt.subplots(figsize=(40,30), nrows=2)\n",
    "pd.Series(ridgemodel.coef_).plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Ridge regression coefficients. Test set R²: {}'.format(ridgescore)).set_fontsize(40)\n",
    "ax1.get_xaxis().set_ticks([])\n",
    "ax1.set_xlabel('Features')\n",
    "\n",
    "# Plot of the coefficients for the lasso\n",
    "pd.Series(lassomodel.coef_).plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Lasso regression coefficients. Test set R²: {}'.format(lassoscore)).set_fontsize(40)\n",
    "ax2.get_xaxis().set_ticks([])\n",
    "ax2.set_xlabel('Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso applies regularization by constraining the sum of the absolute values of the model coefficients (the L1-norm). A result of this is that a lot of model coefficients are set to zero: the lasso performs **feature selection**. This is not the case for ridge regression: the model weights are rarely set to zero. Feature selection is a nice property if we want an interpretable model. Let's list the features with non-zero coefficients in the lasso: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,15))\n",
    "sns.barplot(x= pd.Series(lassomodel.coef_[lassomodel.coef_ != 0]),\n",
    "            y=data.drop(['ViolentCrimesPerPop'], axis=1).columns[lassomodel.coef_ != 0],\n",
    "            palette=['steelblue' if n > 0 else 'coral' for n in lassomodel.coef_[lassomodel.coef_ != 0]],\n",
    "            ax=ax);\n",
    "ax.set_title('Lasso model coefficients for the Community Crime dataset').set_fontsize(20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization methods for $n < p$ data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In high dimensional data, we often have more features than observations. This is often called the $n < p$ scenario. In this situation, linear regression breaks down: the variance on the weight estimates blows up and the model will fail on unseen data. Both ridge regression and the lasso are valuable solutions here.\n",
    "\n",
    "We will work with a dataset that contains spectral measurements on food samples. The target variables are the water, fat and protein content of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./meatNIR1000.csv',header=None)\n",
    "colnames = pd.read_csv('./meatNIR1000.colnames', header=None)\n",
    "data.columns = colnames.values[0]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('./meatNIR1000.csv',header=None)\n",
    "colnames = pd.read_csv('./meatNIR1000.colnames', header=None)\n",
    "data.columns = colnames.values[0]\n",
    "W = data['water']\n",
    "F = data['fat']\n",
    "P = data['protein']\n",
    "\n",
    "data = data.drop(['water', 'fat', 'protein'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 240 observations, but 500 features, so we are in a $n < p$ setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot some random data points\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "for i in range(20):\n",
    "    ax.plot(data.iloc[np.random.choice(range(len(data)))])\n",
    "ax.set_xlabel('Wavelength').set_fontsize(20)\n",
    "ax.set_ylabel('Absorbance').set_fontsize(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise</h2>\n",
    "    <p>Try to fit a linear regression model to predict the fat content of a sample on the entire dataset. Look at the model coefficients. What happens?\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** solution\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = data.values\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X, F)\n",
    "model.coef_[:20]\n",
    "# ** solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h2>Exercise</h2>\n",
    "    <p>Compare the performance of linear regression with ridge regression and with the lasso. Use 5-fold cross-validation to evaluate both models.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#** solution\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "# Select X and y\n",
    "y = F\n",
    "X = data.values\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "linreg_scores = []\n",
    "ridge_scores = []\n",
    "lasso_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    # note that standardscaling probably only has a small or no effect here:\n",
    "    # the data already is on the same scale (see the plot with the wavelengths above)\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    linregmodel = LinearRegression()\n",
    "    ridgemodel = RidgeCV(alphas=np.logspace(-2,4,10))\n",
    "    lassomodel = LassoCV(alphas=np.logspace(-2,4,10))\n",
    "    \n",
    "    linregmodel.fit(X_train, y_train)\n",
    "    linreg_scores.append(linregmodel.score(X_test,y_test))\n",
    "    \n",
    "    ridgemodel.fit(X_train, y_train)\n",
    "    ridge_scores.append(ridgemodel.score(X_test,y_test))\n",
    "    \n",
    "    lassomodel.fit(X_train, y_train)\n",
    "    lasso_scores.append(lassomodel.score(X_test,y_test))\n",
    "\n",
    "print('Average validation fold R² of linear regression: {}'.format(np.mean(linreg_scores)))\n",
    "print('Average validation fold R² of ridge regression: {}'.format(np.mean(ridge_scores)))\n",
    "print('Average validation fold R² of lasso regression: {}'.format(np.mean(lasso_scores)))\n",
    "#**solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
