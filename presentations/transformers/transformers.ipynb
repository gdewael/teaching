{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformers\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/soup.png\" width = 500>\n",
    "\n",
    "*Images created by DALL-E 2 when prompted with \"a bowl of soup that is a portal to another dimension as digital art\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers in the news\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/chatgpt.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformers in the news\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/dalle3.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformers in the news\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/alphafold.jpg\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "1. Data as collections of \"tokens\"\n",
    "2. Recap of neural architectures\n",
    "3. Self-attention & The Transformer\n",
    "4. Transformer tricks\n",
    "5. Applications: ChatGPT, AlphaFold, other biological applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data as collections of \"tokens\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text as collections of \"tokens\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_text.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Images as collections of \"tokens\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_img.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Molecules as collections of \"tokens\"\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_mol.png\" width = 350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (mass) spectra as collections of \"tokens\"\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_spectra.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tabular data as collections of \"tokens\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_tabular.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PyTorch toy example of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "tensor([[1, 0, 3, 3, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low = 0, high = 4, size = (1, 6))\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n",
      "tensor([[[0, 1, 0, 0, 1, 1],\n",
      "         [1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 1, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "x_onehot = F.one_hot(x)\n",
    "print(x_onehot.shape)\n",
    "print(x_onehot.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 3, 3, 0, 0]])\n",
      "tensor([[[0, 1, 0, 0, 1, 1],\n",
      "         [1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 1, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x_onehot.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n",
      "tensor([[[-0.39,  1.22,  0.98,  0.98,  1.22,  1.22],\n",
      "         [-0.57,  0.68, -0.25, -0.25,  0.68,  0.68],\n",
      "         [ 0.73,  0.31,  0.04,  0.04,  0.31,  0.31],\n",
      "         [-0.22,  0.62, -0.76, -0.76,  0.62,  0.62]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedder = nn.Embedding(num_embeddings=4, embedding_dim = 4)\n",
    "x_embed = embedder(x)\n",
    "print(x_embed.shape)\n",
    "print(x_embed.transpose(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of neural architectures\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/cnn.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### CNN advantages\n",
    "- Takes advantage of locality of patterns\n",
    "- Applicable for 1/2/3/... dimensional data\n",
    "- Efficient\n",
    "\n",
    "However ..\n",
    "- Receptive field of convolutions is pre-determined\n",
    "- Hard to learn long-term interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/rnn.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### RNN advantages\n",
    "- Takes advantage of sequentiality of patterns.\n",
    "- Intuitive to use in many-to-many, one-to-many, many-to-one, ...\n",
    "\n",
    "However ..\n",
    "- Inefficient (sequential vs parallel)\n",
    "- Only really applicable for 1D data\n",
    "- Can be liable to forgetting data over long term. (hidden state vector is a bottleneck)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where does the MLP fall in this?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/mlp.png\" width = 500>\n",
    "\n",
    "- Fixed structure, no variable-length data\n",
    "- Not applicable for many-to-many, etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick reminder\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The holy grail\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/whatwewant.png\" width = 500>\n",
    "\n",
    "- Efficient\n",
    "- Compares all inputs vs all inputs?\n",
    "- Also for 1D/2D/...\n",
    "- Also for variable-length data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The holy grail = self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/token_mixing.png\" width = 500>\n",
    "\n",
    "- Comparing all tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Similarity matrices via dot products\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/dotproduct.png\" width = 500>\n",
    "\n",
    "For one vector:\n",
    "$\\boldsymbol{x} \\cdot \\boldsymbol{x}^\\top$, with $\\boldsymbol{x} \\in \\mathbb{R}^{1 \\times d}$\n",
    "\n",
    "For a matrix:\n",
    "$\\boldsymbol{X} \\cdot \\boldsymbol{X}^\\top$, with $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From similarity matrix back to $\\mathbb{R}^{n \\times d}$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/XXtX.png\" width = 700>\n",
    "\n",
    "$A = \\boldsymbol{X} \\boldsymbol{X}^\\top$, with $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$\n",
    "\n",
    "$Z = A \\boldsymbol{X}$, with $\\boldsymbol{Z} \\in \\mathbb{R}^{n \\times d}$\n",
    "\n",
    "$Z = (\\boldsymbol{X}\\boldsymbol{X}^\\top) \\boldsymbol{X} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intuition behind the equations\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/intuitionxxtx.png\" width = 500>\n",
    "\n",
    "Every output vector is a weighted sum of input vectors, weighted by a similarity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rounding out the equations\n",
    "\n",
    "Learning?\n",
    "\n",
    "$(\\boldsymbol{X}\\boldsymbol{X}^\\top) \\boldsymbol{X}$\n",
    "\n",
    "$[(\\boldsymbol{X}\\boldsymbol{W}_q)(\\boldsymbol{X}\\boldsymbol{W}_k)^\\top] {\\boldsymbol{X}\\boldsymbol{W}_v}$, with $\\boldsymbol{W}_q, \\boldsymbol{W}_k, \\boldsymbol{W}_v \\in \\mathbb{R}^{h \\times h}$\n",
    "\n",
    "$(\\boldsymbol{Q}\\boldsymbol{K}^\\top) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rounding out the equations\n",
    "\n",
    "Normalization?\n",
    "\n",
    "$(\\boldsymbol{Q}\\boldsymbol{K}^\\top) \\boldsymbol{V}$\n",
    "\n",
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rounding out the equation\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/xxtx_to_attn.png\" width = 600>\n",
    "\n",
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A recap, what have we achieved?\n",
    "\n",
    "From convs and RNNs to self-attention\n",
    "\n",
    "Self-Attention properties:\n",
    "- Parallel execution\n",
    "- Variable input length\n",
    "- Everything interacts with eachother\n",
    "- Information flow depends on content of tokens\n",
    "\n",
    "==> Generic and elegant mechanism\n",
    "\n",
    "==> Has to learn all the structure in data from scratch, but has the freedom to do so to a better extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n",
      "tensor([[[-0.39,  1.22,  0.98,  0.98,  1.22,  1.22],\n",
      "         [-0.57,  0.68, -0.25, -0.25,  0.68,  0.68],\n",
      "         [ 0.73,  0.31,  0.04,  0.04,  0.31,  0.31],\n",
      "         [-0.22,  0.62, -0.76, -0.76,  0.62,  0.62]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x_embed.shape)\n",
    "print(x_embed.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n",
      "tensor([[[-0.89, -0.07,  0.30,  0.30, -0.07, -0.07],\n",
      "         [ 0.09, -1.42, -0.23, -0.23, -1.42, -1.42],\n",
      "         [-0.20,  0.18,  0.70,  0.70,  0.18,  0.18],\n",
      "         [-0.06,  0.71,  0.56,  0.56,  0.71,  0.71]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W_q = nn.Linear(4, 4)\n",
    "Q = W_q(x_embed)\n",
    "print(Q.shape)\n",
    "print(Q.transpose(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 6])\n",
      "tensor([[[-0.10,  0.20,  0.23,  0.23,  0.20,  0.20],\n",
      "         [ 0.41,  0.25,  1.21,  1.21,  0.25,  0.25],\n",
      "         [ 0.22,  0.16,  0.55,  0.55,  0.16,  0.16],\n",
      "         [ 0.22,  0.16,  0.55,  0.55,  0.16,  0.16],\n",
      "         [ 0.41,  0.25,  1.21,  1.21,  0.25,  0.25],\n",
      "         [ 0.41,  0.25,  1.21,  1.21,  0.25,  0.25]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W_q, W_k, W_v = nn.Linear(4, 4), nn.Linear(4, 4), nn.Linear(4, 4)\n",
    "Q, K, V = W_q(x_embed), W_k(x_embed), W_v(x_embed)\n",
    "\n",
    "A = Q @ K.transpose(2,1)\n",
    "print(A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.10,  0.20,  0.23,  0.23,  0.20,  0.20],\n",
      "         [ 0.41,  0.25,  1.21,  1.21,  0.25,  0.25],\n",
      "         [ 0.22,  0.16,  0.55,  0.55,  0.16,  0.16],\n",
      "         [ 0.22,  0.16,  0.55,  0.55,  0.16,  0.16],\n",
      "         [ 0.41,  0.25,  1.21,  1.21,  0.25,  0.25],\n",
      "         [ 0.41,  0.25,  1.21,  1.21,  0.25,  0.25]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[0.13, 0.17, 0.18, 0.18, 0.17, 0.17],\n",
      "         [0.12, 0.11, 0.28, 0.28, 0.11, 0.11],\n",
      "         [0.15, 0.14, 0.21, 0.21, 0.14, 0.14],\n",
      "         [0.15, 0.14, 0.21, 0.21, 0.14, 0.14],\n",
      "         [0.12, 0.11, 0.28, 0.28, 0.11, 0.11],\n",
      "         [0.12, 0.11, 0.28, 0.28, 0.11, 0.11]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "\n",
    "print(F.softmax(A, dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A_normalized = F.softmax(A / Q.shape[-1], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 6]) torch.Size([1, 6, 4])\n",
      "torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "print(A_normalized.shape, V.shape)\n",
    "Z = A_normalized @ V\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "print(x_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4]) torch.Size([1, 6, 4])\n",
      "tensor([[[-0.39,  1.22,  0.98,  0.98,  1.22,  1.22],\n",
      "         [-0.57,  0.68, -0.25, -0.25,  0.68,  0.68],\n",
      "         [ 0.73,  0.31,  0.04,  0.04,  0.31,  0.31],\n",
      "         [-0.22,  0.62, -0.76, -0.76,  0.62,  0.62]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-0.12, -0.13, -0.12, -0.12, -0.13, -0.13],\n",
      "         [ 0.07,  0.07,  0.07,  0.07,  0.07,  0.07],\n",
      "         [ 0.26,  0.25,  0.24,  0.24,  0.25,  0.25],\n",
      "         [-0.31, -0.32, -0.33, -0.33, -0.32, -0.32]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.W_q, self.W_k, self.W_v = (\n",
    "            nn.Linear(d, d), nn.Linear(d, d), nn.Linear(d, d)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        Q, K, V = self.W_q(x), self.W_k(x), self.W_v(x)\n",
    "        A = Q @ K.transpose(2,1)\n",
    "        A_normalized = F.softmax(A / Q.shape[-1], dim = -1)\n",
    "        return A_normalized @ V\n",
    "\n",
    "selfattn = SelfAttention(4)\n",
    "z = selfattn(x_embed)\n",
    "print(x_embed.shape, z.shape)\n",
    "print(x_embed.transpose(2,1))\n",
    "print(z.transpose(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From self-attention to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiple heads\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/heads.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Transformer block\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/layernorm.jpg\" width = 250>\n",
    "\n",
    "Many of these \"blocks\" are stacked to make up a full transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/transformerblock.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.attn = SelfAttention(d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d, d*4), nn.ReLU(), nn.Dropout(0.2), nn.Linear(d*4, d)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x\n",
    "        x = self.ff(self.norm2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4]) torch.Size([1, 6, 4])\n",
      "tensor([[[-0.39,  1.22,  0.98,  0.98,  1.22,  1.22],\n",
      "         [-0.57,  0.68, -0.25, -0.25,  0.68,  0.68],\n",
      "         [ 0.73,  0.31,  0.04,  0.04,  0.31,  0.31],\n",
      "         [-0.22,  0.62, -0.76, -0.76,  0.62,  0.62]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-0.65,  1.61,  1.12,  1.10,  1.15,  1.37],\n",
      "         [-0.37,  1.02, -0.15, -0.09,  0.64,  0.93],\n",
      "         [ 0.45,  0.28,  0.10,  0.20,  0.51,  0.44],\n",
      "         [-1.08,  0.16, -1.35, -1.29, -0.08, -0.03]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer = TransformerLayer(4)\n",
    "z = layer(x_embed)\n",
    "print(x_embed.shape, z.shape)\n",
    "print(x_embed.transpose(2,1))\n",
    "print(z.transpose(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer \"tricks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Permutation invariance and positional information\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/permutequivar.png.png\" width = 600>\n",
    "\n",
    "Due to its design, shuffling inputs just shuffles outputs => The order in the sequence does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/positionalenc.png\" width = 500>\n",
    "\n",
    "Positional encodings give a \"signal\" to the transformer signifying position.\n",
    "\n",
    "Note: positional encodings can be made for 2D, 3D inputs ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Positional encodings for vision transformers\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/vision_transformer.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/relative_positional_encoding.png\" width = 550>\n",
    "\n",
    "Alternative way of adding positional encodings is through the attention matrix itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Causal attention = decoder = autoregressive\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/causal.png\" width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/causalmask.png\" width = 750>\n",
    "\n",
    "Masking with a very large negative number pre-softmax effectively makes self-attention causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.57,  0.57,  0.84,  0.84,  0.57,  0.57],\n",
      "         [ 0.21,  0.38,  0.24,  0.24,  0.38,  0.38],\n",
      "         [ 0.37,  0.11, -0.07, -0.07,  0.11,  0.11],\n",
      "         [ 0.37,  0.11, -0.07, -0.07,  0.11,  0.11],\n",
      "         [ 0.21,  0.38,  0.24,  0.24,  0.38,  0.38],\n",
      "         [ 0.21,  0.38,  0.24,  0.24,  0.38,  0.38]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W_q, W_k, W_v = nn.Linear(4, 4), nn.Linear(4, 4), nn.Linear(4, 4)\n",
    "Q, K, V = W_q(x_embed), W_k(x_embed), W_v(x_embed)\n",
    "A = Q @ K.transpose(2,1)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.ones_like(A).triu(diagonal = 1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.57,  -inf,  -inf,  -inf,  -inf,  -inf],\n",
       "         [ 0.21,  0.38,  -inf,  -inf,  -inf,  -inf],\n",
       "         [ 0.37,  0.11, -0.07,  -inf,  -inf,  -inf],\n",
       "         [ 0.37,  0.11, -0.07, -0.07,  -inf,  -inf],\n",
       "         [ 0.21,  0.38,  0.24,  0.24,  0.38,  -inf],\n",
       "         [ 0.21,  0.38,  0.24,  0.24,  0.38,  0.38]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.masked_fill(mask.bool(), float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "         [0.46, 0.54, 0.00, 0.00, 0.00, 0.00],\n",
       "         [0.42, 0.32, 0.27, 0.00, 0.00, 0.00],\n",
       "         [0.33, 0.25, 0.21, 0.21, 0.00, 0.00],\n",
       "         [0.19, 0.22, 0.19, 0.19, 0.22, 0.00],\n",
       "         [0.15, 0.18, 0.16, 0.16, 0.18, 0.18]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_masked = A.masked_fill(mask.bool(), float(\"-inf\"))\n",
    "F.softmax(A_masked, dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sparse attention (masks)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/custommasks.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From self-attention to cross-attention\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/machine_translation.png\" width = 700>\n",
    "\n",
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$\n",
    "\n",
    "To go from one \"source\" set of data tokens to another \"target\" set of data tokens:\n",
    "- The keys $K$ and values $V$ come from the source domain\n",
    "- The queries $Q$ come from the target domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross-attention\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/crossattn.png\" width = 700>\n",
    "\n",
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$\n",
    "\n",
    "To go from one \"source\" set of data tokens to another \"target\" set of data tokens:\n",
    "- The keys $K$ and values $V$ come from the source domain (red)\n",
    "- The queries $Q$ come from the target domain (blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 1, 1, 2]])\n",
      "tensor([[[-1.20,  0.98, -0.39, -0.39, -1.20],\n",
      "         [-0.33, -0.25, -0.57, -0.57, -0.33],\n",
      "         [ 0.74,  0.04,  0.73,  0.73,  0.74],\n",
      "         [-0.83, -0.76, -0.22, -0.22, -0.83]]], grad_fn=<TransposeBackward0>)\n",
      "torch.Size([1, 5, 4])\n",
      "tensor([[0, 2]])\n",
      "tensor([[[ 1.22, -1.20],\n",
      "         [ 0.68, -0.33],\n",
      "         [ 0.31,  0.74],\n",
      "         [ 0.62, -0.83]]], grad_fn=<TransposeBackward0>)\n",
      "torch.Size([1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "x_source = torch.randint(low = 0, high = 4, size = (1, 5))\n",
    "x_source_embed = embedder(x_source)\n",
    "print(x_source)\n",
    "print(x_source_embed.transpose(2,1))\n",
    "print(x_source_embed.shape)\n",
    "\n",
    "x_target = torch.randint(low = 0, high = 4, size = (1, 2))\n",
    "x_target_embed = embedder(x_target)\n",
    "print(x_target)\n",
    "print(x_target_embed.transpose(2,1))\n",
    "print(x_target_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 5])\n",
      "tensor([[[0.09, 0.24, 0.21, 0.21, 0.09],\n",
      "         [0.54, 0.57, 0.51, 0.51, 0.54]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = W_q(x_target_embed)\n",
    "K, V = W_k(x_source_embed), W_v(x_source_embed)\n",
    "A = Q @ K.transpose(2,1)\n",
    "print(A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4])\n",
      "tensor([[[-0.17, -0.47, -0.31, -0.86],\n",
      "         [-0.17, -0.47, -0.31, -0.86]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Z = torch.softmax(A / Q.shape[-1], dim = -1) @ V\n",
    "print(Z.shape)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modern Transformer libraries\n",
    "\n",
    "- Included in PyTorch\n",
    "- Huggingface\n",
    "- ...\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/hugginface.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### ChatGPT\n",
    "\n",
    "1. Scrape the internet (essentially) + train a huge autoregressive transformer\n",
    "2. Fine-tune it for answering prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Scrape the internet (essentially) + train a huge autoregressive transformer\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/gpt.png\" width = 400>\n",
    "\n",
    "Bottleneck: $$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Fine-tune it for answering prompts\n",
    "\n",
    "<img src=\"https://i.kym-cdn.com/entries/icons/original/000/044/025/shoggothhh_header.jpg\" width = 700>\n",
    "\n",
    "The smiley says: \"As an AI language model, I have trained to generate responses that are intended to be helpful, informative, and objective ...\"\n",
    "\n",
    "Bottleneck: Data collection (again, $$$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Reinforcement learning from human feedback\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*yCzfUi2CgSl-yW_gYAjDMw.png\" width = 600>\n",
    "\n",
    "1. Collect various completions from prompts, let humans label what they like.\n",
    "1. Fit a model to predict what humans like (Reward model).\n",
    "2. Use the reward model to fine-tune the model to say what humans like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AlphaFold\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/pL18FAkwzN55iHvMt2W4XRGjueHWe0ILqX1Qm2e4qlPsK3yjDSott3LZIgSg2uqPPn7Zvu3hfxUtYtjDs3bM27zcF8AO_jYnfk8q=w1440\" width = 700>\n",
    "\n",
    "- Flexibility of self-attention: learns interactions between the far end and the close end of proteins\n",
    "- Positional encodings: Grounding in 3D space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other biological applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### CpG Transformer\n",
    "\n",
    "<img src=\"https://www.biorxiv.org/content/biorxiv/early/2021/09/17/2021.06.08.447547/F1.large.jpg\" width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### TIS Transformer\n",
    "\n",
    "<img src=\"https://www.biorxiv.org/content/biorxiv/early/2021/11/19/2021.11.18.468957/F1.large.jpg\" width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Take home message: the transformer\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/whatwewant.png\" width = 500>\n",
    "\n",
    "As opposed to CNNs, RNNs, ...\n",
    "- Learn interactions between all inputs\n",
    "- \"Freedom\" to learn any pattern: high skill ceiling, but need lots of data\n",
    "- Because of this: the basis for almost all big-news advancements in AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaetan_utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
