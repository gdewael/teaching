{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformers\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/soup.png\" width = 500>\n",
    "\n",
    "*Images created by DALL-E 2 when prompted with \"a bowl of soup that is a portal to another dimension as digital art\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers in the news\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/chatgpt.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformers in the news\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/dalle3.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformers in the news\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/alphafold.jpg\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "1. Data as collections of \"tokens\"\n",
    "2. Recap of neural architectures\n",
    "3. Self-attention & The Transformer\n",
    "4. Transformer tricks\n",
    "5. Applications: ChatGPT, AlphaFold, other biological applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data as collections of \"tokens\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Text as collections of \"tokens\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_text.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Images as collections of \"tokens\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_img.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Molecules as collections of \"tokens\"\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_mol.png\" width = 350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (mass) spectra as collections of \"tokens\"\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_spectra.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tabular data as collections of \"tokens\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/tokens_tabular.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PyTorch toy example of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "tensor([[3, 2, 3, 2, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low = 0, high = 4, size = (1, 6))\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n",
      "tensor([[[0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1],\n",
      "         [0, 1, 0, 1, 1, 0],\n",
      "         [1, 0, 1, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "x_onehot = F.one_hot(x)\n",
    "print(x_onehot.shape)\n",
    "print(x_onehot.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 2, 3, 2, 2, 1]])\n",
      "tensor([[[0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1],\n",
      "         [0, 1, 0, 1, 1, 0],\n",
      "         [1, 0, 1, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x_onehot.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n",
      "tensor([[[-1.18,  0.50, -1.18,  0.50,  0.50,  2.00],\n",
      "         [ 0.40, -0.62,  0.40, -0.62, -0.62, -0.03],\n",
      "         [-1.56,  0.51, -1.56,  0.51,  0.51,  0.77],\n",
      "         [-2.31, -1.87, -2.31, -1.87, -1.87, -1.20]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedder = nn.Embedding(num_embeddings=4, embedding_dim = 4)\n",
    "x_embed = embedder(x)\n",
    "print(x_embed.shape)\n",
    "print(x_embed.transpose(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of neural architectures\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/cnn.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### CNN advantages\n",
    "- Takes advantage of locality of patterns\n",
    "- Applicable for 1/2/3/... dimensional data\n",
    "- Efficient\n",
    "\n",
    "However ..\n",
    "- Receptive field of convolutions is pre-determined\n",
    "- Hard to learn long-term interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/rnn.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### RNN advantages\n",
    "- Takes advantage of sequentiality of patterns.\n",
    "- Intuitive to use in many-to-many, one-to-many, many-to-one, ...\n",
    "\n",
    "However ..\n",
    "- Inefficient (sequential vs parallel)\n",
    "- Only really applicable for 1D data\n",
    "- Can be liable to forgetting data over long term. (hidden state vector is a bottleneck)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where does the MLP fall in this?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/mlp.png\" width = 500>\n",
    "\n",
    "- Fixed structure, no variable-length data\n",
    "- Not applicable for many-to-many, etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick reminder\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The holy grail\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/whatwewant.png\" width = 500>\n",
    "\n",
    "- Efficient\n",
    "- Compares all inputs vs all inputs?\n",
    "- Also for 1D/2D/...\n",
    "- Also for variable-length data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The holy grail = self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/token_mixing.png\" width = 500>\n",
    "\n",
    "- Comparing all tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Similarity matrices via dot products\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/dotproduct.png\" width = 500>\n",
    "\n",
    "For one vector:\n",
    "$\\boldsymbol{x} \\cdot \\boldsymbol{x}^\\top$, with $\\boldsymbol{x} \\in \\mathbb{R}^{1 \\times d}$\n",
    "\n",
    "For a matrix:\n",
    "$\\boldsymbol{X} \\cdot \\boldsymbol{X}^\\top$, with $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From similarity matrix back to $\\mathbb{R}^{n \\times d}$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/XXtX.png\" width = 700>\n",
    "\n",
    "$A = \\boldsymbol{X} \\boldsymbol{X}^\\top$, with $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$\n",
    "\n",
    "$Z = A \\boldsymbol{X}$, with $\\boldsymbol{Z} \\in \\mathbb{R}^{n \\times d}$\n",
    "\n",
    "$Z = (\\boldsymbol{X}\\boldsymbol{X}^\\top) \\boldsymbol{X} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intuition behind the equations\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/intuitionxxtx.png\" width = 500>\n",
    "\n",
    "Every output vector is a weighted sum of input vectors, weighted by a similarity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rounding out the equations\n",
    "\n",
    "Learning?\n",
    "\n",
    "$(\\boldsymbol{X}\\boldsymbol{X}^\\top) \\boldsymbol{X}$\n",
    "\n",
    "$[(\\boldsymbol{X}\\boldsymbol{W}_q)(\\boldsymbol{X}\\boldsymbol{W}_k)^\\top] {\\boldsymbol{X}\\boldsymbol{W}_v}$, with $\\boldsymbol{W}_q, \\boldsymbol{W}_k, \\boldsymbol{W}_v \\in \\mathbb{R}^{h \\times h}$\n",
    "\n",
    "$(\\boldsymbol{Q}\\boldsymbol{K}^\\top) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rounding out the equations\n",
    "\n",
    "Normalization?\n",
    "\n",
    "$(\\boldsymbol{Q}\\boldsymbol{K}^\\top) \\boldsymbol{V}$\n",
    "\n",
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rounding out the equation\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/xxtx_to_attn.png\" width = 600>\n",
    "\n",
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A recap, what have we achieved?\n",
    "\n",
    "From convs and RNNs to self-attention\n",
    "\n",
    "Self-Attention properties:\n",
    "- Parallel execution\n",
    "- Variable input length\n",
    "- Everything interacts with eachother\n",
    "- Information flow depends on content of tokens\n",
    "\n",
    "==> Generic and elegant mechanism\n",
    "\n",
    "==> Has to learn all the structure in data from scratch, but has the freedom to do so to a better extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 3, 1, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.02, -1.30, -1.30,  1.26,  0.34,  0.34],\n",
       "         [ 1.05, -1.40, -1.40,  0.32,  0.75,  0.75],\n",
       "         [-1.71,  1.33,  1.33,  0.18, -1.14, -1.14],\n",
       "         [ 0.84, -0.93, -0.93, -0.32,  1.98,  1.98]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_embed.shape)\n",
    "print(x_embed.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.48, -1.20, -1.20, -0.03,  0.42,  0.42],\n",
       "         [ 0.50, -0.67, -0.67,  0.37,  0.14,  0.14],\n",
       "         [-0.19,  1.20,  1.20,  0.53, -0.33, -0.33],\n",
       "         [ 0.32,  1.37,  1.37,  0.02, -0.66, -0.66]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q = nn.Linear(4, 4)\n",
    "Q = W_q(x_embed)\n",
    "print(Q.shape)\n",
    "print(Q.transpose(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data/home/gaetandw/utils/teaching/presentations/transformers/transformers.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdeeptought/data/home/gaetandw/utils/teaching/presentations/transformers/transformers.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m W_q, W_k, W_v \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m), nn\u001b[39m.\u001b[39mLinear(\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m), nn\u001b[39m.\u001b[39mLinear(\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeeptought/data/home/gaetandw/utils/teaching/presentations/transformers/transformers.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m Q, K, V \u001b[39m=\u001b[39m W_q(x_embed), W_k(x_embed), W_v(x_embed)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeeptought/data/home/gaetandw/utils/teaching/presentations/transformers/transformers.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m A \u001b[39m=\u001b[39m Q \u001b[39m@\u001b[39m K\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "W_q, W_k, W_v = nn.Linear(4, 4), nn.Linear(4, 4), nn.Linear(4, 4)\n",
    "Q, K, V = W_q(x_embed), W_k(x_embed), W_v(x_embed)\n",
    "\n",
    "A = Q @ K.transpose(2,1)\n",
    "print(A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\texttt{softmax}(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^\\top}{\\sqrt{d}}) \\boldsymbol{V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.09,  0.95,  0.95, -0.09, -0.19, -0.19],\n",
      "         [ 1.53, -2.10, -2.10,  0.42,  2.10,  2.10],\n",
      "         [ 1.53, -2.10, -2.10,  0.42,  2.10,  2.10],\n",
      "         [-0.57,  0.25,  0.25,  0.34, -0.40, -0.40],\n",
      "         [-1.02,  0.77,  0.77, -0.26, -1.35, -1.35],\n",
      "         [-1.02,  0.77,  0.77, -0.26, -1.35, -1.35]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[0.11, 0.30, 0.30, 0.11, 0.09, 0.09],\n",
      "         [0.20, 0.01, 0.01, 0.07, 0.36, 0.36],\n",
      "         [0.20, 0.01, 0.01, 0.07, 0.36, 0.36],\n",
      "         [0.10, 0.22, 0.22, 0.24, 0.11, 0.11],\n",
      "         [0.06, 0.36, 0.36, 0.13, 0.04, 0.04],\n",
      "         [0.06, 0.36, 0.36, 0.13, 0.04, 0.04]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "\n",
    "print(F.softmax(A, dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A_normalized = F.softmax(A / Q.shape[-1], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 6]) torch.Size([1, 6, 4])\n",
      "torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "print(A_normalized.shape, V.shape)\n",
    "Z = A_normalized @ V\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "print(x_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4]) torch.Size([1, 6, 4])\n",
      "tensor([[[-2.02, -1.30, -1.30,  1.26,  0.34,  0.34],\n",
      "         [ 1.05, -1.40, -1.40,  0.32,  0.75,  0.75],\n",
      "         [-1.71,  1.33,  1.33,  0.18, -1.14, -1.14],\n",
      "         [ 0.84, -0.93, -0.93, -0.32,  1.98,  1.98]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-0.13, -0.28, -0.28, -0.41, -0.39, -0.39],\n",
      "         [ 0.59,  0.26,  0.26,  0.12,  0.15,  0.15],\n",
      "         [-0.64, -0.47, -0.47, -0.31, -0.34, -0.34],\n",
      "         [-0.07,  0.24,  0.24,  0.50,  0.44,  0.44]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.W_q, self.W_k, self.W_v = (\n",
    "            nn.Linear(d, d), nn.Linear(d, d), nn.Linear(d, d)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        Q, K, V = self.W_q(x), self.W_k(x), self.W_v(x)\n",
    "        A = Q @ K.transpose(2,1)\n",
    "        A_normalized = F.softmax(A / Q.shape[-1], dim = -1)\n",
    "        return A_normalized @ V\n",
    "\n",
    "selfattn = SelfAttention(4)\n",
    "z = selfattn(x_embed)\n",
    "print(x_embed.shape, z.shape)\n",
    "print(x_embed.transpose(2,1))\n",
    "print(z.transpose(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From self-attention to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple heads\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/heads.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer block\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/layernorm.jpg\" width = 250>\n",
    "\n",
    "Many of these \"blocks\" are stacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/transformerblock.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer \"tricks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation invariance and positional information\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/permutation_invariance.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/positionalenc.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/relative_positional_encoding.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal attention = decoder = autoregressive\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/causal.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/causal_mask.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse attention (masks)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/custommasks.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-attention\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gdewael/teaching/main/presentations/transformers/img/crossattn.png\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Transformer libraries\n",
    "\n",
    "- PyTorch\n",
    "- Huggingface\n",
    "- ....\n",
    "- ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other biological applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaetan_utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
